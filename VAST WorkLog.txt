 
To-Do:

- add VON_REQUEST_ID msg or send VON_HELLO first? (for ID assignment without net layer involvement)
- check if initially only VON_MOVE exists is okay (no VON_MOVE_B for boundary neighbor checks)
- check redundency in messages sent out


- check timestamp compare are only for those generated at same host (it's meaningful)
- optimize MOVE event publications (combine event & AOI update into one)
- VoroCast (area publication)


Bug List:

BUG (*) denotes design bug:

---
2012-06-23: 1st & 2nd level enclosing neighbors are the same
2012-07-23: voro.overlaps returns 'false' always when there's only one region for any point
2012-08-29: scaling many nodes will cause TCP connection errors
2012-09-03: EADDRINUSE crash bug (after running a while with >50 nodes)
2012-09-24: send_msg in vast_net does not necessarily send out in FIFO order



----
2009-06-12 UDP transmission causes connection lost in real network
2009-07-08 realnet: agent join does not work correctly for over 3 agents or after existing agent departs


2009-07-29* if all known AOI peers of a relay's client fail (due to the relay that manages them fail).
            after the clients re-join, this relay may be partitioned, as no AOI neighbor would know / be
            interested to contact it (so this is a massive failure scenario --> if all your AOI neighbors disappear,
            might cause partition) see VAST-case-2009-07-29

2009-07-30b some agents would have undeleted objects in multiple arbitrators (probably not receiving OBJECT_D properly)
2009-08-07  after ownership reclaim is enabled, some nodes inside the 1st arbitrator's region would disappear
2009-08-21  avatar objects remain visible after agents have moved out of AOI (caused by having deletion buffer)
2009-09-29  linux server would attempt to re-connect to lost agents and freeze
2010-04-30  too many relays are removed

- ghost objects at client-view
- each client subscribes more than once, (though may be harmless, but re-subscription should be avoided)
- client crash (in stress test, some clients seem to depart early, about 5%)
- gateway crash 
      join rate of 1 node / sec and 500 nodes, gateway crashes after roughly 200 nodes
      in 300 nodes, 2 sec / join, gateway would seg fault
- join retry timeout is buggy
- 2010-10-06 after repeated join/leave of worlds, cannot join again (gateway could not assign origin matcher)

Overall Goals:
extract non-VON-specific code from VON_peer as common code for handlers
build VSS server (to take & respond RESTful pub/sub commands)
connect VSS functions with VON peer
debug VON peer usage from VSS server
allow ident be queried when get subscribers


2012-09-28 (5)
--------------
goal: debug & test JSON response
      debug node register / revoke

      
      

2012-09-27 (4)
--------------
goal: debug & test JSON response
      debug node register / revoke

- review current code & procedure.
      

2012-09-26 (3)
--------------
goal: extract non-VON-specific code from VON_peer as common code for handlers
      build VSS server (to take & respond RESTful pub/sub commands)
      connect VSS functions with VON peer
      debug VON peer usage from VSS server
      allow ident be queried when get subscribers
      add revokeNode to VSS function
      
- revokeNode & registerNode added

2012-09-25 (2)
--------------
goal: extract non-VON-specific code from VON_peer as common code for handlers
      build VSS server (to take & respond RESTful pub/sub commands)
      connect VSS functions with VON peer
      debug VON peer usage from VSS server
      debug send_msg mis-ordering
      
for the 2012-09-24 bug:      
- add queuing up messages to be sent, if connection is not yet established.
  can run till 50 nodes ok (on PC), but crash due to EADDINUSE for 100 nodes
  but we still get remote id is NaN error

- might consider add explicit id exchange during handshake, to ensure remote id is always available.

- seems the NaN id is reduced (for 50 nodes), but some nodes still get NaN id
  at times (esp. as the # of nodes increases). 
  
  
      
2012-09-24 (1)
--------------
goal: extract non-VON-specific code from VON_peer as common code for handlers
      build VSS server (to take & respond RESTful pub/sub commands)
      connect VSS functions with VON peer
      debug VON peer usage from VSS server
      
- test again test_VON_scale for scaling VON server @ dev server. can run up to 50+100 = 150 nodes. 
  however, client movement in browser stops when all nodes are joined

  when all existing nodes at server are closed:
  gateway (test_VON_peer) experience this: connect ECONNREFUSED and it'll crash
  
- test run 100 VON nodes @ dev server, seems to continously moving notice the following:
  * CPU is used at 100%, RAM 10 - 15% (out of 512MB, left 168 MB)
  * CPU is used by the test_VON_scale 
  * movement becomes slow after a while (data accumulated?)
  * file handles were in the range of 10,000.. while after termination, only 700 file handles are used normally  

BUG:    found a bug caused by the asyncrous nature of js function calls
        when a newly joined VON peer tries to send HELLO / EN to a newly learned neighbor,
        the EN message can be sent *BEFORE* the HELLO is sent (or before the connection is established)
        the reason is that when sending the first HELLO message, connection is yet established,
        so an attempt is made to connect. 
        but this is returned directly, so the next message is sent and stored.
        when the connection is finally made, the 2nd message gets delivered first before the first message.

        basically we cannot assume that two messages will be sent in a FIFO order.
          
  
2012-09-23 (7)
--------------
goal: extract non-VON-specific code from VON_peer as common code for handlers
      build VSS server (to take & respond RESTful pub/sub commands)
      connect VSS functions with VON peer
      test VON peer usage from VSS server
      
- done integrate
  publishPos
  subscribeNearby
  unsubscribeNearby
  querySubscribers      
      

2012-09-22 (6)
--------------
goal: extract non-VON-specific code from VON_peer as common code for handlers
      build VSS server (to take & respond RESTful pub/sub commands)
      connect VSS functions with VON peer
      
- done parameter extraction from RESTful request      
- publishPos, subscribeNearby done preliminary integration with VON peer      

2012-09-21 (5)
--------------
goal: extract non-VON-specific code from VON_peer as common code for handlers
      build VSS server (to take & respond RESTful pub/sub commands)
      connect VSS functions with VON peer
      
- implement basic functions wtih VSS server, will now need to use & connect them with VON peer      


2012-09-20 (4)
--------------
goal: extract non-VON-specific code from VON_peer as common code for handlers
      build VSS server (to take & respond RESTful pub/sub commands)
      parameter breakout & analysis
      
- done parameter breakout
      

2012-09-19 (3)
--------------
goal: extract non-VON-specific code from VON_peer as common code for handlers
      build VSS server (to take & respond RESTful pub/sub commands)

- done creating basic layout for VSS server

2012-09-18 (2)
--------------
goal: extract non-VON-specific code from VON_peer as common code for handlers

- add VSS server binding to provide pub/sub functions
- study about js inhertance & prototype mechanism

2012-09-17 (1)
--------------
goal: VAST porting
      basic pub/sub functions
      implement VASTClient
      implement join()
      build sharable network layer (for VASTClient & VON)
      extract non-VON-specific code from VON_peer as common code for handlers

- still stuck at not sure how to extract net layer so the following effects are achieved:
  * multiple logics supportable on same net layer
  * logic are independent of each other (can be modularly inserted/removed)
  * do not use a singleton (so that multiple net instance can exist in same process)
      

2012-09-16 (7)
--------------
goal: VAST porting
      basic pub/sub functions
      implement VASTClient
      implement join()
      build sharable network layer (for VASTClient & VON)

- evaluate and decide to follow original VAST C++ approach (message handler) 
  for message processing, pending fixes for js-specific behaviors.
      

2012-09-15 (6)
--------------
goal: VAST porting
      basic pub/sub functions
      implement VASTClient
      implement join()
      build sharable network layer (for VASTClient & VON)

- create message_handler.js
  can plugin independent logic to handler, while sharing a common network layer for send/recv
  
  required methods:
  
  processPack()     handle incoming messages relevant to this handler
  sendMessage()     send a message to one or more targets at a different handler
  tick()            perform incoming message processing
  

2012-09-14 (5)
--------------
goal: VAST porting
      basic pub/sub functions
      implement VASTClient
      implement join()
      build sharable network layer (for VASTClient & VON)

- need to consider:
  whether functions of VON/VSO/SPS (& possibly future VoroCast) can be completely de-couple?
  also the net layer can be physical or simulated
  in other words, any one of them can be enabled/disabled.  

a possible division of labor:

VON:    peer connectivity, neighbor discovery within peer's AOI
        - find gateway and join network
        - position movement
        - update from neighbors
        
SPS:    pub/sub record keeping, pub/sub matching, request passing
        - pub/sub request
        - sub record storage
        - pub request passing to neighbors
        - pub/sub matching
        - delivery of messages

VSO:    region resizing, pub/sub record migration, fault-tolerance (?)
        - load detection
        - overload handling (resize)
        - overload handling (join)
        - underload handling (resize)
        - underload handling (leave)
        - load migration (ensure consistency & fault-tolerance)
        
VCAST:  request forwarding


Dependencies: 
VSO -> VON
VCAST -> VON/VSO
SPS -> VON/VSO/VCAST


2012-09-13 (4)
--------------
goal: VAST porting
      basic pub/sub functions
      implement VASTClient
      implement join()
      build sharable network layer (for VASTClient & VON)

- reorganize join procedure. 
- decide to use singleton mode for net layer sharing

2012-09-12 (3)
--------------
goal: VAST porting
      basic pub/sub functions
      implement VASTClient
      implement join()


- see that the network layer needs to be shared by VON, VAST, VSO...
- might need to pass in net layer to VON (and also VASTClient), 
  and create a separate MessageHandler class (any other way?)
  
  

2012-09-11 (2)
--------------
goal: VAST porting
      basic pub/sub functions
      layout modular framework (for independent features)


- upload vast.js codebase to github

- basic idea for making VASTClient, VASTRelay, VASTMatcher modular:

VASTMatcher:
  support for pub/sub matching should exist at VASTMatcher
  it deals with pub/sub record keeping & matching
  VASTMatcher can be implemented with either VON or VSO,
  the latter will provide dynamic load balancing among matchers.
  if only the former is used, then only static partitioning is available.

  basic matcher functions:
  * accept subscribe request (from VASTClient)
  * accept publish request (from VASTClient)
  * forward publish request to neighbors
  * deliver publications to relevant subscribers within own region
  * list currently known subscribers
  * join a VON network and maintain topology with neighbors


VASTRelay:
  support physical coordniate identification and connection to closest Relay

VASTClient:
  basic client functions to forward all requests to current "owner matcher"
  and get all responses from "owner matcher" (possibly via Relay)




2012-09-10 (1)
--------------
goal: VAST porting
      basic pub/sub functions


- review & learn that a VAST node actually has three components:
    VASTClient  - end user pub/sub requests
    VASTMatcher - server that handles pub/sub matching
    VASTRelay   - locate and connect with physical neighbors

 need to think about how to make feature-adding modular


2012-09-09 (7)
--------------
goal: VSO porting
      basic pub/sub functions

- create VSO_peer.js file and build initial API methods


2012-09-08 (6)
--------------
goal: VSO porting
      basic pub/sub functions
      debug asymmetric AOI 

DEBUG:
2012-09-07: aymmetric AOI doesn't work
 	the asymmetric AOI bug disappears if 'radius' is not updated from webpage input
        it means that updating AOI radius thus might have corrupted the data
        suspect it's caused by parameter passing in that do not conform to VAST types       
        > check if all input to VON_peer, will conform to internal data structure format
        > seems to solve the problem


2012-09-07 (5)
--------------
goal: VSO porting
      basic pub/sub functions

- allow position (keyboard input) & AOI-radius update to be reflected on GUI

BUG: asymmetric AOI does not seem to work


2012-09-06 (4)
--------------
goal: run 100+ nodes at Linux server (dev server)


BUG -new error when running 100 VONpeers on gateway: EMFILE

- seems workable for some time, though still will get EMFILE error
after some time (20min+) the program will still be terminated by system


2012-09-05 (3)
--------------
goal: verify can run for long time.. (if all exceptions are caught)

ECONNRESET error (when other peer disconnects)

- seems like we can run for sometime before running into the 10055 error

    { [Error: connect Unknown system errno 10055]
      code: 'Unknown system errno 10055',
      errno: 'Unknown system errno 10055',
      syscall: 'connect' }

eventually there's the error:

    FATAL ERROR: CALL_AND_RETRY_2 Allocation failed - process out of memory

possible cause is that as TCP does not free connected sockets immediately (do so after 240 seconds),
sockets are occupied and not released, causing available sockets to be used up, if frequent connect/disconnect is used

see: http://www.proxyplus.cz/faq/articles/EN/art10002.htm



2012-09-04 (2)
--------------
goal: debug scaling connection problems
      debug EADDRINUSE 
      pinpoint EADDRINUSE place
ERCONNR	

- still cannot locate where error occurs
- possibly will change course first and develop support for pub/sub first



2012-09-03 (1)
--------------
goal: debug scaling connection problems
      debug EADDRINUSE 
      pinpoint EADDRINUSE place

- try to catch exception in 'net_nodejs.js' by warpping 'exception' & 'catch'.
  however, is there a cleaner approach?

BUG   after running 50 nodes for a while.. (using test_VON_scale.js mostly)
      a node in test_VON_scale would crash due to EADDRINUSE.

BUG   "no mapping exist for send target" (how did it happen? is it normal?)





2012-09-02 (7)
--------------
goal: debug scaling connection problems
      debug EADDRINUSE 

- found out binding to default 37700 port could fail (no response) after already binded



2012-09-01 (6)
--------------
goal: debug scaling connection problems
      debug EADDRINUSE 

process.ic      
- learn that EADDRINUSE is caused by binding to same port. might be caused by
  running out of ports

- 50 nodes on NB is fine, but not 100 nodes

2012-08-31 (5)
--------------
goal: debug scaling connection problems
      pinpoint cause of individual errors (localize it to code sections then catch it)

- appears nodes would overlap on same positions (possibly the cause for the voro site < actual nodes problem)
  


2012-08-30 (4)
--------------
goal: debug scaling connection problems


BUG run 200 nodes at localhost of windows client (crash due to error 10055). 
    many errors if running for prolonged time

    while running:
    EADDRINUSE
    no mapping exist for send target XX

    when lots of nodes disconnect:
    ECONNRESET
    ECONNREFUSED


- ports currently binded (used) for a few selected nodes: 6, 9, 24, 20 (running 50 nodes)
  so neighbor count seems to be in the 'alright' range


2012-08-29 (3)
--------------
goal: massive node movement demo
      
MILSTONE
- catch all exception errors, now can relatively stably run to 200 nodes at dev server

BUG:	appears there are problems with frequent connect/disconnect
        after a while many errors still to appear, some could crash the program



2012-08-28 (2)
--------------
goal: massive node movement demo
      

- port clusterMovement from C++ code in VAST to javascript



2012-08-27 (1)
--------------
goal: build connector to server-side vast node via websocket
      basic GUI control in browser
      debug 2nd node see only one neighbor


DEBUG 2012-08-17: vast.io server hangs after several clients join (clients > 3)

      > the enclosing neighbor not found bug is caused by having boundries set incorrectly (the top boundary should be 
      smaller than 'bottom' boundary)

DEBUG	when nodes leave, still existing nodes do not see their departure
      > two causes:
        1. removeNonOverlapped is not called  (now try to call it periodically during tick)
        2. sendBye does not work properly (when a client disconnects from vast.io server, it should
           generate a disconnect event, which should in turn cause leave() be called,
           and sendBye be called

MILESTONE 
	seems like node join / leave / neighbor discovery all works properly! ^^
        basic GUI also works (movement & display)


2012-08-26 (7)
--------------
goal: build connector to server-side vast node via websocket
      basic GUI control in browser
      debug 2nd node see only one neighbor

BUG:  seems like no halfedge is produced after 2nd node joins
      (via vast.io mechanism), so node [1] is not enclosing neighbor 
      to node [2]
      > test with test_VON_peer, appears that there's also 0 halfedge after
      2nd node joins (yet it still returns node [1] as enclosing neighbor)


2012-08-25 (6)
--------------
goal: build connector to server-side vast node via websocket
      basic GUI control in browser

- vast.io client&server function implemented



2012-08-24 (5)
--------------
goal: build connector to server-side vast node via websocket
      basic GUI control in browser
      debug VON_QUERY forwarding / neighbor discovery


DEBUG initial neighbor check by GW does not notify for enclosing neighbors to a joiner
2012-08-23: some nodes only connect with gateway
      
      > seems like the problem is caused by GW node, not able to maintain enclosing neighbor relations properly
      so when it tries to notify additional new neighbor first to a joiner, it only finds itself as being relevant

      > after some probing, seems like the EN mechanism at GW is somewhat flawed.
      where only GW itself is identified as the EN for a newly joined node. appearantly EN list is not built properly

      > it's found because the boundary range in vast_voro is set to [10000, 10000] but the acutal nodes
      move to negative coordinates, the Voronoi computation thus is only partial. The range is now enlarged to
      [-10000, 10000]. seems to work better


2012-08-23 (4)
--------------
goal: build connector to server-side vast node via websocket
      debug VON_QUERY forwarding
      basic GUI control in browser
      debug multiple logins via HTML cannot get IDs

DEBUG 2012-08-22 
     found out when using MOVE to update position to other neighbors, as 'addr' field is empty, 
     but it's also updated to other nodes, so original addr is erased. 
     solved by updating how MOVE data is being processed, so only provided fields will replace old (not empty ones)

BUGS:
  1. some nodes only learn of gateway
  2. all nodes always connect to gateway
  3. no neighbor discovery is done beyond original contact from server


2012-08-22 (3)
--------------
goal: build connector to server-side vast node via websocket
      debug VON_QUERY forwarding
      basic GUI control in browser
      debug multiple logins via HTML cannot get IDs

- seems like joining works, but nodes keep add up.. num of neighbors is not bounded

BUG: some address returned by gateway is invalid, so contacting them would fail for a joining node. 




2012-08-21 (2)
--------------
goal: build connector to server-side vast node via websocket
      debug VON_QUERY forwarding
      basic GUI control in browser
      debug multiple logins via HTML cannot get IDs

- modify join() procedure in VON_peer a bit, so that listen to 
 socket and detecting self IP are both done, before returning
 a join success


2012-08-20 (1)
--------------
goal: build connector to server-side vast node via websocket
      debug VON_QUERY forwarding
      basic GUI control in browser
      debug multiple logins via HTML cannot get IDs

BUG:
- appears stuck at when 4th node joins, some messages are passed from gateway to other exiting nodes (but fail). However, for some reasons these traffics are blocked so can't tell. 


2012-08-19 (7)
--------------
goal: build connector to server-side vast node via websocket
      debug VON_QUERY forwarding
      basic GUI control in browser
      debug multiple logins via HTML cannot get IDs

- still tracing multiple logins, appears 1st node is correct, onto 2nd node.. 



2012-08-18 (6)
--------------
goal: build connector to server-side vast node via websocket
      debug VON_QUERY forwarding
      basic GUI control in browser
      debug multiple logins via HTML cannot get IDs

- tracing join procedure to discover problem


2012-08-17 (5)
--------------
goal: build connector to server-side vast node via websocket
      debug VON_QUERY forwarding
      basic GUI control in browser
      debug multiple via HTML cannot get IDs

- change join procedure a bit, HTML client only specify vast.io server's IP & port to connect
  whereas within the vast.io server, the VON gateway IP/port is specified

BUG  after 3 clients, vast.io server appear to be frozen.



2012-08-16 (4)
--------------
goal: build connector to server-side vast node via websocket
      debug VON_QUERY forwarding
      implement client-side vast binding (socket.io based API)
      basic GUI control in browser
      socket.io-style vast.io interface

- connect through HTML -> vast.io -> socket.io VAST node server -> VON peer 
  and get self ID

BUG: if multiple webpages are opened, seems like it gets messed up,
     additional client IDs can be assigned, but not always... 


2012-08-15 (3)
--------------
goal: build connector to server-side vast node via websocket
      debug VON_QUERY forwarding
      implement client-side vast binding (socket.io based API)
      basic GUI control in browser

- try to imitate socket.io's implementation to produce vast.io
- study difference between module.exports & exports in node.js
  

2012-08-14 (2)
--------------
goal: build connector to server-side vast node via websocket
      implement client-side vast binding
      debug VON_QUERY forwarding

- make vast_voro & LOG work under both browser & node.js environment
- setup GUI environment to integrate with socket.io client-side API


2012-08-13 (1)
--------------
goal: build connector to server-side vast node via websocket
      implement client-side vast binding
      debug VON_QUERY forwarding
      self connection as gateway

- self connection as gateway ok
- finish TODO:
  * remove 'id' 'VoronoId' for node.aoi.center when sending outbound messages


2012-08-12 (7)
--------------
goal: build connector to server-side vast node via websocket
      implement client-side vast binding
      debug VON_QUERY forwarding
      self connection as gateway



2012-08-11 (6)
--------------
goal: build connector to server-side vast node via websocket
      implement client-side vast binding
      debug VON_QUERY forwarding

- try to allow client connect to itself as gateway if it's the first
  node to connect

2012-08-10 (5)
--------------
goal: build connector to server-side vast node via websocket
      implement client-side vast binding

- multiple browser clients can now login and create VON peers

BUG
- semes like VON_QUERY forwarding is not done correctly,
  new nodes aren't getting the VON_NODE messages they need to confirm
  joining.

DEBUG
2012-08-09: cannot bind port continously in VONpeer
            main problem is that only net_nodejs can report bind success/failure
            but makes more sense to do re-try in vast_net
            so need to be careful of who does what


2012-08-09 (4)
--------------
goal: build connector to server-side vast node via websocket
      implement client-side vast binding

BUG
- VON peer cannot attempt to bind ports continously (event mishandled)


2012-08-08 (3)
--------------
goal: build connector to server-side vast node via websocket
      implement server-side vast functions
      implement client-side vast binding

- finish server implement
- rough client version, tested joining success

BUG
- on second join will join as server still, causing binding to fail & server crash



2012-08-07 (2)
--------------
goal: build connector to server-side vast node via websocket
      implement server-side vast functions

- implement socket.io server join/leave/move functions



2012-08-06 (1)
--------------
goal: build connector to server-side vast node via websocket

- build a socket.io + VON_peer test client, also define events the client will send
  to this "connector" in order to join a VON.

- add continous port re-binding until successful to vast_net
- add return true/false when listening to port for net_nodejs


2012-08-05 (7)
--------------
goal: build websocket test sample

- run sample socket.io successful, can now run server with node.js @ Windows
- can also send custom message back & forth (sending a message + a js object)


2012-08-04 (6)
--------------
goal: porting: VON_peer.js 
      test _checkNeighborDiscovery()
      test _removeNonOverlapped()

- study how to use websocket


2012-08-03 (5)
--------------
goal: porting: VON_peer.js 
      test _checkNeighborDiscovery()
      _removeNonOverlapped()

- implement all VON functions (pending tests)


2012-08-02 (4)
--------------
goal: porting: VON_peer.js 
      VON_MOVE processing
      _checkNeighborDiscovery()
      _removeNonOverlapped()

- notice that when the network has just few nodes, movements are sent via MOVE (not MOVE_B), so meaning that
  not explicitly neighbor discovery check will be performed. Need to confirm is this ok behavior. 
  (possibly okay, as EN will suffice to discovery, only later will boundary neighbor be important as node size increases)

- do not use 'typeof' to check whether a js object has certain thing (in order to further decode)
  use 'hasOwnProperty' to check

- implement checkNeighborDiscovery() (pending test)




2012-08-01 (3)
--------------
goal: porting: VON_peer.js 
      VON_MOVE
      _checkNeighborDiscovery()
      _removeNonOverlapped()

- make sure a remote disconnect will generate a proper VON_DISCONNECT
- finish implement move()
- review & move all C++ codebase to js (but comment out non-functional parts)

      

2012-07-31 (2)
--------------
goal: porting: VON_peer.js 
      debug same pos no EN issue (still need to join)

DEBUG
2012-07-30: if two nodes have same position, raymond hill's voronoi produce site for just one node
            this impacts neighbor discovery correctness

- solution: send VON_HELLO & VON_EN separately, so a new node still sends HELLO
  even if its EN list for the remote node is empty (can happen if both 
  nodes occupy the same position, one of them won't get recorded into 
  the voronoi object, causing inproper judgment)

- VON_EN works


2012-07-30 (1)
--------------
goal: porting: VON_peer.js 
      test leave(), 
      debug VON_HELLO
      (get_en does not return any neighbor when the first client node joins)

BUG: 
IMPORTANT NOTE:
- found out if two sites are located at the exact same place,
  under Raymond Hill's implementation, only one 'cell' will result.
  this may have implication for the join procedure, as when a node result its initial
  neighbor list via VON_NODE, if just two nodes exist and occupy the same location
  only one cell exist and no enclosing neighbor are there. therefore the 
  2nd joining node will not be able to contact the first gateway for join 
  
- implemented VON_HELLO, VON_HELLO_R, VON_BYE
- leave works properly  


2012-07-29 (7)
--------------
goal: porting: VON_peer.js 
      test leave(), 
      debug VON_HELLO

- modify vast_voro internals so that external queries/checks (get_en, 
  contains, overlaps) will perform recompute() first and check for
  node id correctness, before moving further. 



2012-07-28 (6)
--------------
goal: porting: VON_peer.js 
      test leave(), 
      debug VON_HELLO

- try to debug get_en() does not return as expected (mismatch between what's passed
  and what's stored: string vs. number) 
  learn that storing things as keys in a {} map will turn key into 'string'
2012-07-27 (5)
--------------
goal: porting: VON_peer.js 
      test leave(), 
      process _new_neighbors array
      debug leave undefined target (implement tick())

- implement checkNewNeighbors

BUG
- VON_HELLO does not seem to work correctly when discovering new neighbors



2012-07-26 (4)
--------------
goal: porting: VON_peer.js 
      test leave(), 
      process _new_neighbors array

- implement init data structure, leave()
- make leave work & send neighbors VON_BYE
- right now new neighbors are not processed for VON_NODE, need to fix this.


2012-07-25 (3)
--------------
goal: porting: VON_peer.js 
      test leave(), 
      allow assignment of new id from gateway
      process _new_neighbors array

- connect success callback added
- change test client to connect to remote gateway
- can get new ID from gateway, at both network layer & VONpeer
- add getID() function to get selfID at vast_net



2012-07-24 (2)
--------------
goal: porting: VON_peer.js 
      test join(), leave(), 
      implement VON_QUERY response
      allow assignment of new id from gateway

- join done
- implemented VON_NODE (processing new_neighbors pending)
- add new VAST type: ratio

2012-07-23 (1)
--------------
goal: porting: VON_peer.js 
      test join(), leave(), 
      implement VON_QUERY response
      allow assignment of new id from gateway

- sendNodes seems to work :)
- allow VAST.pos to inherent from point2d (to have 'distance' function)

BUG:
- isRelevantNeighbor seems to produce different results if overlap
  test is chosen to be 'accurate' vs. 'non-accurate'. with the 'accurate' 
  result not passing overlap test... (a bit strange because a site's position 
  should overlap with its region).


2012-07-22 (7)
--------------
goal: porting: VON_peer.js 
      test join(), leave(), 
      implement VON_QUERY response
      allow assignment of new id from gateway

- done porting helpers:
  _sendEN
  _sendHello
  _sendNodes


2012-07-21 (6)
--------------
goal: porting: VON_peer.js 
      test join(), leave(), 
      implement VON_QUERY response
      can determine node id of the remote node sending the message (network-layer?)

- now when init connection to remote node, connecting node will send its 'id'
  first as handshake. the listening node can then learn of the connecting node's id.
  however, if no remote id is provided (0, or VAST_ID_UNASSIGNED), then listening 
  node will assign internal IDs (-1, -2, -3...) to send back messages


2012-07-20 (5)
--------------
goal: porting: VON_peer.js 
      test join(), leave(), 
      implement VON_QUERY response
      can determine node id of the remote node sending the message (network-layer?)

- exchange node id when connecting (in progress)




2012-07-19 (4)
--------------
goal: porting: VON_peer.js 
      test join(), leave(), implement VON_QUERY response

- finished implement VON_QUERY
- BUG: now enters infinite loop (forwarding request to another node)
  bug occurs because from_id is [-1] (cannot yet see remote node's self id)



pending: need to implement:

_isRelevantNeighbor
_isTimelyNeighbor
_sendNodes
_aoi_buffer



2012-07-18 (3)
--------------
goal: porting: VON_peer.js 
      test join(), leave(), implement VON_QUERY response

rename:
 IPaddr to addr   (vast_addr)
 addr to endpoint (vast_endpt)
 g_log to LOG

- implement insertNode() (still in progress)


2012-07-17 (2)
--------------
goal: porting: VON_peer.js 
      test join(), leave(), implement VON_QUERY response

- make deserialize into VAST objects work for all basic types


2012-07-16 (1)
--------------
goal: porting: VON_peer.js 
      test join(), leave()

- test out join's send msg to server successful
- in process of implementing VON_QUERY's server response


2012-07-15 (7)
--------------
goal: porting: VON_peer.js 
      join(), leave()

- implement join() & send message within VON_peer


2012-07-14 (6)
--------------
goal: porting: VON_peer.js 
      join(), leave()

- add 'disconnect' to vast_net

BUGFIX:
2012-07-11: vast_nodejs cannot notify for client socket disconnection (can't identify which)



2012-07-13 (5)
--------------
goal: test refactored net_nodejs

- makes it work (vast_net)
- a problem is that if connection breaks using 'disconnect', 
  two disconnect events are fired ('end' and 'close')
  but if the connection is broken by force (via Ctrl-C) only one disconnect is fired
  (only 'close' is fired but not 'end')


2012-07-12 (4)
--------------
goal: porting: VON_peer.js 
      join(), leave()

- start revising net_nodejs internal implementation (refactor to use less code)


2012-07-11 (3)
--------------
goal: porting: VON_peer.js 
      join(), leave()


- extract common data types as a global VAST object (vast_types.js)
- remove requirement to provide a network layer when creating VON_peer

BUG:
- found out vast_nodejs doesn't seem to be able to notify client socket 
  disconnection for a listening server



2012-07-10 (2)
--------------
goal: porting: VON_peer.js 
      join(), leave()

- a first version, very simple example case of using VAST & VONpeer, test_VON_peer.js
  (all includes are available once 'common.js' is required)



2012-07-09 (1)
--------------
goal: porting: VON_peer.js 
      join(), leave()

- write constructor test case
- define some common data structures (area, addr, pos, IPaddr, node)


2012-07-08 (7)
--------------
goal: implement VON_peer.js 

- done constructor & VON message enumation


2012-07-07 (6)
--------------
goal: define VON_peer.js interface

- done initial version


2012-07-06 (5)
--------------
goal: define vast.js interface

- to design: whether world_id should be inherent to a given gateway
  (that is, no need to assign it when joining, the gateway simply setup
   a unique world_id)

- finished defining interface

2012-07-05 (4)
--------------
goal: implement vast_net.js 
      test send() & register()

- remove register(), put as part of constructor

MILESTONE:
finish first version of vast_net, supporting socket send & listen
      

2012-07-04 (3)
--------------
goal: implement vast_net.js (major net-related functions required by VAST)

- implemented send(), register(), storeMapping()


2012-07-03 (2)
--------------
goal: implement vast_net.js (major net-related functions required by VAST)

- define vast_net's interface


2012-07-02 (1)
--------------
goal: debug net_nodejs.js

- finished :)


2012-07-01 (7)
--------------
goal: implement working net_nodejs.js

- finish 1st implementation (buggy & not yet working)


2012-06-30 (6)
--------------
goal: implement working net_nodejs.js

- layout net_nodejs main functions, ready to implement


2012-06-29 (5)
--------------
goal: layout basic functions / interface for vast_net 
     (based on c++ version)

- orgnaized some functions from VASTnet in VAST C++



2012-06-28 (4)
--------------
goal: finish graphical demo for vast_voro.js (boundary neighbors, AOI-radius)

MILESTONE:
- done ^^
- can display enclosing & boundary neighbors (correctly & switchable)



2012-06-27 (3)
--------------
goal: provide a graphical demo of the new vast_voro.js

- can now detect which id the mouse covers
- can display enclosing neighbors ^^


2012-06-26 (2)
--------------
goal: provide a graphical demo of the new vast_voro.js

- convert test_voro_rh to use vast_voro.js (works)
- able to display mouse position 


2012-06-25 (1)
--------------
goal: rh_voronoi: port closest_to() and rest

- done ^^

MILESTONE:
all vast_voro.js methods have been ported with rh_voronoi as the underlying mechanism :)


2012-06-24 (7)
--------------
goal: rh_voronoi: port overlaps
      debug 2012-06-23 bug: 1st & 2nd level enclosing neighbors are the same

- done (but havn't checked visually)
- appears that accurate & non-accurate mode of overlap check
  produces very different results (so should use accurate mode)

- also separate each unit test into function forms


2012-06-23 (6)
--------------
goal: rh_voronoi: port get_en()

- done (for 1st level). also optimizes to use simpler procedure

BUG
- but when going to 2nd level enclosing neighbor, sometimes
  both 1st & 2nd level neighbors are the same set.


2012-06-22 (5)
--------------
goal: rh_voronoi: port is_enclosing() & get_en()

- is_enclosing() appears to work correctly :) after calling
  getNeighborSet() directly


2012-06-21 (4)
--------------
goal: port is_boundary() in rh_voronoi


- is_boundary() and enclosed() and both compile and run correctly
- however, found that simply !enclosed() does not necessarily
  produce a 'boundary neighbor" 
- need other solutions to identify / calculate a boundary neighbor


2012-06-20 (3)
--------------
goal: debug contains() in rh_voronoi

- confirm that the halfedges sent within 'result' of the rh_ovoronoi class,
simply are "edges" forming a circle around thep (with bounding boxes as the end of edges)

- done insideRegion() ^^
- replace with a correct & possibly faster algorihtm from:

http://www.ecse.rpi.edu/Homepages/wrf/Research/Short_Notes/pnpoly.html

2012-06-19 (2)
--------------
goal: debug contains() in rh_voronoi

- check insideRegion() against java version, seems to be the same
- stuck at not knowing how to solve insideRegion()'s correctness


2012-06-18 (1)
--------------
goal: debug contains() in sf_voronoi

line general form: 
ax + by + c = 0

slope-intercept form:
y = mx + b

givn two points (x1, y1) (x2, y2)
slope:
m = (y2-y1) / (x2-x1)

intercept:
(y2-y1)/(x2-x1)*x1 + b = y1

y = y1 + [(y2 - y1) / (x2 - x1)]¡P(x - x1)
(x2 - x1)¡P(y - y1) = (y2 - y1)¡P(x - x1)

(x2¡Py - x1¡Py) - (x2¡Py1 - x1¡Py1) = (x¡Py2 - x¡Py1) - (x1¡Py2-x1¡Py1)
x2¡Py - x1¡Py - x2¡Py1 + x1¡Py1 = x¡Py2 - x¡Py1 - x1¡Py2 + x1¡Py1
(y2-y1)x + (x1-x2)y + (x2¡Py1 - x1¡Py1 - x1¡Py2 + x1¡Py1) = 0

a = (y2-y1)
b = (x1-x2)
c = (x2¡Py1 - x1¡Py1 - x1¡Py2 + x1¡Py1)

- contains() work with rh_voronoi, but still buggy 
  (check returns 'true' very often)


2012-06-17 (7)
--------------
goal: modify vast_voro to work with sf_voronoi.js

- finished converting contains & insideRegion (runnable) but buggy 
  (edges surrounding a cell appears to be all empty)


2012-06-16 (6)
--------------
goal: modify vast_voro to work with commonly exposed Voronoi data structures (esp. from rh_voronoi)
      first to work with SFVoronoi

- got sorted sites to expose in vast_voro (from both sf_voronoi & rh_voronoi)
- started to convert contains() (still has errors)


2012-06-15 (5)
--------------
goal: modify vast_voro to work with commonly exposed Voronoi data structures (esp. from rh_voronoi)

RBTree methods:

rbInsertSuccessor(node, successor)
rbRemoveNode(node)
rbRotateLeft(node)
rbRotateRight(node)
getFirst(node)
getLast(node)

- calculating basic Voronoi ok by using vast_voro.js


2012-06-14 (4)
--------------
goal: modify vast_voro to work with commonly exposed Voronoi data structures

- found out seg and vertexIndex (used by elements in mEdges) are actually defined & stored in 'line2d' data structure



2012-06-13 (3)
--------------
goal: make vast_voro work with sf_voronoi

- note what is required from sf_voronoi (used by vast_voro)

compute(sites, bbox)    calculates Voronoi diagram, returns result with edges & cells
get_idx(id)             get the index in mSites given an id of the site

mSites                  list of voronoi sites (sorted) 
    site_num            id of the site (user-specified ID)
    coord               position of site
    edge_idxlist        list of id's of today's working

mEdges
    a,b,c               parameters to define a line (edge)
    vertexIndex         two vertices forming the edge, used by collides()
    seg.intersect
    bisectingID
	
mVertices
result                  contains a list of calculated results (sorted sites & drawn edges)


2012-06-12 (2)
--------------
goal: separate vast_voro from sf_voronoi

DEBUG: executing functions from sf_voronoi.js from vast_voro.js
       causes the required script to be printed out

      > need to 'new' the object before use (otherwise it's treated
        as a simple script file)


2012-06-11 (1)
--------------
goal: separate vast_voro from sf_voronoi

DEBUG: infinite loop was caused by min/max x & y values not assigned correctly

- done 


2012-06-10 (7)
--------------
goal: separate vast_voro from sf_voronoi

- revise sf_voronoi.js to behave same as rhill-voronoi-core.js
  but when calculating Voronoi, program hangs (infinite loop)


2012-06-09 (6)
--------------
goal: separate vast_voro from sf_voronoi

supported functions:

insert(id, pos)	
remove(id)
update(id, pos)
get(id)
clear()

contains(id, pos)               --> v insideRegion(index, p)
is_boundary(id, pos, radius)    --> v enclosed()
is_enclosing(id, center_id)     --> get_en()
get_en()                        --> v getNeighborSet()
overlaps(id, pos, radius, mode) --> v collides()

needed functions from sf_voronoi (to be used within compute())

clear()
insert()
getedges() or recompute() & access to mEdges





2012-06-08 (5)
--------------
goal: adopt sf_voronoi to use test_voro_html5.html

- done ^^

2012-06-06 (3)
--------------
goal: test applicability of raymond hill's voronoi implemention to VAST


raymond hill functions:

var sites = [{x:300,y:300}, {x:100,y:100}, {x:200,y:500}, {x:250,y:450}, {x:600,y:150}];
var bbox = {xl:0, xr:800, yt:0, yb:600};
result = voronoi.compute(sites, bbox);

result.edges
result.cells
result.execTime

edge.lSite
edge.rSite
edge.va
edge.vb



needed functions:

v insert(id, pos)	
v remove(id)
v update(id, pos)
v get(id)

contains(id, pos)				--> insideRegion(index, p)
is_boundary(id, pos, radius)	--> enclosed()
is_enclosing(id, center_id)		--> get_en()
get_en(id, level)				--> getNeighborSet()
overlaps(id, pos, radius, mode)	--> collidess
clear

v closest_to(pos)
getedges()
get_bounding_box()
get_sites()
get_site_edges(id)
getstat()




2012-06-05 (2)
--------------
goal: convert a generic net layer 

- make sfvoronoi.js usable under both web & node.js


2012-06-04 (1)
--------------
goal: make web demo

[Milestone]

- finished rough web demo with performance stat record:

  100 sites   	20 - 30ms
  1000 sites	220 - 240ms
  10000 sites	1400ms

in comparison, raymond hill's version:

  100 sites   	< 10ms
  1000 sites	30 - 80ms
  10000 sites	300ms



2012-06-01 (5)
--------------
goal: debug get empty edges problem

DEBUG:	found out it's caused by accessing public variables this.mEdges with "this.mEdges"
        (should use l_mEdges when accessing public variables from private functions)




2012-05-31 (4)
--------------
goal: check sfvoronoi.js against java version


MILESTONE: runnable for 1000 sites! :)

DEBUG:	found out some inconsistent assignment between 'null' vs 'undefined' (should use 'null' for some)
BUG: 	edges are calculated but become empty when retrived later.




2012-05-30 (3)
--------------
goal: check sfvoronoi.js against java version

- found bug! 
  after ELhash is initialized with 'null' crash bug disappear
  but appears again if Halfedge is init with certain values as default

- check till ELinsert()


2012-05-29 (2)
--------------
goal: check sfvoronoi.js against java version

- found bug in Java version!

 in function:
  private void clip_line (Edge e) 

  y1 = e . c - e . a * x1;

should be:

  y1 = e.c - e.a * x1;


- check through clip_line()



2012-05-28 (1)
--------------
goal: check sfvoronoi.js against java version

- check till intersect()


2012-05-27 (7)
--------------
goal: check sfvoronoi.js against java version

- handle small bug in getNeighborSet (access to 'edge' is incorrectly placed)
- recompute(), insideRegion()

2012-05-26 (6)
--------------
goal: check sfvoronoi.js against java version

- is_enclosing(), get_en()


2012-05-25 (5)
--------------
goal: check sfvoronoi.js against java version

- is_boundary() done


2012-05-24 (4)
--------------
goal: check sfvoronoi.js against java version

- found the 'sites' data struct should contain 'coord' parameter
  but currently it's just a 2D pos (but it's ok?)

2012-05-23 (3)
--------------
goal: check sfvoronoi.js against java version

- done through intersect()



2012-05-20 (7)
--------------
goal: debug sfvoronoi.js

DEBUG:	when calling nextone()
        it should return the sorted sites (from mSites) but currently the 
        code uses th elements from 'sites' (originally inserted, unsorted).

        also address another potential bug, where the idx2id structure was
        used incorrectly (a Hash object, but uses [] to access directly elements)
        should use .set .get.. 
        > fix this by changing to use js array [] directly


2012-05-19 (6)
--------------
goal: debug sfvoronoi.js

- check whether the == check for halfedge object indeeds checks for equivalance.
  found out it's okay. will now need to trace from start and see what's causing the
  'null' ELedge


2012-05-17 (4)
--------------
goal: debug sfvoronoi.sf

- modify sites to use just plain javascript {} instead of custom Hash object
  (tested ok, but the null ELedge bug still exists)
   

2012-02-21 (2)
--------------
goal: debug sfvoronoi.sf

MILESTONE: solve a big sfvoronoi.js bug (lasting 4 weeks!)
DEBUG: some pointers appear to be null (cannot access its child members)
       > found out it's because the DELETED marker (an Edge object) was changed to null
       > but it's actually used for some purpose, so still needs to be in place




2011-10-24 (1)
--------------
goal: re-build VAST on Linux (redhat)

got VAST & ACE (6.0.0 download to test machine), had problem building ACE:

/usr/bin/ld: .shobj/Local_Name_Space.o: relocation R_X86_64_32 against `std::nothrow' can not be used when making a shared object; recompile with -fPIC
.shobj/Local_Name_Space.o: could not read symbols: Bad value



        
2011-07-26 (2)
--------------
partitioning methods & approaches

resizable cells - triangle strips, Voronoi
split-merge - quadtree 
micro-cell - squares / triangles


VAST.js Goals
------------
Clean, Fast, Solid

Support Commercial-grade Network Support
- network monitoring
- transaction logging
- server validation
- secure transaction
- complete knowledge of operations
- comparable user experience
- easy to integrate
- switch between P2P & C/S mode



2011/05/24 (4)
--------------
goal: allow server to listen to port and accept multiple incoming connections

- allow server to auto-look for available ports after a given port number



2011/05/04 (3)
--------------
goal: define & implement network-layer API 

* what's needed in a 1st version?

- GUI display to see nodes / Voronoi / movement
- basic VON functions (join / sub / move / list)
- stat collection (for feedback / performance evaluation)

* considerations:

- should also consider the ease of porting to other environments (server/web client/Unity client)

most basic (GenericNetwork)
socket 
  connect (addr)
  send (msg, size)  
  disconnect (id)
  
callback  
  received (msg, size)
  disconnected (id)
  
* first target:
a client to establish TCP/IP connection 



2011/04/21 (4)
--------------
goal: start VAST.js project :) define network layer

Needed functions for network layer. Also need to assume ID assignment mechanism exists.

- store ID-to-addr mapping
- VON    send/recv to target
- socket send/recv to target
- get time
- get ID
- get states
- get stats



// basics
start()
stop()
storeMapping(in: id, Addr)
sendMessage(in: target, msg, reliable, headertype)
receiveMessage(out: host, msg)
flush()
process()

// sockets
openSocket
sendSocket
receiveSocket

getTimestamp
getIPfromHost

// state query
isJoined
isPublic
isEntry
isConnected

// tools
validateIPAddress
validateConnection
tickLogicalClock

// get/set
setTimestampAdjustment
setBandwidthLimit
getTimestampPerSec
getConnections
addEntries
getEntries
getAddress
getHostAddress
getUniqueID
getHostID

// stat
getSendSize
getRecvSize
resetTransmissionSize
recordLocalTarget

           
    
2011/03/21 (1)
--------------
goal: build VAST with SSL support

Official Instructions:
http://www.dre.vanderbilt.edu/~schmidt/DOC_ROOT/ACE/ACE-INSTALL.html#sslinstall

Here's the Linux instructions:
from: http://groups.yahoo.com/group/tao-users/message/19352

1) Create $ACE_ROOT/bin/MakeProjectCreator/config/default.features with
the following contents:

ssl = 1

2) cd $TAO_ROOT; $ACE_ROOT/bin/mwc.pl TAOACE.mwc
3) Add "ssl = 1" to $ACE_ROOT/include/makeinclude/platform_macros.GNU.
4) Run make from $TAO_ROOT.

For Windows:
1) Create $ACE_ROOT/bin/MakeProjectCreator/config/default.features with
the following contents:

ssl = 1

2) Edit $ACE_ROOT/bin/MakeProjectCreator/config/MPC.cfg 
   with following lines only:
    
    dynamic_types = $ACE_ROOT/bin/MakeProjectCreator, $?DDS_ROOT/MPC, $?TAO_ROOT/MPC
    main_functions = cplusplus:ACE_TMAIN
    default_type = vc10   
   
3) cd $ACE_ROOT/ace
   run $ACE_ROOT/bin/mwc.pl ace.mwc

4) open the following with VC10 and build project "SSL"
   $ACE_ROOT/ace/ace.sln
           
Make sure OpenSSL build libraries are installed properly first, can be downloaded here:
http://www.slproweb.com/products/Win32OpenSSL.html

Note: download the non-light (full) version, so build libraries are included    
    
    
2011/01/05 (3)
--------------
goal: add matcher promotion according to loading

to-add:
- load detection
- load reporting
- matcher selection
    
    
2010/12/28 (2)
--------------
goal: upgrade ACE in VAST & build with VC 2010

BUG     many ACE_OS functions (sleep, hostname...) become unavailable in ACE 6.0.0 (appears that OS.h no longer exists)

- after searching for functions directly in the ACE source code, found that    
need to change include "OS.h" to the following:

#include "ace/OS_NS_unistd.h"       // ACE_OS::sleep
#include "ace/OS_NS_netdb.h"        // gethostbyname
#include "ace/OS_NS_arpa_inet.h"    // inet_ntoa

    
    
2010/12/09 (4) 
--------------
goal: sync clocks to gateway for all regular clients
        
    
    
2010/11/23 (2)
--------------
goal: sync clocks to gateway for all regular clients

   
2010/11/03 (3)
--------------
goal: use VAST socket to send/recv JSON BCNet messages

Milestone:
- get VAST socket working, can establish connection with gateway & send, also receive reply, without 
  having to first create a VAST node (only VASTnet is needed)
    
    
2010/10/08 (5)
--------------
goal: refactor VASTnet
    
- try to figure out if there's potential thread conflict between incoming / outgoing messages
two levels need to consider: 

1) message queue: inserting and removal of messages of mesage queue, should not affect each other
2) connections: when connecting / disconnecting, the two should not overlap each other
    
consider cases:

1) send & remote_disconnect occurs at the same time -> access to connection object needs mutex
   to avoid trying to send, but connection is already invalid

2) recv & local disconnect occur at same time -> no overlap data structure (should be ok?)

3) send & local disconnect -> will happen in same thread (no problem)

4) recv & remote_disconnect -> in same thread (no problem)

5) disconnect & remote disconnect occur at same time -> access to connection object & handler may conflict

6) connect & remote connect occur at same time -> duplication connections may result (but will be protected
   because socket_connected () & socket_disconnected () will check for duplication
        

2010/10/26 (2)
--------------
goal: VASTnet refactored & tested

MILESTONE: 1st refactored VASTnet up & running.

mostly working alright, using VLC found 28 memory leaks, but all are due to use of ACE (reason unknown)

        
2010/10/07 (4)
--------------
goal: refactor VASTnet

- change definition of timestamp_t from uint32_t to uint64_t, also net_ace's getTimestamp ()
  returns millisecond time since 1970, instead of the program start, this is so that 
    * times on synchronized hosts can be somehow compared
    * systems running VAST do not require reboot every so often (with uint32_t, can only store millisecond for 50 days)
      now it's like 584942417 years :)
    
    
2010/09/21 (2)
--------------
goal: implement BCNet API based on JSON

- install JSON with the following steps
    * get jsoncpp from http://jsoncpp.sourceforge.net/
    * extract into a path parallel to VAST
    * under /makefile/vs71, run jsoncpp.sln and upgrade to VC2008
    * test build lib_json
    * include project 'lib_json' into BCNet's solution space
    * set lib_json's properties Use of MFC as: "Use MFC in a Shared DLL"
    * add to BCNet's include path: "..\jsoncpp-src-0.5.0\include"
    * add #include <json/json.h> to BCNet.cpp
    * add "Json::Value root;" to BCNet.cpp to test creation of JSON objects

    
      
      
2010/09/14 (2)
--------------
goal: help integrate BCNet with game client


DEBUG   gateway's response cannot reach the client
        > caused by the message is sent to the client's relay component, but target is 'host_id' instead of 'sub_id'
        > added a "self-check", if the send target is my hostID then automatically translate

BUG     gateway leaves sometimes will crash (seems like occurs in receiveMessasge for processing, invalid message
        is being processed)
        
        
        
2010/09/14 (2)
--------------
goal: prepare BCNet wrapper around VAST for use by BlackCat client

- compiled the class, also wrote a simple demo_console

BUG     if table server is connected immediately when gatewayConnected () is called, it's okay,
        but if the joining to table server is triggered by keypress, it hangs in JOINING stage
        

DEBUG   after BCNet is ready & run with demo_console for 1st time, we got "R6025 pure virtual function call"
        > found out it's because the callback class was defined as local variable in init () (should be global)
          otherwise the instance is destroyed when init () finish execution.


DEBUG     the BCNet project does not produce BCNet.lib and demo_console could not compile
        > the BCNet class was not decleared EXPORT so no lib needs to be produced.
      
- found out getIPFromHost () in VASTnet would create multiple additional threads      
    * when calling ACE_OS::hostname () or ACE_OS::gethostbyname ()
    * when calling ACE_DEBUG for 1st time
    * when listening for TCP port (net_ace_acceptor)
    * when listening for UDP port (net_ace_handler)
        
2010/09/11 (6)
--------------
goal: separate join procedure into 1) connect to gateway and 2) connect to origin matcher of a particular world


      
      
2010/09/10 (5)
--------------
goal: run VAST in separate thread, define preliminary game API (on top of VAST)

- VASTThread done, now we can do ticking in separate thread



      
2010/09/08 (3)
--------------
goal: run VAST in separate thread, define preliminary game API (on top of VAST)

BUG:    seems like if origin matcher was temporily suspended, restoring it will cause various issues
        > possible solution is to let all matchers (active / candidate / origin) continously update their status
        > to gateway
      
2010/09/07 (2)
--------------
goal: run VAST in separate thread, define preliminary game API (on top of VAST)


      
2010/09/02 (4)
--------------
goal: build sample program for event-driven VAST

- simplify 'demo_console' from 'test_console'

- extend report () into reportGateway () & reportOrigin ()

BUG     tick per second in demo_console is only 16 (should be 20, given the setting)

           
      
2010/08/20 (5)
--------------
goal: externalize VSO backup matcher maintain

- finish merging matcher pool for VSOPeer & VASTMatcher, now all matchers are centrally managed by 
  the gateway node (VSOPeer only queries for new matcher from the VASTMatcher component at the gateway)
  
BUG     appears that the neighbor list is not up-to-date after Matcher crash & take-over 
        (if no movement, then late joiner will not know existing node, or deleted nodes unremoved?)



2010/08/16 (1)
-------------
goal: externalize VSO backup matcher maintain


BUG     after origin matcher crash, some issues observed:
        - some clients keep staying in state = JOINING and cannot re-join successfully
        - clients seem to be able to remove crashed origin matcher (neighbor list unaffected)
            but neighbors are updated only when the client tries to contact the failed matcher.
            and after failing, the matcher info is removed
          
- need to distinguish between join tasks that only need to do ONCE (creating the object, init variables),
  and those that may suceed then fail, and could be performed multiple times during the life of the program
  (detecting & finding relays, send subscribe request to matcher and get response correctly).
  The former can be done once, the latter may need repeated checking & fallback mechanism

  

      
2010/08/13 (5)
-------------
goal: stress test of multiple rooms

- perform comparison test as follows:

condition:
gateway & origin on separate nodes
run time: 6000 steps ~ 10min
join interval: 2 sec. 

* reference run (single room)
100 nodes in single world
runtime ~ 13min
max concurrent  84
leave recorded: 85

* multiple rooms run
100 nodes in groups of 4 (25 worlds)
runtime ~ 13:20
max concurrent  97
join recorded: 345
leave recorded: 96

basic findings: running in multiple rooms improve max concurrent, though still some nodes appear
to be unable to join. Client crash however has improved (non crashed, appearantly). but some 
may fail to join.

ToDo:
- test ghost client removal in origin matcher fail (backup takes over) case
- avoid client crash
- ensure client always join successfully if gateway still alive
- gateway / matcher can still function properly after temp network problems
- decentralize VSO gateway function


2010/08/12 (4)
--------------
goal: remove client ghosts

BUG     if matcher is not contactable, a joining client may be in JOINING state indefintely
        (should somehow switch mode and re-request from gateway, for example)
        
DEBUG   if a matcher loses origin matcher status due to inactivity, it may re-send keepalive
        but its knowledge about origin matcher is still itself, thus may replace the gateway's record,
        and make the world to matcher_id id & matchers info inconsistent
        > forcefully notify the previous origin that it's not longer the origin
          also, would always send a MATCHER_INIT message to either a promoted or demoted origin

DEBUG   client may not be able to send to gateway for any report, if connection is removed
        > added check to re-notify the gateway's address, in case of a disconnected gateway
        
MAJOR        
BUG     when the origin matcher is replaced, VSO node still hasn't learned of it, so may when it tries
        to promote a new node, the request may be sent to a gateway no longer exist (the departed origin matcher)
        somehow the VSO node's idea for gateway should also update, in case of change.

BUG     if a matcher has lost connections to all nodes then resumes it, it tries to re-join to gateway,
        but by that time gateway has removed its matcher as well as candidate matcher record, casuing the
        matcher unable to join.

- found that two major cases of fault tolerance needs to be considered: 
    1) when nodes permanently fail
    2) when nodes temporaily lose network connection, but then resume
 
 
2010/08/10 (2)
--------------
goal: debug matcher fault tolerance

BUG     when origin matchers fail, it seems the gateway will lose the origin matcher record for 
        the failed matcher's world ID. subsequent JOIN request to gateway thus will be incorrect

       
	  
2010/08/08 (7)
--------------
goal: debug matcher fault tolerance

DEBUG  	found that the matcher keepalive check would remove nonresponding matchers,
        but subsequent MATCHER_ALIVE would not restore it.                
		> solved by combining MATCHER_ALIVE with MATCHER_JOINED and remove MATCHER_JOINED, so 
        each time a keepalive is sent, the effect is similiar to notifying a new matcher
        preliminary testing on fault tolerance seems to work okay, except the following bug

BUG     when current matcher disconnects, a client's view of neighbors could also be incorrect
        (as no matcher can notify for neighbor deletion)
        
              
2010/07/29 (4)
--------------
goal: debug matcher fault tolerance
      
DEBUG   when a origin matcher fails, the backup matcher cannot take over
        - sendGatewayMessage in VASTClient would attache sub_id as from field, when gateway tries to 
          send the new origin matcher info to a VASTClient that sends a JOIN request, as the gateway
          doesn't have subID -> hostID mapping, it delivers the NOTIFY_MATCHER to itself
        > solved by removing the auto-pending of subID to the client messages' from field

      
      
2010/07/22 (4)
--------------
goal: implement dynamic origin matchers

- the implement direction is to make VSOPeer stay the same as much as possible, while allowing VASTMatcher
  to also manage / handle dynamic matcher joining
  
- three peer roles in VSO:
    * gateway   (records all candidates and choose candidate node to join upon request)
    * origin    (entry point to a particular VSO space
    * peer      (a regular node on the network)
      
      
2010/07/21 (3)
--------------
goal: allow worlds to be created dynamically

- some design thoughts / requirements:
    * "origin matcher" for a given world can be spawned on demand by gateway
    * all capable nodes should contribute (i.e., can become a "origin matcher"), via registeration at gateway
    * VAST node joining includes:
        1) notifying candidacy as potential matchers (including origin) to gateway
        2) obtaining the origin matcher of the joining world
    * fault tolerance of "origin matcher" should be supported (i.e., failed matcher is taken over smoothly)
    * matchers from different worlds share the underlying "relay mesh" & candidate matchers (i.e., a candidate may support any world)    
    * of course, everything should be done quickly & smoothly

- implement thoughts:
    - VSOPeer should be as simple as possible (close to current form), "worlds" concept should exist outside it.
    - should be easy to 'network' different worlds / gateways (share the relay mesh or matcher candidates)

    
2010/07/20 (2)
--------------
goal: add support of multiple rooms to VAST (add support for optional 'world_id' in join ())

- add optional 'world_id' parameter to the join () in VAST interface, so users can choose which "world" to join



2010/07/18 (7)
--------------
goal: 100+ concurrent without bugs

DEBUG: # of movements is less than expected
       > due to too much sleep in between steps, observe that oversleep could occur 
         for example, if the desired movements / second is 10, so it's 100ms between each step. 
         If framerate is 20 loops / second, it's on average 1000 / 20 = 50ms / frame. However,
         possible that by 20 frames the elapsed time is 97ms, then a movement would not occur
         if we're checking if 100ms has passed. A more precise clock and sleep may help
      
       > fixed by changing the time units used in TimeMonitor from milliseconds to microseconds
    

DEBUG   minimal bandwidth size is unusually large in log under Linux
      > fixed by replacing printf parameter from %lu to %l  (long unsigned to unsigned)
      


2010/07/16 (5)
--------------
goal: debug to run 500+ concurrent actually

DEBUG:  subscription requests may be sent continously to already failed matcher
        > all send to matchers now will go through error check, if matcher send is unsuccessful,
          it will be removed & replaced

- stress performance:   300 nodes, 2 sec / join, concurrent 140
-                       100 nodes, 2 sec / join, concurrent 96
          
          
2010/07/15 (4)
--------------
goal: debug to run 500+ concurrent actually

- will test for 10 minutes runs = 6000 time-steps (10 steps/sec) and 500 nodes, joining at 1 node / sec
  (so will take 500 / 60 = 8.3 min to join completely)


2010/07/14 (3)
--------------
goal: stress test VAST to 500+ nodes on physical network

DEBUG:  VASTnet::storeMapping (): existing address and new address mismatch.
      > issue may be caused when a host accepts new connection from remote host, 
        it first records the detected remote port (which differs from listen port). Later when the 
        remote host is registered again, its listen port is used. So the addresses will be different. 
      > change to if host accepts connection, the remote host's detected port is registered as '0' (not a listen port)
        later when listen port is provided (via a RELAY or SUBSCRIBE message), listen port will be used
        if the new replacement port is 0 (caused by a disconnected remote host re-connects again), then
        it's considered normal.
      
DEBUG:  when running test gateway, sleep time is always 25ms (40 frames / sec) regardless of load
      > found out it's that VastVerse did not actually use TimeMonitor to ensure it runs only within specified time
        budget. Updated to do so. 
                    


2010/07/13 (2)
--------------
goal: stress test VAST to 100+ nodes on physical network

05/13
DEBUG:  > gateway shows repeating error message
            'net_ace_handler (): UDP message received, but handler's remote_id not yet known'
        
        try to run 100 clients, the stat was 230 joins and 43 leaves were recorded by gateway

        > made the handling of the UDP error message to terminate UDP handler, run 30 nodes okay (no more error)
        however, there was quite a few following warnings:
        
            VASTnet::storeMapping (): existing address and new address mismatch.

        also, when all clients leave, gateway still considers one client connected, and there was
        repeating subscription request sent to certain node (its current matcher?)
        


2010/06/23 (3)
--------------
goal: integrate hole-punching to VAST

DEBUG   C# VAST client cannot connect to server, due to SUBSCRIBE message sent to self, would
        receive error such as this:
        
        [13882530681060196353] VASTMatcher::handleMessage () non-Matcher receives Matcher-specific message of type 26 from [13882530681060196353]

        > found out it's because the test program thinks it is a gateway, yet gateway already exists
        
DEBUG   port is not released even though VAST is shutdown (so init after shutdown does not work)
        > found out it's caused by net_ace_acceptor did not call _acceptor.close (); in handle_close ()
        > a long standing bug addressed :)
                
                
DEBUG   sending a picture of size 63408 bytes would crash the program & gateway

2010/06/03 (4)
--------------
goal: deply VASTMapChat on SF with a Linux server

DEBUG   seems Win32 client cannot connect to Linux server due to byte alignment problem
        the first ID request message cannot be sent / received properly by the Linux server.
        it appears that the length of an VASTHeader is 8 bytes (though should be an unsigned int = 4 bytes?)
        on Linux, the ID_REQUEST message is 42 bytes, but Win32 is only 34 bytes. Message from Win32 client 
        is also received with a several seconds delay.
        
        > found 'unsigned long' in win32 is 4 bytes, but on linux is 8 bytes, here's some comparison:
        > adopt a win32-specific "stdint.h" and unix-default <stdint.h> and change all types to explicitly specify size
        > also changed size_t in VASTTypes.h to vsize_t (defined as uint32_t, as sizeof (size_t) returns 8 bytes in linux
        > Win32 clients then can connect properly to Linux server :)
  
align: 4 (win32)
========
sizeof sizes:
VASTheader: 4 id_t: 8 timestamp_t: 4 length_t: 4 coord_t: 4
Position: 16 Area: 28 IPaddr: 12 Addr: 28 Node: 72

transfer sizes:
VASTheader: 4 coord_t: 4 Position: 12 Area: 20 IPaddr: 8 Addr: 20 Node: 52
        
linux:
========
sizeof sizes:
VASTheader: 8 id_t: 8 timestamp_t: 8 length_t: 8 coord_t: 4 
Position: 24 Area: 48 IPaddr: 24 Addr: 48 Node: 120

transfer sizes:
VASTheader: 8 coord_t: 4 Position: 12 Area: 28 IPaddr: 12 Addr: 28 Node: 72
        
        



2010/05/29 (6)
--------------
goal: make subID receivable at destination when calling VAST's send ()

CHANGE:
make a significant design change to remove automatically build up fromID -> hostID mapping when receiving
incoming messages. So all ID to hostID mapping now need to be explicit (by calling notifyMapping ())
The change is so that complex mapping won't be built. Also to avoid VASTnet-level auto-forwarding, which
makes interactions complex & difficult to debug

DEBUG:   after removing the mapping, certain later nodes cannot join properly when regions split, 
         also, matcher node would see more nodes than AOI neighbors. Suspect it's caused by some bad forwarding.
         
         > solved by make sure matcher also record the ID->hostID mapping for relays of clients, so that
         the SUBSCRIBE_NOTIFY message can be properly delivered. The problem was caused by 
         SUBSCRIBE_NOTIFY unable to deliver properly and be processed by the matcher itself.
         



2010/05/25 (2)
--------------
goal: VAST plugin for Firefox (make join work)


MILESTONE:
  VAST plugin joins successfully to server.. after creating VAST object into ScriptablePluginObjects
  
DEBUG:
  Plugin cannot join successfully, also when termintating would crash, found out it's caused by
  Plugin being DEBUG mode but VAST.dll is release.  If same mode is used for both then there'd be no problem.  




2010/05/20 (4)
--------------
goal: VAST plugin for Firefox.

Try to build Win32 samples, got samples build (but some would fail first) but cannot be accessed / used correctly.




2010-05-14 (5)
-------------
goal: debug problem cases

DEBUG:
- 05/13 in churn test of 50 nodes, recovery from churn is not consistent (between 95% to 99%, depend on cases)
- 05/14 sometimes subscribe is delayed due to slow relay join

solved by requiring matcher to notify relay directly of the subID -> client host ID mapping, this way
unknown mapping can be minimized, also addressing the above two problems. Test case reveals that consistency
can recover nicely to 99% for up to 10 nodes join/leave / sec for both 50 & 100 stable nodes


2010-05-13 (4)
--------------
goal: re-do churn test & finish paper writing

DEBUG
- 05/13 in churn test of 50 nodes, recovery from churn is not consistent (between 95% to 99%, depend on cases)
        > appears to be caused by fast switching over regions, so ownership transfer may not be completed

        found a serious problem where the SUBSCRIBE request is forwarded to neighbor matcher, but currently
        matcher responds directly to client (will not reach). Changed to respond via relay, but appears
        not all relays have yet the mapping setup
        
        
      

2010-04-30 (5)
--------------
goal: simulate 1000 nodes

DEBUG 0.4.4 linux buildable
      found out it's caused by the include library order (vastcommon is first, but should be the rightmost as 
      it's the least dependent)
      
- also install a Ubuntu server as Plug server

     
2010-04-29 (4)
--------------
goal: test fault tolerance

DEBUG
- 04-28 joining is still slow (relay querying)
    force a timeout on relay querying (currently set to 10 requests) afterwards we will force
    the client to join, even if Vivaldi's error value is not converged yet
    
DEBUG   gateway hangs when relays exit
        - found it's because there may be RELAY_QUERY coming in disconnected clients
        
        



2010-04-28 (3)
--------------
goal: test fault tolerance
  
- added command line flag to determine if this node will add as relay or matcher / none / or both



2010-04-23 (5)
--------------
goal: VAST 0.4.4 release & doc
  
DEBUG   vastsim_gui shows only '1' as ID
        > fixed by building map between node index in VASTsim & subscription ID
        
        

2010-04-22 (4)
--------------
goal: VAST 0.4.4 release & doc

DEBUG   node not moving after relay fail, indefintely
        > found that it's caused by the node kept trying to contact an already failed relay
        > added mechanism to check if the send is successful (if sent to relays) and if not,
          remove the failed relays

DEBUG   nodes strangely do not move, without any matcher/relay failure
        > caused by the relay disconnects a client, due to inactivity, as all NEIGHBOR messages are 
          sent directly from matcher to client.
          client may need to PING relays periodically.
          

        

2010-04-20 (2)
--------------
goal: real network disconnection test via pskill


DEBUG:  node stop moving after matcher fail (case 2)
        > another problem uncovered was that a re-subscribing client checks in with a newly joined
          matcher that does not have the client's subscription record, and thus assigns a new subID.
          however, the client program is unaware of the subID change, and thus all future movements
          are not accepted by the VAST client component
          
          fix by remove the check, however, this may be dangerous due to redundent ID assignment
          TODO: may need a way to periodically re-check ID with the assigning entry point
          


2010-04-18 (7)
--------------
goal: debug node stop moving after matcher fail

DEBUG:  node stop moving after matcher fail
        > one problem was that when clients try to re-subscribe to new matcher
          the new matcher does not consider the re-subscribe client as "owned", and thus
          would not send updates to it about its position.
          
          fixed by forced ownership when accepting SUBSCRIBE request
          
          another fix was to force notify client of new matcher (MATCHER_NOTIFY) when claiming orphan clients



2010-04-16 (5)
--------------
goal: churn test

DEBUG:  node 24 does not join properly (it can see others after a slow join, but others can't see it)
        > slow join is caused by relay pinging each other, causing physical coord to diverge
        > due to the slow relay join, SUBSCRIBE cannot yet be sent, and later when re-subscribe
          is initiated, layer number 0 and join location (0,0,0) is used. Causing other neighbors
          to not receive updates about the joining node.
          
          Fixed by recording subscription request regardless of whether the client is ready to 
          subscribe, so later re-subscribe attempt can be correct. 
          
BUG: seems like after relay only fail, node movement would beomce dragging (one update every two moves)

BUG: after matcher fail, some clients stop to have movements

- Perform churn test (allows one node to join / fail per 1 or 2 seconds, with stable size of 30 or 50)

  main finding is that consistency drops with smaller stable size or higher churn rate (50-60%)
  with 50 node as stable size and 2 second join/fail rate, consistency may be around 80%.
  
  two issues found: 
    - after failure (of relay?) client movement is dragging
    - some nodes would stop to move
    
  both are consistent with existing problem in failure simulations.
  


- Added JOIN_RATE to VASTsim.ini, to indicate how many steps before a new node joins
- Added STABLE_SIZE to VASTsim.ini, to indicate # of minium nodes to exist before failure can take place


2010-04-15 (4)
--------------
goal: finish relay / matcher fail sim

DEBUG:  nasty bug where clients belonging to the 3rd matcher joined would not getting update
        > found out it's caused by the MATCHER_NOTIFY message sent by previous matcher to clients,
          notifying a new matcher, is sent via relay, but relay only pass through MESSAGE / NEIGHBOR messages.
          Changed so that any uncongnized message received by relay is treated as a forwarded message to 
          client

BUG:    connection size is too large 
        > possibily too many relays are kept. need to periodically clean up
          

- Added MATCHER_SIZE to VASTsim.ini          
          
2010-04-13 (2)
--------------
goal: debug relay mechanism

DEBUG:  discover relays do not route properly, problem occurs when relay & matcher exist on
        the same node. Matcher may send mssages to relays, which end up is only received by the Matcher's own Client        
        > route all messages to client to relay first, then it's the relay's task to
          forward to clients
        > found out it's caused by the relay mechanism not designed properly. Messages sent from 
          Matcher have subscription ID as destinations. The original idea was to let the network layer
          do the subID -> hostID mapping translation. However, as each network layer has only one mapping,
          yet proper routing requires two (Matcher -> Relay mapping, Relay -> Client mapping), so it works
          if Relay & Client are same node (matcher sends directly to client, in fact bypassing relays),
          but if Matcher & Relay are the same node, then the Matcher->Relay mapping turns the message
          to itself, but then no mechanism exists to send to Client.
          
          Resolve the problem by having relay set up a subID -> clientHostID mapping explicitly 
          (client needs to notify relay of the mapping), and allow the network-layer translation used
          only by matcher (for finding relays).  Interestingly, NEIGHBOR messages and MATCHER_NOTIFY
          (sent by Matcher to Clients) appear can be sent directly without relays, as clients do have
          direct connections with their matchers. Other messages such as PUBLISH & SEND need to sent via
          relay (as their targets could include clients not managed by the current matcher, so send via 
          relay is more general). 
        

2010-04-12 (1)
--------------
goal: simulate relay & matcher fault


BUG:    after sure of making relay records are exchanged correctly, now the traffic increases
        (from 27 kb / sec to 35 kb / sec without much change in functionality or correctness)
        
BUG:    if few relays are specified, it seems like some relay join is not successful
        and thus their clients will miss the messages targeted at them..         

BUG:    discover relays do not route properly, problem occurs when relay & matcher exist on
        the same node. Matcher may send mssages to relays, which hs


2010-04-08 (4)
--------------
goal: simulate relay & matcher fault


DEBUG: found a rare bug that consistency becomes low (90%) in one case of 30 nodes sim (but all others report 98%)
       VAST-case-2010-04-08 node 24 join unsuccessful
       
       > it's caused by a node not joining properly, as its matcher has just joined and not yet initialized
         so the SUBSCRIBE messge was lost.
         
         fixed by first ensuring there's timeout & re-subscribe even for initial join, and also
         allow matchers to queue up receivied messages for later processing when its VSOpeer is still joining.
         
DEBUG:    client cannot send a message and received by itself, but the matcher node to which it sends, whose client
        component could receive the message
        in other words, only gateway can receive messages sent by VASTnode->send ()
        
        > found it's caused by VASTMatcher saving up the MESSAGE message type instead of forwarding it
          to the client. because when for non-matchers, its VSOpeer will not be created, and thus all 
          messages forwarded to it is saved (potential problem for non-matchers?)
          
          solution is to simply allow MESSAGE to be processed at non-matchers

         
2010-04-07 (3)
--------------
goal: simulate relay & matcher fault

DEBUG: trying to build Linux version for kenny, got this error when linking demo_console

    undefined reference to `vtable  
    
    for the SFVoronoi class, found this explanation: 
    http://www.daniweb.com/forums/thread114299.html#
    
    and move the definition of SFVoronoi's destructor to be not inline and solves the linker error
    
BUG:  linker error, appears to be not specifying the right libraries (however it's not the case...)

VONPeer.cpp:(.text+0x36bf): undefined reference to `Vast::VoronoiSF::VoronoiSF()'
../../lib/libvast.a(VONPeer.o): In function `Vast::VONPeer::VONPeer(unsigned long long, Vast::VONNetwork*, unsigned long, bool)':
VONPeer.cpp:(.text+0x3917): undefined reference to `Vast::VoronoiSF::VoronoiSF()'
../../lib/libvastnet.a(net_ace.o): In function `Vast::net_ace::receive()':
net_ace.cpp:(.text+0x56d): undefined reference to `Vast::TimeMonitor::getInstance()'
net_ace.cpp:(.text+0x575): undefined reference to `Vast::TimeMonitor::available()'


        


2010-04-06 (2)
--------------
goal: simulate relay & matcher fault


MILESTONE: simulate up to 200 nodes (3000 steps) with 96% consistency and 8 kb / sec bandwidth usage
                    up to 500 nodes (7000 steps) with 94% consistency and 5.1kb / sec bandwidth
            
BUG:       but occasionally some nodes may stop and not be moving (not sure why)            

BUG:       when building linux version, keep getting the following error:

            /usr/bin/ld: cannot find -lACE

            kenny suggests it's because of the static link flag, but the version MMnet provides
            is dynamic only                    


2010-04-05 (1)
--------------
goal: simulate relay fault

DEBUG: found for 30 nodes in standard 768x768 world, still some cases where consistency drops due to 
     inconsistent views among the matchers. 
        
        - fixed by adjusting matcher's AOI, solves the problem while introducing slight overhead
          to matcher AOI radius adjustment


2010-03-30 (2)
--------------
goal: experiment matcher fault tolerance

BUG: found a node would stop getting updates once crossing a region boundary (but not for other nodes)
     seems like a new matcher is added afterwards.. 

    - it's caused by a complex interaction. The problem is that when subscriber A crosses from
      matcher B to matcher C, at matcher C there has actually be some previous ID->host mapping
      for subscriber A, such that A is mapped to relay D. What happens is that relay D somehow
      disconnects matcher C (timeout?), and matcher C thus removes all mapping reference associated
      with relay D (which unfortunately includes subscriber A). Subscriber A's info thus is 
      removed from matcher C. This removal actually occurs *after* a successful ownership transfer,
      so when subscriber A later sends movement updates to matcher C, there will be no processing
      & no response.

MAJOR CHANGE:
    - make all time-related tasks consistent to use timestamp for checking
      both for countdown or periodic tasks
      
      getTickPerSecond () changed to getTimestampPerSecond () in VONNetwork


2010-03-25 (4)
--------------
goal: debug & test matcher fault tolerance



2010-03-24 (3)
--------------
goal: implement matcher fault tolerance

- client-side object seems to work more properly when backup-to-closest matcher is used

BUG: still some ghost objects at client side, suspect due to change in region for the client

BUG: very poor consistency for 50 nodes under 768x768, as low as 50%!

BUG: insertion of new matcher seems to jump at locations upon joining

DEBUG: in gui verion the node ID seem to be redundent
       - found out it's because we use subscription ID as ClientID, but clients can get subscription ID from
         different matchers, changed to display port number as ID, to show it uniquely in GUI
         
DEBUG: clients would 'jump' when cross boundaries, fixed by sending movement to both current & closest matcher
         

2010-03-23 (2)
--------------
goal: finish load balacning

MILESTONE
load balancing works using the new VSOPeer class. Currently subscription info would only be copied to a neighboring
region if the subscription AOI covers the neighbor region. Found out the most important load balancing design
is to make sure the region center (a VSOPeer node) always follows the center of the load (right now defined as
all 'owned' objects within the VSOPeer's region), this allows the most proper region shape/resizing.
Still an issue is that of the 'ghost objects'.. where replicas still exist though the original has stopped 
to send updates to the neighbor region.

will try use timeout + ping&pong to remove obsolete subscription records (object records in VSOPeer)

- after removing timeout objects (1 second), ghost objects mostly disappear. However, there are still some 
  left that do not disappear (possibly due to crossing region boundary so the old matcher has not been
  able to notify deletion. 
  
- nodes also would briefly 'jump' when crossing region boundary, probably due to clients' commands
  are not yet sent to the new matcher, so a small inconsistency (pause) while crossing.
  (previous VASTATE solution was to send movement to both manager nodes)

- under 768x768 30 nodes testing, consistency is between 90% - 97%. not very stable.  
  

2010-03-15 (1)
--------------
goal: debug simulation layer (make it work)

BUG seems like if physical coordinates are assigned, they will become NULL soon. (not sure why..)



2010-03-09 (2)
--------------
goal: debug pub/sub

- notifyMapping would store a different Address object for the same nodeID, caused by
  the detected port & actual listening port of a service differ. 




2010-03-02 (2)
--------------
goal: join success

DEBUG   connections seem to terminate abnormally
        - found it's because the new message handling method, returning (0) for size would be seen as serious problem
          that connection will terminate
          
DEBUG   packing / unpacking of the Addr object seems incorrect (gets corrupted) when sending RELAY_QUERY message

 


2010-02-24 (3)
--------------
goal: linux build for VAST 0.4.3

- different cases for incoming messages
    (a) ID request
    (b) ID assignment    
    (c) initial handshake (notification of remote host's ID)
    (d) regular message
    
    we now assume if _remote_ID is not known by net_ace_handler, then
    it's either (c) or (a) 
    
2010-02-09 (2)
--------------
goal: pub/sub mechanism done

Class name changes:
- change VASTNode to VASTClient
- change Topology to VASTRelay
- add VASTMatcher


2010-01-28 (4)
--------------
goal: very fast VASTnet layer with topology-aware join & globally unique ID assignment


join procedures:

    1. input a number of entries to VASTnet
    2. connects to a randomly selected entry point
        - to get unique ID
        - to learn of more relays
    3. concurrent query to relays to find closest relay
    4. connect to closest relay


2010-01-26 (2)
--------------
goal: separate JOIN process into (a) join relay (b) join SPS (a VON network)
      so that multiple worlds can be supported on top of a single overlay


- The relation between MessageHandler and unique ID is now that:
    - each host has a globally unique HostID (currently consists of publicIP + privateIP)
    - each "relay-level" host can generate a per-world unique NodeID (HostID + variations)
    - each host can have several MessageHandlers, and they are addressable via HostID
    - MessageQueue supports both HostID and NodeID as transfer target

steps:
    - allow VASTVerse.isLogin () to determine is_public / obtain physical coord with Topology class
    - add getHostID () and getNodeID () to IDGenerator
    - move use of IDGenerator.getNodeID () into 2nd stage join 
    - implement VAST.getPhysicalNeighbors ()
    - implement VAST.getLogicalNeighbors ()
   

2010-02-05 (5)
--------------
goal: finish join procedure, begin pub/sub

- for join procedure, some general steps may be the following
    - initiate something (like query for closest relay)
    - set timeout and wait
    - if reponse is gotten, process, initiate more things
    - if timeout, re-send request
    
- for join, the basic flow is:
    1. get unique ID & determine whether public IP or not   (VASTnet)
    2. determine physical coordinate via ping/pong relays   (Topology)
    3. query the physically closest relay                   (Topology)
    4. contact the physically closest relay                 (Topology)
    5. query for logically closest manager                  (VASTNode)
    6. contact logically closest manager                    (VASTNode)
    
    if the connected relay fails, or a contact request is not responded within timeout,
    another request should be sent to a different target


   
2010-02-04 (4)
--------------
goal: finish join procedure, begin pub/sub

    
- see that during join there may be two types of routing:
    - connect to physically closest "relay"
    - connect to logically closest "arbitrator/region manager"
  in current design a 'client' connects to physically closest 'relay', but may send/recv pub/sub
  messages from its logically closest 'manager'. how to match-up the two efficiently & effectively?
  
   one possibility is that events / requests are sent to logical managers first,
   but updates (bulky messages) are sent via relays back to the subscriber / original requester
        
   high bandwidth hosts are more suitable as relays
   high processing hosts are better as managers

   pub/sub are directed to managers
   while message receivals are passed via relays   
    

2010-01-21 (4)
--------------
goal: separate JOIN process into (a) join relay (b) join SPS (a VON network)
      so that multiple worlds can be supported on top of a single overlay

- Redesign login process as follows:
    1. a joiner first contact one to several Relays to 
        (a) learn of other Relays (if it only knows of 1 initial Relay)
        (b) obtain its physical coordinate (by querying these known Relays)      
        (c) connect with the closest Relay and a few neighboring Relays
        
        at this point, the joiner knows a few physically close Relays
        
    2. a joiner then hooks to a VAST network with a gateway server's hostname, this involves:
        (a) request joining and pass authenticationi with the VAST network's gateway server
        (b) obtain a unique ID that identifies the joiner within this world
        
        at this point, the joiner is known as a VASTNode and ready to specify subscriptions and publications
        
    3. a joiner is considered joined and can perform one of the followings:
        (a) subscribe an area of interest (AOI) at a given Layer
        (b) perform point or area publications for messages to a given Layer
        (c) receive messages sent to the subscribed area(s) at the given Layers
        (d) move an existing subscription to a new location
        


2010-01-19 (2)
--------------

DEBUG:  clients would crash when joining a gateway on experiment on PlaneLab 
        > found out it crashes at call to ACE_OS::gethostbyname () in net_ace's getIPFromHost ()
          reason may due to missing dynamic link library for using gethostbyname ()
          
DEBUG:  FLoD gateway cannot send message back to a newly joined VAST node
        > found out it's because it tries to send the message via sendMessage (),
          but target is a VONpeer (so mapping of VONpeer node ID to hostID still maps back
          to the gateway itself). 
          It means that if a discovered ID is a logical VONpeer id, then it must be addressed
          and sent via vastnode's send () (see example usage in demo_chatva).



2010-01-18 (1)
--------------
goal: optimize bandwidth usage (data structure)

-
BUG:    regardless whether timestamp is sent along with MOVE, bandwidth usage is the same



2010-01-15 (5)
--------------
goal: VAST 0.4.2 release


- make sure chat_va compiles okay. both DEBUG & RELEASE mode compiles okay

- change default simulation environment to (basically a Second Life region x 3)
    world       768 x 768
    nodes       30
    velocity    3
    AOI         195
    steps       3000
    overload    10

basic sim stat
    for 30 nodes, 3000 steps,
    we eventually use only 5 arbitrators, 98%+ consistency, 14-16 kb /sec bandwidth use

2010-01-11 (1)
--------------
goal: optimize bandwidth usage (data structure)

- observe that current load balancing method (moving arbitrator boundaries) would make
  arbitrators relatively close to each other (stick at center). need to find better load
  balancing policies.

  > seems like if arbitrators are more spread out, then it's better

  > seems like if arbitrator size is relatively fixed (instead of allowing dynamic join / leave)
    then it's more stable (better consistency)

  > it appears a "come closer" only arbitrator movement request would make all arbitrators to
    move close to each other

- added OVERLOAD_LIMIT parameter to VASTATE's INI file

MILESTONE:
  > allow arbitrators to chase after the center of agents
    created desired load balancing effect (region dynamic re-sizing according to load)

    in a 800x600 space, 90 nodes, 1500 steps, eventually only 15 arbitrators are added
    with 92-93%+ consistency
    but use 37 kb / sec on average bandwidth

    also the movement is too abrupt


2009-12-29 (2)
--------------
goal: remove bad landmark

BUG:    bad landmarks are not reported & removed correctly

        > seems like a terminated program's sockets are still connectable, this also explains
          bug 2009-12-28
        > fix by explicitly notify gateway of invalid landmarks to be removed


2009-12-28 (1)
--------------
goal: debug physical coordinate determination

BUG:    for 2nd or above nodes joining the system, "no known relay to contact" error occurs
        after repeat login/logoff

        > suspect it's caused by relay missing due to failures, yet the failed relays are not
          removed propoerly



2009-12-23 (3)
--------------
goal: debug physical coordinate determination

BUG: can build under Release but stuck on Debug mode for the VAST library

3>   Creating library ..\lib\VAST.lib and object ..\lib\VAST.exp
3>vastnet.lib(net_ace.obj) : error LNK2001: unresolved external symbol "__declspec(dllimport) char * __cdecl ACE_OS::strcpy(char *,char const *)" (__imp_?strcpy@ACE_OS@@YAPADPADPBD@Z)
3>vastnet.lib(net_ace.obj) : error LNK2001: unresolved external symbol "__declspec(dllimport) void __cdecl ACE_OS::thr_yield(void)" (__imp_?thr_yield@ACE_OS@@YAXXZ)
3>vastnet.lib(net_ace.obj) : error LNK2001: unresolved external symbol "__declspec(dllimport) char * __cdecl ACE_OS::inet_ntoa(struct in_addr)" (__imp_?inet_ntoa@ACE_OS@@YAPADUin_addr@@@Z)
3>vastnet.lib(net_ace.obj) : error LNK2001: unresolved external symbol "__declspec(dllimport) struct hostent * __cdecl ACE_OS::gethostbyname(char const *)" (__imp_?gethostbyname@ACE_OS@@YAPAUhostent@@PBD@Z)
3>vastnet.lib(net_ace.obj) : error LNK2001: unresolved external symbol "__declspec(dllimport) int __cdecl ACE_OS::hostname(char *,unsigned int)" (__imp_?hostname@ACE_OS@@YAHPADI@Z)
3>vastnet.lib(net_ace.obj) : error LNK2001: unresolved external symbol "__declspec(dllimport) int __cdecl ACE_OS::sleep(class ACE_Time_Value const &)" (__imp_?sleep@ACE_OS@@YAHABVACE_Time_Value@@@Z)
3>common.lib(VASTUtil.obj) : error LNK2001: unresolved external symbol "__declspec(dllimport) class ACE_Time_Value __cdecl ACE_OS::gettimeofday(void)" (__imp_?gettimeofday@ACE_OS@@YA?AVACE_Time_Value@@XZ)
3>..\lib\VAST.dll : fatal error LNK1120: 7 unresolved externals

    > found out it's caused by the following line in net_ace.h  (took a whole working day to find out this problem!)

    #define ACE_NO_INLINE 1



2009-12-15 (2)
--------------
goal: translate latency into physical coordinate


BUG
    - transmission stat in simulation mode isn't correct (Send < Recv)

    - after destroyNode () is called, if the process holds (waits) then crash occurs
      this happens during testing with Actor wrapper



2009-11-29 (7)
--------------
goal: debug simulation mode

DEBUG:
    - all agents stop moving when new arbitrator is inserted, after some time movements resume
      > arbitrator doesn't send updates if its is_owner flag isn't set, howerver, during ownership transfer
        the flag is not set at all, so no updates are sent. changed so that updates are not sent only
        if I'm not owner, but also the object is not in transit:
        (is_owner not set, but ALSO in_transit == 0)

    - new arbitrator join would make existing arbitrator leave immediately
      > seems like arbitrator ownership transfer isn't complete successfully when new arbitrator joins
        transfer occurs but the receiving arbitrator does not know the object
        request for object seems to fail / not occurring

      > problem is caused by the arbitrator uses sendAgent () to send full object update to
        neighbor arbitrators, but sendAgent () overrides the msggroup to MSG_GROUP_VASTATE_AGENT
        and so the arbitrators would never receive the object states properly




2009-11-11 (3)
--------------
goal: priortized messaging

* debug message processing (remove FIFO receive queue)
    - arbitrator departure's VON_BYE isn't properly received by neighbors
    > found out it's because the new storeRawMessage () treats messages with size 0 (VON_BYE contains zero field)
      as error and drops the message.




2009-11-09 (1)
--------------
goal: priortized messaging

* current problems observed (no cleanup of old object, object reclaim on but no REJOIN message sent to agents)
    - arbitrator still claims ghost objects (incorrectly)
    - however, agents do not consider them as current arbitrator (existing one not failed)
    - therefore arbitrator deletion notice has no effects to outside agents
    - extra messages occur for deletion (no message overhead for reclaim)
    - currently, consistency under 10 nodes is till 95% +



2009-11-06 (5)
--------------
goal: correct ownership reclaim & no deletion of self agent object

* note that JOIN procedure of agent & REJOIN may merge

current JOIN procedure:
1. agent.join (): VASTnode subscribes to AOI
2. publish JOIN event --> received by current arbitrator
3. current arbitrator
    - creates agent object (addAgent)
    - notifies agent of arbitratorship (ARBITRATOR)
4. agent adds list of arbitrators (learn of CurrentArbitrator)
5. DONE. agent will send future events to CurrentArbitrator

current TRANSFER procedure:
1. agent moves into another region (MOVEMENT event)
2. current arbitrator transfer ownership to neighbor (TRANSFER)
3. neighbor arbitrator
    - accepts ownership (if object doesn't exist, request)
    - create agent object (addAgent)
    - notifies agent of arbitratorship (ARBITRATOR)
    - send back acknowledge (TRANSFER_ACK)
4. original arbitrator removes its in_transit flag
5. if original arbitratrs in_transit is not removed, reclaim ownership

current REJOIN procedure:
1. arbitrator counts # of ticks an un-owned object is within its region
2. reclaim directly after time-out
3. sends out REJOIN to agent
4. agent sends out JOIN event to current arbitrator

** step 3 in REJOIN should not occur (let agent detect and send JOIN event by itself)




2009-11-04 (3)
--------------
goal: add back ownership reclaim while keeping correctness / debug crash

* run successfully 50, 100 nodes under 800x600 for 1000 steps. (50 nodes 2000 steps are also successful)
  problems found:

    - arbitrator may delete an owned agent that it doesn't see in its agent list (the avatar object is thus lost)

    - runs very slowly (too much processing / messages)

    - sometimes the partitioning gets weird (doesn't look Voronoi)






2009-10-30 (5)
--------------
goal: stabalize and correct physical layer VASTATE

    - agents received self deletion message (self disappearance)
    > found out it's caused by arbitrator reclaiming ownership for a ghost object (objects it's
      no longer receiving updates), and notify the agent that it is its legal arbitrator
      (but actually not). The arbitrator then deletes the object (as it's actually outside of
      its region, when receiving new update). Thus notifying the agent to perform self deletion.

      current solution is to simply remove the reclaim mechanism..

    > also added reclaim ownership of in-transit object if acknowledgement is not received in time

    - arbitrators receive exceed EVENTs (continously)

    - JOIN events sometimes fail to receive (50%+ times)
    > found out it's because initial subscription area for arbitrator does not cover fully join locations



2009-10-29 (4)
--------------
goal: physical layer VASTATE 50+ nodes

potential bug:
    - agentLogic's self used / referenced before initialization (by SimPeer, but AgentImpl could already call the onXXX () callbacks
      which currently assumes the _self variable is valid (though it may not), causing a crash

      > relocate _self variable from SimAgent to AgentLogic, making it a default function, remove setSelf () function

    - agents may become invalid yet unremoved (possibily due to ownership transfer, or in-transit transfer + agent fail)
      however, results of sending updates are not checked against such errors
      > add sendAgents () function that checks the success of send results and remove any invalid agents

    - object discovery may occur many times at once (possibly requesting many times after receiving
      multiple position updates
      > record already sent requests and avoid redundency

    - arbitrator receives many events not belonging to it
      > it's normal if it's an agent standing close to the arbitrator (as all enclosing arbitrators receive the events too)

    - many pos_version = 0 are detected at gateway


2009-10-27 (2)
--------------
goal: debug VASTATE real network deploy (should add 30-50+ concurrent)

BUG spotted:
    - crash for agent when arbitrator insertion occurs
    - many object discovery suddenly appear at once (not necessarily bug)
    - in pure simulation mode, 100 nodes would cause crash (even 50 nodes would have problems, crash on 1500 steps)
    - receive msg on deletion of self from arbitrator

suspected cause:
    - arbitrator subscribes to WORLD, instead of its own region only..
      too many publications / VON maintainance for large node size

    - arbitrator insertion point is not good (too frequent partitioning)
      what's the optimal join point?
      consider arbitrator MOVE?


2009-10-23 (5)
--------------
goal: deploy VASTATE onto Planetlab

- experimented with inserting arbitrator at mid-way between agent center & arbitrator center

  found that having 10 nodes as threshold is better than 5 nodes (in general)
  many instances of agents being notified of self avatar object deletion occurs for 5 node threshold

  but consistency still maintains at over 95%+ if such deletion requests are ignored




2009-10-19 (1)
--------------
goal: combine VASTATEsim & VASTsim into one library

to support:

1) node creation / movement / destruction
2) stat collections
3) rendering / printing of current state





2009-10-20 (3)
--------------
goal: debug dynamic arbitrator join


MILESTONE: dynamic arbitrator join/leave seems okay (up to 30 nodes tested with 95%+ consistency)

main problems found were:
    - suspected was VON stationary departure (but checked it was working, after adding forced notify after depart detection)
    - not all arbitrators join as relays (join process simplified)
    - join locations were redundent (check for redundency at each selection, also randomize locations)


2009-10-07 (3)
--------------
goal: make dynamic arbitrator join work

Debug: All of a sudden VASTATEsim doesn't work any more.. cannot even begin simulation,
       found out it's caused by addition of VON_EN message, causing message # to exceed 10 and thus overlap with VAST
       message #.. (which begin at 10)


2009-10-06 (2)
--------------
goal: debug dynamic arbitrator

added VON_EN message type, so that when VON_DISCONNECT or VON_BYE is received, the receiver may perform a missing neighbor
check with VON_EN.  Note that VON_EN is processed as part of VON_HELLO as well.



2009-09-29 (2)
--------------
goal: deploy linux self-runnable linux on PlanetLab

DEBUG:
  after setting export LD_LIBRARY_PATH=./
  the following library error still shows:

  ./demo_console: /usr/local/lib/libstdc++.so.6: version `GLIBCXX_3.4.9' not found (required by ./libACE-5.6.3.so)
  ./demo_console: /lib/libc.so.6: version `GLIBC_2.8' not found (required by ./libACE-5.6.3.so)

  solved by building ACE directly on older build environment (at MMnet server)

- compile VAST & ACE successfully at MMNET and build a runnable version
  (binary built at Plug server cannot be run because the installed libc version in MMNET is older)



2009-09-24 (4)
--------------
goal: deploy linux self-runnable linux on PlanetLab




2009-09-07 (1)
--------------
goal: debug re-join problem (cannot re-join after node fail) in real network
      port to Linux system



2009-09-04 (5)
==============
goal: debug VASTATE integrated approach

- by copying events to also enclosing arbitrators (but only when crossing boundary), we're able
  to transit over arbitrator boundary without the 'jumping' in position. Likely because
  the event was immediately effective after the ownership transfer.

- "lean back" is now supported by sending object updates to only the closest enclosing arbitrator
  to an object. This way all objects have only ONE backup. However, this approach requires
  that arbitrators also have a request-based mechanism similiar to what agents have in
  requesting from a neighbor arbitrator the full object states if it receives only partial updates

MILESTONE: integrated approach works & tested (10 nodes up to 10 arbitrators seem correct)




2009-08-27
==========
goal: optimize & debug VASTATE


DEBUG   solved one self disapperance bug (10 nodes, 5 arbs)
        ownership transfer okay, agent also gets new current arbitrator notification.
        Problem occurs as there was a previous mapping of the new arbitrator (but to
        the incorrect host), due to a previously forwarded QUERY request (as it was
        forwarded from gateway, the agent records the arbitrator as map to gateway.
        All subsequent events therefore would not reach the new arbitrator.

        Solved by calling notifyMapping () whenever a new ARBITRATOR is received by an agent

DEBUG   solved a potentially nasty bug where agent movement across arbitrators is correct
        under DEBUG mode but incorrect (agent stops moving) in RELEASE mode. when
        # of arbitrators == 5 (under 10 nodes)

        observe that the agent IDs are different in debug & release. Therefore required
        arbitrator be created FIRST before agent is created. then solved the problem
        (same agent ID & behavior for both DEBUG / RELEASE mode)

DEBUG   solved another disapperance bug, caused by the new arbitrator not knowing how
        to contact a new agent within its region. solved by calling notifyMapping ()


2009-08-21
==========
goal: debug self disappearance in multiple arbitrators

DEBUG:  objects near & outside of AOI boundary keep appearing & disappearing
        Caused by object deleting occurred while still receiving update publications (so agent would request
        the object again).

        Solved by setting two AOI buffer zone, a closer one for notifying agent discovery / disappearance
        (onCreate and onDestroy), another further one for actually deleting the object, and a furthest one for
        subscriptions. (so with the define VASTATE_BUFFER_RATIO as the zone to remove, VASTATE_BUFFER_RATIO/2 as
        the zone to notify agent, and VASTATE_BUFFER_RATIO*2 for the subscription area).


2009-08-18
==========
goal: debug VASTATE attribute update bug in real network
      debug self disappearance in multiple arbitrators


DEBUG:  found a bug of unable to start simulation after 2nd node.
        problem was caused by agent trying to join (subscribe AOI & send JOIN event)
        before the arbitrator on the same node has joined (yet the arbitrator
        manages its region). So this creates a case that the agent's JOIN event was
        never processed by a valid arbitrator.



2009-08-12
==========
goal: test the new "discovery by request" concept

MILESTONE:
successfully tested "discovery by request" as a valid way for agents to find objects
without arbitrators having to backup more than neighboring arbitrator objects :)


2009-08-11
==========
goal: debug agent-side object discovery bug

- document two important bug cases in VASTATE-case-2009-08-11
  that includes
    1) inproper discovery due to incorrect arbitrator knowledge
    2) undiscovery caused by AOI spanning across more than two
      arbitrators' regions.




2009-08-09 (7)
--------------
goal: debug ownership reclaim

DEBUG
    - found out first problem in reclaim is that the node in VASTATE may not have failed properly (added calls to leave ()
      for both arbitrator & agent when a VASTATE node fails

    - found out another problem is that when ownership for avatar object is reclaimed, the "agent" info is not re-created,
      thus the arbitrator may consider the avatar object's agent disconnected and destroyed the avatar object
      (addressed by sending ARBITRATOR to the disconnected agent with a flag, after which the agent would send back
      the proper TRANSFER to the new arbitrator)





2009-08-07 (5)
--------------
goal: debug ownership reclaim



2009-08-05 (3)
--------------
goal: implement arbitrator fault-tolerance

- realize that proper arbitrator fault-tolerance rests on
    1) proper notification of object creation/existence and ownership transfer
    2) assume ownerships of unowned objects in case of neighbor failure

  and it's important to clarify / define behaviors for an arbitrator when
    1) a new neighbor arbitrator is learned
    2) an existing neighbor arbitrator has departed (or become non-enclosing neighbors)

  arbitrators should also notify enclosing neighbors (or all known neighbors?) when
    1) it creates / updates / deletes an object (to all)
    2) it transfers ownership to a neighbor (to target only)
    3) it acknowledges ownership transfer (to target only)

  right now arbitrators process messages only if it's from an agent whose avatar object it owns
  (regardless whether the agent is inside its region or not)

  ownership according to region containment can be seen as a 'soft-state' decision
  (i.e., ownership is not 100% according to region division, as there could exist temporary inconsistency)


- methods to take when
    1) a new arbitrator is learned
        a. send current list of owned objects
        b. new arbitrator will check against its own and request for any unknown object
        c. send new arbitrator missing objects in full

        this will help a new arbitrator to learn fully enclosing neighbors managed objects,
        without receiving redundent full object updates for those already known

    2) a known arbitrator has departed
        a. send list of owned objects
        b. departed arbitrator will mark those objects as deleted



RELEASE 0.4.0   (file: VAST-c++-0.4.0-src.7z    size: 4,479,687)

This is a major release of VAST, re-adapting the original support of VON
(Voronoi-based Overlay Network) to the support of SPS (Spatial Publish Subscribe).

Details of VON and SPS can be found in the following papers:

VON: http://vast.sourceforge.net/docs/pub/2006-hu-VON.pdf
SPS: http://vast.sourceforge.net/docs/pub/2009-MMVE-SPS.pdf

For other details please see "README.txt" for general descriptions and
"INSTALL.txt" for how to use VAST



2009-08-04 (2)
--------------
goal: debug 2nd node join bug in chatva, release VAST 0.4.0

DEBUG
2009-08-03  chatva 2nd node+ does not move properly (after some pause, 3rd node doesn't move at all),
            also cannot display chat messages except 1st node

            found out it's because the VONpeer is initialized with the local host's time (which may be more
            advanced), causing subsequent movement updates cannot reflect to the VONpeer by the original Client host.
            (unless its logical clock has advanced enough to exceed the Relay's logical clock at the VONpeer's
            creation). Solved by initializing VONpeer's logical clock as 0.


DEBUG       another bug following the above is that chat msg from 2nd node is shown only at 1st node.
            found it's caused by adding 'send time' to the PUBLISH message (to measure latency).
            However, as when processing MESSAGE, it's sent to local nodes first before remote nodes,
            so the sendtime was extracted, and thus the message sent to remote node isn't correct.
            Solved by reversing the order to send messages


2009-08-03 (1)
--------------
goal: implement fault tolerance for VASTATE arbitrator



2009-07-30 (4)
--------------
goal: debug ownership transfer (over 4+ arbitrators)

DEBUG
2009-07-29 seems like an arbitrator that gets a transferred object would try to delete it at some point

            caused because the agent info is not also transferred, so the new owner does not think
            as a valid avatar objects for one of its peers and will consider it as a disconnected agent
            (thus removing it)

           transfer agent info as well


MILESTONE: ownership transfer under multiple arbitrators success (up to 5)

            did two things to address bugs:
            - transfer agent info along with ownership
            - upon receiving ownership transfer, also notifies neighbor arbitrators of object creation
              (so that subsequent ownership transfer can work correctly)




2009-07-29 (3)
--------------
goal: debug ownership transfer


- improve the joinRelay () mechanism in VAST.
- agent in VASTATE now also would check for redundent neighbor INSERT (inserting existing, known neighbors)


2009-07-27 (1)
--------------
goal: implement ownership transfer

- use Visual Leak Detector (VLD) and found memory leakages for VAST, removed some,
  but pointers to a the same newed object seem to be counted as leakage even though
  the object would be deleted at some point




2009-07-26 (7)
--------------
goal: debug update publication & implement ownership transfer

DEBUG:  update publication debugged (07-23 bug)

2009-07-23 for more than 1 arbitrator (2), the second arbitrator's agents do not seem
           to correctly get position updates for agents managed by the 2nd arbitrator
           except when just discovering at boundary (then no updates). Appearantly
           the arbitrator still updates & keeps track of agent positions correctly,
           just that updates are not correctly published / received by agents
           (discovery works because it's a one-to-one communication). So seems like
           something's wrong with the 2nd arbitrator's publication mechanism.

            later found it's not just 2nd arbitrator, 1st as well.

        found out it's caused by msggroup being filled incorrectly.
        for publiation via a vastnode, msggroup should be MSG_GROUP_VAST_RELAY
        for updates sent to neighbor arbitrators directly, msggroup sould be MSG_GROUP_VASTATE_ARBITRATOR
        the problem occurs when the same message is filled as MSG_GROUP_VASTATE_ARBITRATOR,
        but then sent via vastnode (so it cannot be processed by the correct relay).

        re-written the Message.clear () so that msggroup is clear up each time




2009-07-25 (6)
--------------
goal: debug arbitrator publication to agents, ownership transfer




2009-07-24 (5)
--------------
goal: object & ownership transfer among arbitrators


2009-07-24 appearantly the arbitrator & agent in VASTATE would join at different physical coordinates for same host
           this seems to be caused by Valaldi giving synthetic coordintes based on host_id (arbitrator & agent have different
           host_ids)

           partial fix: valvidi currently assigns the same physical coord to two consequentive hosts (e.g. 1&2, 3&4)

2009-07-24 relay join cannot succeed for the 3rd node under ENABLE_LATENCY for VASTATEsim
           preliminary investigation shows it's caused by the physical coordinate of node 2's agent /arbitrator
           has resorted back to the movement coord instead of physical coords given by Valvidi

           found out it's caused by Agent's join () updating the _self's position, which happens to be the
           a pointer to the vastnode's position (which should be a physical coord used by the VASTnode to join)
           thus corrupting.. this means that getSelf for Agent and Arbitrator should mean different things
           (acutally.. both should be the logical coord, while the vastnode's coord is protected from external
           corruption). Solved by having separate _self variable maintained within both AgentImpl and ArbitratorImpl
           indepedent from the _self variable in vastnode or VONpeer



2009-07-23 (4)
--------------
goal: object & ownership transfer among arbitrators






2009-07-22 (3)
--------------
goal: agent can call getArbitrator () and obtain current arbitrator info

DEBUG:
2009-07-22 certain position updates do not seem to take effect in VASTATE (esp. when the node is at boundary)

           solved by using an AOI-buffer multiplier for the agent's subscription area.
           appearantly the bug is caused by the agent not receiving updates sufficiently for
           border objects

- multiple arbitrators join a VON success, can take events and send updates.
  however, currently all arbitrators subscribe the same area for events, and create avatar objects
  for only those users within their regions (a bug exists that clients do not see other users created
  by neighboring arbitrators).

DEBUG:      found out the reason why a 2nd arbitrator was not able to join a VON is because of message
            ID collison. the VON messages are in numbers between 1 - 10, so other message handlers
            intending to use VON internally and helping to relay VON messages (such as Relay class and now
            Arbitrator) should define their message number starting 10 + .





2009-07-18 (6)
--------------
goal: debug peer target not found after node fail


DEBUG peer target not found, found out it's caused by the failed node

        - not sending explicit VON_BYE to all existing neighbors
        - not sending explicit VON_BYE to potential neighbors (who might have accepted HELLO already)

DEBUG
2009-06-29 in VAST, some "ghost nodes" sometimes exist when # of node size is large (~50)

        caused by receiving outdated NODE messages
        solution: use explicit VON_HELLO acknowledgement (handshake before recongnzing a neighbor)


2009-07-09 (4)
--------------
goal: support debug & integration (known bugs: join may not succeed for 2nd node)

- add join timeout for VAST node to re-send QUERY message to gateway




2009-07-08 (3)
--------------
goal: debug large object creation crash & join not exceeding 3

- found out large object creation (100 objects, 1000 attributes, 9 MB of objects created at
  a time) or frequent movements do not cause crash, even with ACE. However, if arbitrator
  allocates more than 1.7 G RAM (real & virtual) then crash occurs due to memory allocation
  error.





2009/07/07 (2)
--------------
goal: debug crash bug when peer disconnects

DEBUG: solved the long-standing crash bug when an agent leaves, other existing agents
       would crash

       found the issue was caused by ACE printing error message about not
       being able to re-connect. After removing the invalid debug parameters %p the
       problem was solved

DEBUG: after an agent successfully disconnects, other existing agents would temporily
       pause upon movement once in a while.

       it's caused by not clearning VONPeer for the disconnecting agent cleanly,
       so existing agents still try to send VON movements to it.

       solved by sending the VON_DISCONNECT properly







2009/07/01 (3)
--------------
goal: test multiple arbitrator's JOIN to VON & SUBSCRIBE

- tested up to 4 nodes in real network (chatva) that shows correct VON behavior




2009/06/29 (1)
--------------
goal: debug VAST's consistency issues

DEBUG:
2009-06-12 in VASTsim neighbor position updates sometimes are lost, or jump abruptly
           likely caused by timestamp issues

        found out some small delay in position update is caused by position-only updates
        (vs. full update) does not have timestamp, so outdated updates are treated the same
        as new updates. Add timestamp to incremental position updates

        another issue is caused by the removeNonOverlapped policy, where the drop count is
        checked only every MAX_DROP_COUNT times, and would reset to 0 if the relevance check
        passes. So it's possible that a drop count exceeds the limit but the neighbor has
        just become irrelevant. This makes too short a time to disconnect irrelevant neighbor.
        re-design the check so that an irrelevant neighbor must remain so for MAX_DROP_COUNT
        ticks before being removed.

        also found that the AOI_BUFFER setting, should use a resonably large value
        (increased from 10 to 20). Also when the receiver of NODE message try to decide
        if to accept the notification, it should be more tolerant (use AOI_BUFFER * 2 as
        relevance check buffer)

        some simple stats:

        with 10 nodes,

                AOI-buffer  Consistency     per sec U/L     connected neighbor  AOI neighbor
                15          99.9620%        3235/3231
                17          99.9848%        3239/3236
                20          100.0000%       3246/3242       5.3 ~ 6.3           1.8 ~ 3.8

        with 50 nodes
                15          99.8485%        9684/9674
                20          99.9928%        10087/10076     14.4 ~ 16.9         11.3 ~ 12.8



2009/06/27 (6)
--------------
goal: debug VASTATEsim (join, move)


DEBUG:  when gateway replies to LOGIN request by an agent, it cannot find the receiver's ID->address mapping,
        as the agent may use a previously existing connection (with unassigned ID).

NOTE  for each SimPeer (in VASTATEsim), two hostIDs are allocated (one for agent, one for arbitrator),
      however, ID assignment may occur in any order (agent or arbitrator may obtain unique ID sooner than another)
      across different SimPeers


BIG BUG:
        when agents join VASTATE, they may publish JOIN event to be received by an arbitrator.
        However, as agents and arbitrators have different AOI (subscription areas) an agent's AOI
        may not contain the arbitrator, and thus will not know the presence of the arbitrator to publish
        the JOIN event.

        solution seems to be to support mutual discovery in the VON layer (i.e., A is notified of B not only if
        A covers B, but also if B covers A).

MILESTONE

VASTATEsim_console seems to be working, after fixing the above Big Bug, by changing the definition of isRelevantNeighbor ()
in VONPeer to be of mutual interest instead of just node A's AOI covers node B.
seems like LOGIN, JOIN, and MOVE are working using VASTATE...

- also try to add cleanConnections () in VAST, but havn't gotten the right way to do it..




2009/06/26 (5)
--------------
goal: debug VASTATEsim (join, move)


found out the initial JOIN publication of a joining agent cannot be received by arbitrator
as the agent has not yet subscribed AOI properly (i.e., not a member of the VAST network
to send valid publications).




2009/06/25 (4)
--------------
goal: debug VASTATEsim (join, move)

DEBUG
2009-06-25 calling publish () would crash demo_chatva, detailed trace shows that
           using addTarget () inside publish () corrupts the dellocation of Message object
           likely issues are caused elsewhere

           found out it's caused by instancing a Message object in chatva and passed into
           VAST, when VAST adds a target it allocates memory internally, which cannot
           be de-allocated in chatva (outside of VAST). Temp solution: create another
           internal copy of the message to manipulate.





2009/06/19 (5)
--------------
goal: debug VASTATE funcions (create/update/destroy objects)



- makes publish () works for VAST (point publication with layer stored as the 'pad' field inside
  IPaddr




2009/06/18 (4)
--------------
goal: debug VASTATE funcions (create/update/destroy objects)


DEBUG:
in VAST when using chatva and only a single relay (client-server mode), when peers leave the
info are still retained.

found out it's because the VON_DISCONNECT messages have not been properly sent to still
existing peers. Also add the removePeer () function in Relay.cpp


2009/06/17 (3)
--------------
goal: implement VASTATE funcions (create/update/destroy objects)


A note on current ID usage / generation

VASTVerse           - maintains host_id
MessageQueue        - maintains host_id

VONPeer             - always externally supplied (including gateway)

VAST                - equals host_id, differentiable with msggroup
VASTATE             - none (only factory class)
  VONPeer           - for arbitrators, obtained via getUniqueID (locally generated)
  Agent             - host_id + msggroup ?
  Arbitrator        - host_id + msggroup ?

  Agent-VAST        -
  Arbitrator-VAST   -



2009/06/12 (5)
--------------
goal: debug VAST's real network layer


found UDP is causing the disconnection, temporarily use TCP for movmment updates



2009/06/11 (4)
--------------
goal: debug new VAST architecture (messagehandler)

* MILESTONE:
VAST works in new architecture. simulation works for 50 nodes in 50 relays or 1 relay.
Can see obviously centralized approach uses slightly less bandwidth.. (as expected :)
real network can connect, but still buggy (okay up to 3 nodes)

DEBUG:
2009-06-11 non-relay VONPeers do not properly receive neighbor movement updates via their relays
    as only a single MOVE is sent if multiple targets exist for the same host,
    at the handling side, the same received message is passed to multiple VONPeers several times.
    This causes issues in content extraction, where after each pass we need to 'reset'
    the current pointer in the content


DEBUG:
2009-06-10 no global node position for non-relay nodes
    caused by ambiguiy of hostID & VONPeer ID. So when a VONPeer 7 sends a message to VONPeer 2
    via VONPeer 1 (host 1). At host 1, it's unclear whether the message should be handled by
    host 1, or host 2. As there can only be one mapping between an ID and a host (id2host mapping)

    solve this issue by separate hostID & VONPeer ID, such that VONPeer ID is now running
    in the reserved bits of a hostID (the most significant X bits). this will target to host
    mapping would be clear. Also, remove the strict check of destination ID before processing
    (now when process messages, a message is locally handled as long as the message is not
    for forwarding (no id -> host translation shows a different host) and a local handler
    for the message group can be found. This provides flexibility for VONPeer messages
    be first sent to a Relay message handler.

    result is when now non-relay nodes also can learn of neighbors, but only at 66% consistency




2009/06/10 (3)
--------------
goal: debug new VAST architecture (messagehandler)

BUG:
- 2009-06-10 normal operations if all nodes are relays, but no global node position for
  non-relay nodes & redundent NEIGHBOR messages / notifications (duplicates of neighbors)
  probably caused by VONPeer not properly created at a relay

DEBUG:
join bug caused by
    * too short an interval time to determine timely neighbors
      (thus a new peer doesn't accept info from VON_NODE message)
    * not ticking each VONPeer (during Relay's postHandle ()), therefore
      HELLO & handshake are never sent


2009/05/15 (5)
--------------
goal: real network with 30-40 nodes


DEBUG:
allow failed node to be removed from neighbor list by recording node's last active time
and remove non-active neighbors after some time (also add sending keepalive MOVE messages
periodically)



2009/05/14 (4)
--------------
goal: real network with 30-40 nodes

DEBUG (after node fail simulation ceases to work)
- other nodes still move after node fail, caused by improper handling of DISCONNECT message
  (generation and processing of the DISCONNECT message)


2009/05/13 (3)
--------------
goal: debug real network

MILESTONE:
- real network works for 1st time (3-4 nodes :)


2009/05/12 (2)
--------------
goal: debug real network layer

- code real network
- send / receive tested with SimNode


2009/05/08 (5)
--------------
goal: debug migration and test on real network


2009/05/07 (4)
--------------
goal: code on peer migration (fault tolerance of relay)


- separate MOVE & MOVE_B into MOVE, MOVE_F, MOVE_B, MOVE_FB, where F indicates full AOI info
  tested successfully with reduced message overhead

- make metric collection work (topology consistency, bandwidth, etc.)

DEBUG:
2009-05-07  send only position update seems to cause topology inconsistency
            caused by undefined assignment operator for classes such as Position/Area
            so we would update empty positions or AOIs for neighbors


2009/05/06 (3)
--------------
goal: debug relay->client message
      code on peer migration (fault tolerance of relay)


- correct run of relay -> client messages



2009/05/05 (2)
--------------
goal: optimize relay -> client messages (position updates)
      correct stat messages (consistency / avg. bandwidth)

BUG:
- notify Client of a Peer's neighbors incrementally (but buggy)



2009/04/30 (4)
--------------
goal:   debug VAST, have VASTsim_gui running

DEBUG:
VASTBuffer did not initilize its internal buffer size to the allocated one
(which might cause inefficiency in double-allocating every time)

DEBUG:
the ID request sent from 2nd Client to 1st Gateway does not decode properly.

found out receiveMessage does not treat the received message as a combined message
(each with individual total_size in front), this is due to how net_ace & net_emu
treat received messages differently (ACE strip out the total_size upon receiving
while net_emu doesn't).

DEBUG
- for the initial VON_QUERY message, cannot put the 1st relay (gateway) as the contact
  address, as the gateway will not have the message handler for the sender node.
  solved this by including both the "joiner" and "relay" info in the query message

MILESTONE:
VASTsim_console runnable at 4:22pm :)
VASTsim_gui     working  at 5:00pm :)



2009/04/29 (3)
--------------
goal:   VASTsim converted
        1st VASTsim_gui running

BUG:
serialize & deserialize have different results
DEBUG:
found out it's because after deserialization in Message, the _curr pointer did
not return to 0, so the subsequent calls to extract () were incorrect


DEBUG
- add send self messages (route to receive buffer directly)



2009/04/28 (2)
--------------
goal:   VON procedure MOVE
        start to run simulation with VASTsim


- MOVE done, as well as JOIN, SUBSCRIBE for Peers
- eliminate the Coord class from Movement.h in VASTsim




2009/04/24 (5)
--------------
goal:   client & relay join cleanup
        VON procedures



2009/04/23 (4)
--------------
goal:   client & relay joining

- initial steps for joining (client contacts VAST node at gateway, which rely with the
  closest relay info, client then contacts the relay directly to request join)


2009/04/22 (3)
--------------
goal:   creation of Client and Relay nodes

DEBUG
- serialize / deserialize methods in VASTTypes
  virtual functions add 4 bytes to the sizeof () operator to a class,
  so inherentance a Serializable class would cause problems when using purely sizeof () and
  memcpy (this...) to serialize the class.

  re-implement all serialize (), deserialize () methods for Serializable classes

IMPORTANT:
- byte-aligned all basic types to 8 bytes, which is VC's default


2009/04/21 (2)
--------------
goal:   correct id2host mapping
        VAST check whole process,

- id2host mapping done



2009/04/18 (6)
--------------
goal:   correct id2host mapping
        VAST check whole process,

- work on FLoD-IC draft


2009/04/17 (5)
--------------
goal: VAST check whole process,
      Relay, Topology class implementation

- remove Relay class (realize it can be implemented in the main VAST class
  for performing both Client and Relay tasks)

- unique ID assignment / login procedure done
- find out id2host mapping in MessageQueue needs to be implemented and is important



2009/04/16 (4)
--------------
goal: unique ID assignment, VAST implementation

- finished unique ID assignment,
- compiled first VAST library

DEBUG
- many linker errors at first, resolve by building VASTnet & common as static libraries
  then errors occur as MFC libraries are staticially linked in both VASTnet & common
  projects. Resolved by linking MFC dynamically as DLL.
  Finally resolve linker problems by not using any netemu_bl classes (not converted yet)


2009/04/15 (3)
--------------
goal: VASTnet refactor,
      unique ID assignment


- try out SVN and backup everything to it (both VAST, FLoD & Plug)
- can compile VASTnet except net_emu_bl



2009/04/13 (1)
--------------
goal: finish VASTnet refactor
      finish VAST modificiation (area subscription, point publication)

- consolidate various VASTnet classes. compiled most of them now.



2009/04/12 (7)
--------------
goal: revise VASTnet & message handler mechanism
      (support easy logical node implementation & migration)

- realize nodeID + messageType would be enough for unique handler identification
  no need for GroupID or HandlerID as MessageQueue can do the necessary mapping
  lookup

- also connection/disconnection from VASTnet can be removed, just use a notify
  function for storing nodeID -> address mapping and timeout for removing
  unused connections



2009/04/10 (5)
--------------
goal: debug & revise VASTnet interface / data structures (compilable)


2009/04/09 (4)
--------------
goal: define all interface and data structures & figure out how they work together

CHANGE:
separate existing classes into VAST, VASTsim, VASTnet, common libraries

BUG: when building VASTnet, all files compile but generate the following linker error

net_ace.obj : error LNK2019: unresolved external symbol "__declspec(dllimport) public: __thiscall ACE_Thread_Mutex::ACE_Thread_Mutex(wchar_t const *,struct ACE_mutexattr_t *)" (__imp_??0ACE_Thread_Mutex@@QAE@PB_WPAUACE_mutexattr_t@@@Z) referenced in function "public: __thiscall Vast::net_ace::net_ace(int)" (??0net_ace@Vast@@QAE@H@Z)
net_ace_handler.obj : error LNK2001: unresolved external symbol "__declspec(dllimport) public: __thiscall ACE_Thread_Mutex::ACE_Thread_Mutex(wchar_t const *,struct ACE_mutexattr_t *)" (__imp_??0ACE_Thread_Mutex@@QAE@PB_WPAUACE_mutexattr_t@@@Z)
net_ace.obj : error LNK2001: unresolved external symbol "public: virtual int __thiscall ACE_Shared_Object::init(int,wchar_t * * const)" (?init@ACE_Shared_Object@@UAEHHQAPA_W@Z)
net_ace_handler.obj : error LNK2001: unresolved external symbol "public: virtual int __thiscall ACE_Shared_Object::init(int,wchar_t * * const)" (?init@ACE_Shared_Object@@UAEHHQAPA_W@Z)
net_ace.obj : error LNK2001: unresolved external symbol "public: virtual int __thiscall ACE_Shared_Object::info(wchar_t * *,unsigned int)const " (?info@ACE_Shared_Object@@UBEHPAPA_WI@Z)
net_ace_handler.obj : error LNK2001: unresolved external symbol "public: virtual int __thiscall ACE_Shared_Object::info(wchar_t * *,unsigned int)const " (?info@ACE_Shared_Object@@UBEHPAPA_WI@Z)

DEBUG:
Found it's because ACE was built with VC7, after building new ACE with VC9 it gets fixed.


2009/04/08 (3)
--------------
goal: define data structures

- completed half, defined unique ID format, simplify MessageID (remove handlerID concept)


2009/04/03 (5)
--------------
goal: revise VAST paper sections (done 4/6)
      integrate VAST with modified networking layer (by Marvin)


2009/04/02 (4)
--------------
goal: modify VAST interface to support SPS
      integrate VAST with modified networking layer (by Marvin)

- modify the new VAST interface in VAST.h

coding roadmap:

- modify interface (include network layer)
- integrate network layer

- three main components
    - logical client's VON (join, move, leave, list)    VON
    - publish support                                   pub/sub
    - relay join / modify / leave                       topology-aware

- simulation evaluation (we need to simulate a physical network?)
    - nodes join / leave (churn)
    - movements (sub area moving)
    - publish (text chat?)
    * should see that subscriber list is up to date, publications are sent correctly



2009/03/11 (3)
--------------
goal: make cleaned up VAST version compilable

- done. however, still can't run properly due to some bugs


2009/03/04 (3)
--------------
goal: re-check / refine interface defintion for VAST.h & Network.h
      estimate schedule and workload
      write down / revise major procedures




2009/03/03 (2)
--------------
- restart log for VAST (SPS version)

goal: re-check / refine interface defintion for VAST.h & Network.h
      estimate schedule and workload




2008/11/22 (6)
----------
goal: remove IP-lookup from Network. Compile existing functionalities


2008/11/21 (5)
----------
goal: remove IP-lookup in Network interface. Make it compilable with existing
      functions

- see that we need to extract common functionality from net_emu & net_ace
  such as send/recv messages

2008/11/20 (4)
----------
goal: remove complicated stuff from network interface (simplify!)

- simplify Network.h interface, compile and fix places where the code breaks

2008/11/19 (3)
----------
goal: remove complicated stuff from network interface (simplify!)



2008/10/15 (3)
--------------
goal: clean up networking layer
      fix memory leak bug (VAST consumes continously increasing memory as simulation proceeds)


2008/10/14 (2)
--------------
goal: clean up networking layer
      fix memory leak bug (VAST consumes continously increasing memory as simulation proceeds)

- removing unused codes

2008/08/27 (3)
--------------
goal: make better consistency
      provide same functionality with peer subscription (manager need not move)

DEBUG (consistency improvement)
- consistency improves dramatically after the following fixes
    * no teleport check (do a JOIN only the first time)
    * clear up _new_neighbors every time after it's used
    * do not clear entries in _new_neighbors simply due to delay (during bootstrapping some NODE info are late)
    * add back 'time' to Node structure. Also timestamp node info when they're sent out by MOVE messages

DEBUG
- consistency was bad if IP address mapping lookup is used. (better consistency is achieved by including Address within Node structure)
  found out it's because when doing insertNode, if no address are provided, a connection should still be attempted.
  In other words, regardless of whether IP address is provided, insertNode () should always try to establish connection.




2008/08/26 (2)
--------------
goal: debug VAST so VASTsim_gui is runnable
      provide same functionality with peer subscription (manager need not move)


DEBUG
- found out when deleting an item in a vector or map, *always* record the item index to be deleted in a list first,
  then delete in order. DO NOT delete while going through (iterating) the list.

DEBUG
- if program crash when deleting something, it's possible it's an exported class that gets NEW outside of its DLL, then attempts
  are tried to delete it outside of DLL. Such can happen if the exported class doesn't have STL variables such as map or vector
  (because they're NEWed inside the DLL when used). Solution: *always* use a factory class for creation / deletion of DLL objects
  this is new/delete are always kept inside the DLL


- Modify JOIN mechanism. Now only if a 1st time join will a manager contact the gateway.

DEBUG
- put back 'time' into Node (seems like Node position updates need 'time' in order to be correct)


BUG:
- seems like already known nodes to each other would not update each other about its own position correctly (but others are updated).
  not sure why..


DEBUG:
- when doing checks for ENs, one should also check if an enclosing neighbor is a "relevant" neighbor (not just AOI neighbor) to
  the moving / joining node. (this way we would not miss non-AOI, but enclosing neighbor discovery)


DEBUG       An important design decision is when nodes connect to new neighbors, how should the remote address be known?
(big)       original VON attaches address info along with Node structure, so every neighbor notification tells the network
            address. In a later attempt to try to hide as much Address from the overlay as possible, the upper layer only
            notifies "which nodes might know the address of which other nodes", and thus the network layer can
            query on its own. This somehow can work.. provided such information about "address knowledge" are kept correctly.

            fix the "neighbors do not update each other" bug above, after adding bogus address during insertNode.
            reason is that HELLO messages are not able to reach newly discovered neighbors, due to the lack of address
            knowledge.




2008/08/25 (1)
--------------
goal: VON functionaltiy
      Manager can join overlay & move (maintain proper AOI neighbors)
      implement: join (), leave (), manage (), subscribe (), move (), getManagers () in new VAST


- NOTE: need to make sure when disconnect () in MessageHandler is called, the same physical link is not disconnected until
        no handler is interested in the physical link any more (because it's possible more than one handler could utilize
        the same physical link)

- the above is done at 16:20. remove the _net pointer in MessageHandler and allows only MessageQueue has access to network interface.
  all connect/disconnect are now handled by MessageQueue where it can keep reference count for each handler that connects to a remote node
  only reference count reaches 0 will actual physical disconnection occurs



DEBUG
- got a really strange bug that after new VAST is buildable, building VASTsim (exactly the same code) produces lots of syntax errors.
  After spending 40 min. found out it was because there's a new DEFINE in VAST called AOI_BUFFER that replaces the AOI_BUFFER parameter
  used in VAST.h

- finished integrating old VAST code into new VAST, but running GUI still requires debugging.



2008/08/24 (7)
--------------
goal: implement half of the functions in VAST (new Peer and Manager class)
      Manager can join overlay & move (maintain proper AOI neighbors)

- made MessageHandler & MessageQueue work with virtualized handlers (each node can have several handlers serving manager or peer roles)

TODO:
- found a fairly serious design flaw / limitation, where the unique id for each node is assumed to coincide with the network layer's node ID.
  This has the convenience of being able to use NodeID directly to send / recv messages. However, it creates problems when we want to
  support multiple handlers to be used with a network address (IP/port pair). As unique handlerID may consist of more than just nodeID or
  networkID.

- convert existing VAST into new generic overlay VAST.. compilable but VASTsim cannot compile.



2008/08/23 (6)
--------------
goal: make new VAST interface compilable with modified VASTsim components
      implement half of the functions in VAST (new Peer and Manager class)


- copy from OGRE the following, to mask the warning about MessageQueue's use of <map> as part of
its private variables

- add the following
// disable: "<type> needs to have dll-interface to be used by clients'
// Happens on STL member variables which are not public therefore is ok
#   pragma warning (disable : 4251)

- made VASTsim compilable under new VAST functions.

- add Peer.h and Manager.h class, inherting MessageHandler.  Thought of modifying id to accomodate virtual node roles.
  But cannot find a uniform way to put different handlers of messages (VAST, Peer, Manager, VSM... ) using a
  unified framework.



2008/08/22 (5)
--------------
goal: make new VAST interface compilable with modified VASTsim components
      implement two functions in new VAST


- new VAST buildable  :)
- introduce new classes

    MessageHandler      - can be inherented by any component pluggalbe into the message queue
    MessageQueue        - represents a unique IP/port interface to network

- various fixes to make the new library buildable



2008/08/21 (4)
--------------
goal: make new VAST interface compilable with modified VASTsim components
      implement one function in new VAST

- need a proper messaging mechanism / network abstraction that allows messages be sent & received over the network,
  but also usable by different components. i.e., a shared hub for network messages



2008/08/20 (3)
--------------
- can compile old VAST under VC9

DEBUG
- a linker error where .lib for VAST or VASTsim is not found. Solved first by copying old project file.
  later found out it's because the WIN32;_WINDOWS macros are not defined


2008/08/19 (2)
--------------
- starts work on generic overlay VAST
- create new project under VC9 for VAST, compilable, also include VASTutil inside VAST,

BUG:
- VASTsim would be unable to link to some vastverse or VASTutil functions



=============================



2008/03/26
----------

- first try out SVN to get current development from cscsx
- refactor insert_node () so there's only one version (instead of overloaded ones)



2007/12/24 (1)
--------------
goal: merge movement behavior generator for both FLoDsim & VASTsim

VASTsim now workable version with new VASTutil behavior & file logger

DEBUG
- get rid of a crash bug caused by the 'delete' of a map within MovementGenerator
  by moving the new and delete into .cpp instead of having them in the .h
  (something to do with memory allocation / de-allocation cannot occur accross DLL)

DEBUG
- found out VASTsim has very poor consistency in 10,000x10,000 world with 500 nodes.
  discovered reason is that join is not yet finished for all nodes before simulation
  starts. Fix by requiring all nodes pass the is_joined () test before starting
  simulations


2007/12/22 (6)
--------------
goal: merge movement behavior generator for both FLoDsim & VASTsim




2007/05/01 (1)
--------------

major DEBUG:
[linux] when demo_gateway is run on Linux, it cannot link back to a U.S. node/host.
- a second cause has been found that in net_ace's disconnect, the mutex for connection object
  should not be used

(potential) BUG:
- VAST's send/recv msg now does not use 'recvtime' but the time received is actually the sender side's



2007/04/30 (1)
--------------

major DEBUG:
a strange bug while using FLoD that the server is usable under Win32 but not Linux.
- found out it's due to inconsistant byte alignment of the VAST structures between Win32 & Linux.
  So that even the first QUERY message cannot be properly processed.



2007/04/19 (4)
--------------
- goal: release VAST 0.3.2

RELEASE


DEBUG:
DD & RS print-out are out of range and very high. Found out later it's because yuli has recorded
and printed out deflated-send/recv and didn't print the captions, yet the deflated-send/recv sizes
do not record correctly if message compression is not used. temporaily disabled printing if
not using the VAST_MC model.





2007/04/11 (3)
--------------

additions:
- when sending DISCONNECT should use timestamp = 0 in net_emu
- when doing sendmsg () or flush () in net, should return actual # of bytes sent instead of supposed bytes



2007/03/31 (6)
--------------
- ACE 5.5 built successful on linux, after failing manual build, downloaded pre-built
  RPM at http://dist.bonsai.com/ken/ace_tao_rpm/

  then build vast-c++-0.3.1 successful with ACE


2007/03/22 (4)
--------------

MAJOR CHANGE
- add 'vastbuf' class and used for all send/recv buffer within VAST (so that message of
  any size can be sent without buffer overflow). However, this does not apply to
  UDP messages


2007/03/15 (4)
--------------
- hide VASTID from overly exposing (so that other libraries won't have to link to VAST
  all the time)

DEBUG:

found out chatva client not responding is due to:
1. when gateway replies ID it's not using reliable delivery
2. chatva does not run tick () when ID is not yet obtained, therefore it never processes
   incoming messages

* chatva runs succesfully to up to about 40 nodes (starts to fail joining) in Released mode
  but CPU utiliziation is around 80% and seems like the problem might be caused by the
  local host not being able to handle it...




2007/03/10 (6)
--------------
- goal: write visual diagnostic for voronoi construction (large data-set verification)

DEBUG:
found out topology consistency decreases dramatically for VASTsim in the new model.
discover the cause as logical time not advancing (vastverse is not tick'ed) and nodes
do not process messages as nodes join (so too many join request may accumulate). Solve
this by adding back the node processing. however this effectively decrease the
data collection.



2007/02/09 (5)
--------------
- goal: finish adding new materials to NOSSDAV draft

paper additions:

- add considerations for
- smooth out paper





2007/01/30 (2)
--------------
- goal: results, convert to Latex

DEBUG: peers have very large IDs
       cause: VASTATE is not ticked when peer joins in the beginning.. causing many IDs to be
              assigned to the same peer

VASTATE
BUG
- peer receives message deleting its own avatar object
- arbitrator thinks it is owner, then delete a peer object, notifies others (so no one now has it)
  yet the peer still wishes to connect...

- notices that after a long pause at one step, many pos_version = 0, then crashes...


what would be some of my own questions? for VSP..?

failure
 - what happens when node fail ? arbitrator fail.. can you recover? under what condition?
   how much cost? how effective is it?

load balance
 - will nodes really distribute load? how much cost? how effective is it?
 - load of the server/arbitrator, initially? later?

scalability
- can your system scale?
- what are some bottleneck points? in what situation..

bootstrapping
 - how is server inserted?
 - how much arbitrator capacity is required?

others
 - can an event / object becomes invalid?
 - how does node capability percentage/distribution affect loading/load balancing?


Overload/Underload threshold
27, 66, 191kb  for min, avg, max during randomwalk test (600x600, with 100, 200 & 400 people)
                                                                        11, 22, 44  / region (out of 9)
  46.5

  50            & 130kb
  underload       overload



2007/01/29 (1)
--------------
- goal: finish whole draft
- done: procedures, ver 1-0.

VASTATE bug:
- ownership received but object doesn't exist..

FUTURE WORK:
- for boundary arbitrators, they could backup to nodes that'll never be able to take-over
  (thus wasting storage...)




2007/01/28 (7)
--------------
- goal: finish draft (procedure - morning, evaluation - afternoon, intro & background - evening)
- done: psuedo code

VASTATE bug note:

- a peer sends three movements + one grab event of food to arbitrator (all delayed to be executed
  in the same run), event may not be forwarded to the enclosing neighbor as forwarding
  was based on current location. if the 'food' is in another region, the region owner may not
  have received & executed the event

- when arbitrator updates its list of enclosing arbitrators (EAs) and find that a node is no longer the
  EA, it would send DELETE to that EA for objects it owns. This currently works as VAST does not
  disconnect right away non-EA nodes, and the EA judgment is done internally on a separate Voronoi
  by the arbitrator. However, in real situations, it's possible the EA link is already destroyed by
  VAST, making DELETE unable to reach the EA, causing zombie objects.




2007/01/26 (5)
--------------

- VASTATE: events that create object may be processed by all enclosing arbitrators
            -> multiple ownerships


            - peer if join fail, then need to re-join via Gateway
            - send 'tick' for objects, record last updated event, remove aged zombie objects
            - rockets should create its own event by arbitrator


new arbitrator notification should send along pending events



2007/01/17 (3)
--------------
- goal: finish VASTATE implementation

- LEAVE handled for both peers & arbitrators (fault-tolerance)
- arbitrator promotion/demotion requests mechanism changed (to querying the VASTATE interface)
- AOI adjustment for arbitrators


2007/01/16 (2)
--------------
- goal: finish VASTATE implementation


- enable new ID scheme in VASTATE (compilable)
- add cleanup mechanism to net_emu in VAST
  (when stop() is called, all connections are dropped/disconnected,
   also, message queue is freed up in destructor)


2007/01/15 (1)
--------------
- goal: finish VASTATE implementation

- "chatva" now workable with the new VAST (separated ID assignment)

- VASTATE modified to suit the new VAST (compilable), but initial ID assignment not yet integrated

DEBUG
- add critical sections to various access to the network connections in net_ace to avoid thread
  contentions of the connection (sending while deleting)



2007/01/10 (3)
--------------
- goal: add LEAVE procedure
        modify network.h to add message handler hooks & subsequent user behaviors (VAST, VASTATE, FLoD)




2007/01/09 (2)
--------------
- goal: debug the remove non-AOI objects for arbitrator

DEBUG
- discover that the bug was caused by non-AOI objects' positions are no longer sent to the
  arbitrators that cannot see them, the positions were there still out-dated and will not be
  removed by the remote arbitrators' removal mechanism. Fix the problem by having the sending
  arbitrator makes all the decisions (including whether to remove non-visible AOI-objects, by
  sending a OBJECT update with pos_version = 0).



2007/01/03 (3)
--------------
- goal: implement LEAVE mechanism in VASTATE



2007/01/02 (2)
--------------

DEBUG
- discover a bug in net_ace_handler attempt to delete a UDP pointer to an
  ACE_SOCK_Dgram object, which was actually supplied by net_ace. So modify the UDP socket creation
  process and create the openUDP () for net_ace_handler. Tested with 3D streaming app
  where program exit no longer crashes

- another crash bug when terminating, is when deleting a vast node under VASTsim.
  cause is that the 'network' interface for some nodes are already removed, yet other nodes would
  still attempt to use them in order to call remote_disconnect (). Simple fix by unregistring
  the network object from bridge when deleting.

- increase buffer size from 4096 to 40960 as if a TCP message exceeds 4096 the program will simply
  crash


2006/10/03 (2)
--------------
- goal: add 'storage' interface to VASTATE (allow query for stored data item without necessarily using C/S or distributed
       query/delivery).




2006/09/27 (3)
--------------
- goal: debug VASTATE (remove non-AOI crash bug & consistency decrease)

- remove non-AOI objects for arbitrators now no longer crashes (as now all objects are created within
  an arbitrator's AOI. however, removal would decrease the update consistency for states. Also,
  discovery consistency is not 100% some of the times, reasons still unknown.





2006/09/26 (2)
--------------
- goal: debug VASTATE (remove non-AOI crash bug & consistency decrease)


BUG (big):
    1) in VAST, a boundary neighbor may not properly notify for new neighbor discovery, if it considers
       the moving node and the new neighbor as enclosing neighbors of each other, when in fact they
       aren't (this is due to the boundary neighbor not knowing another more distant neighbor that
       prevents the moving node and the new neighbor to be mutually enclosing.)

       soluions? (connect beyond EN set when connection limit is not yet reached? in other words,
       dynamically increase AOI-radius when CN is not reached, and adjust back to maintain fixed
       number of connected neighbors?)


2006/09/20 (3)
--------------
- goal: debug VASTATE, add arbitrator object management (removing non-AOI objects periodically)


- add 'beh_clustered' to the stock of behavior_models



2006/09/19 (2)
--------------
- goal: debug VASTATE

DEBUG:
    1) found a bug where the arbitrator notifies a peer of new objects (using extended AOI-radius)
       but the peer immediately removes it by using only its original radius to check
       (object undiscovery therefore occurs)

    - solve this problem by dividing the client object discovery process into two parts:
      storage & notification. where a notification occurs only if the object comes into AOI view.
      but will only be remove from object store if it leaves an buffered AOI.  update consistency
      has come to 100% after 3 steps in most cases.

- convert project file to .net2003 (all projects compilable)


2006/09/13 (3)
--------------
- goal: debug VASTATE

DEBUG:
    1) an annoying bug where many times, the program would crash within sfvoronoi, where doing
       a free (buf_list[]) in calvs() tries to free invalid buf_list.

       the bug (found by ¤Öºd) was that in processmsg of VAST, we'd collect stat info of msgtype
       via

       _msg_stat[msgtype] += size;

       But after VASTATE is added (and it uses msg number starting from 100), it access
       unallocated memory in _msg_stat, causing data in Voronoi to corrupt.

       A fairly easy fix, but difficult-to-trace problem.

       after this is solved, not only voronoi works properly now, a previous "unable to run under
       Release mode" problem is also solved.


_msg_stat[msgtype] += size;

2006/09/09 (6)
--------------
- goal: debug VASTATE

DEBUG:
    1) after updating interests using not just owned objects, but all known objects
       (but do not use arbitrators as destinations), the program would mysteriously crash
       upon init. Trace reveals that the map '_interested' contains NULL items
       (that is, a 'find' would get something, but the value is 0, or NULL).
        suspects that it's because of an access such as

        notify_peers (_interested[info.obj_id], STATE, msg, size);

        although no value is assigned to _interested, by accessing it using [] operator,
        a NULL value is put into the map

      * confirmed the hypothesis and solved the bug at 18:00

BUG:
    1) small bug, if a node doesn't move, it won't be made known to a peer joined later
       (even if it's in the AOI of the late peer). Initial object notification wasn't done properly?


2006/09/08 (5)
--------------
- goal: debug VASTATE

BUG:
    1) neighbor undiscovery across regions (peers cannot learn of AOI neighbors in another region)
    2) removal of non-AOI objects

MAJOR CHANGE:
    revise directory structure, now into the following:
        /bin        - binaries
        /demo       - demo files (chatva, gateway)
        /errout     - error_out
        /include    - shared include (VAST, VASTATE, VASTsim, VASTATEsim)
        /lib        - libraries (unused)
        /sim        - simulations
        /test       - test files
        /VAST       - VAST files
        /VASTATE    - VASTATE files

Projects have also changed names into
    errout
    VAST
    VASTATE
    VASTATEsim
    VASTATEsim_gui
    VASTsim
    VASTsim_console
    VASTsim_gui



2006/09/06 (3)
--------------
- goal: debug VASTATE

DEBUG:
    Another bug found where the peer does not remove non-AOI objects is that: the arbitrators immediately
    stops sending updated position to a peer if the objects become non-AOI, however, the peer thus
    still retains old knowledge and considers the objects to be AOI-objects. So the peer does not
    remove the non-AOI object.



2006/09/05 (2)
--------------
- goal: debug VASTATE

DEBUG:
'_peers' was not updated after initial creation, solves the bug where some nodes stop moving after a while..
(because they were seen as 'not interested' in the object updates, due to their positions were
 never updated after initialization)


2006/09/01 (5)
--------------
- goal: debug VASTATE

BUG:
    1. system consistency in 4 steps would drop after simulation has run for a while
    2. object attribute version would still increase even if they were not modified.




2006/08/17 (4)
--------------
- goal: debug the peer not moving issue

DEBUG
    solved the issue of peers not moving when crossing arbitrator regions.
    found the issue were
        1) the peer did not learn properly the arbitrator list when entering a new region
        2) region transfer check was not called by peer



2006/08/12 (6)
--------------
- goal: finish first implementation of VASTATE  (load - balancing)

- finished first completed implementation of VASTATE


2006/08/11 (5)
--------------
- goal: finish first implementation of VASTATE  (load - balancing)



2006/08/09 (3)
--------------
- goal: add n-tier neighbor to Voronoi class
        finish first implementation of VASTATE  (load - balancing)

- add a 'level' parameter to the Voronoi class's get_en (), so that which levels of ENs may be
  specified.

MAJOR CHANGE:
- change the internal data representation of a "Position" class from 'long' to 'double'



2006/07/28 (5)
--------------
- goal: finish first implementation of VASTATE  (load - balancing)

DEBUG
- solve the event ordering issue by revising net_emu, where messages are now handled by
a multimap with timestep as index, as opposed to the linklist.

DEBUG
    be careful of using the "[] = NULL" to check for something that's not in the map,
    it could very well be that something exists but returns NULL (0) nevertheless..

succesfull make event forwarding to work, however, found out that if two foreign nodes
both try to use their previous values to update a new value, then the effect of
one operation would be cancelled. For example, if node 1 owns A, with value 7.

Then in one step node 2, 3 all try to increment.. with the knowledge that A = 7.
Then both would send in the request of A = 8 to node 1, where after applying both
updates, the result will be A = 8. However, it might sometimes be desirable the result
is A = 9 (applying two +1 operations).

But at least updates are displayed and notifed consistently across nodes.



2006/07/27 (4)
--------------
- goal: finish first implementation of VASTATE  (ownership check/transfer & load-balacning mechanism)

finish 1st working ver of owner transfer check
still has small problem about event ordering (inserting a timestep=0 event would not come
in the beginning of message queue)



2006/07/26 (3)
--------------
- goal: finish first implementation of VASTATE  (ownership check/transfer & load-balacning mechanism)

The process:
    - send in command (an update to a particular/specified object)
    - generate bytestring
    - deliver to event handler (local or remote)
    - execute the request after checking ownership (owned: execute, un-owned: forward to owner)




2006/07/25 (2)
--------------
- goal: finish first implementation of VASTATE

DEBUG
    solved an annoying bug where the encode/decode of messages seem to have problems
    (decoding incorrect bytestring). found out after much inspection that the problem
    was in for loop, the 3rd paremter will actually execute/validate first before seeing
    if the 2nd parameter will run.. and I had an pointer advancing like this

        for (int i=0; i<n; i++, p += sizeof (some class))

    which makes the value of p incorrect..

DEBUG
    spent a lot of time trying to track two nasty bugs:
        1. not setting the 'dirty' flag when doing a position update, as the code checking
           for packing only checks the dirty flag, it makes position update not seen as
           something that needs packing (found this out after also doing a states update, and
           everything turned normal)
        2. another bug relates to when doing simply position update, full states are sent
           as well (even though they weren't modified). found out it was caused by the
           a small bug in pack_dirty (), where even if no attributes were change, the
           packing still continues..

BUG
    still an unsolved bug where when a peer calls create_event, and then send_event,
    the event pointer cannot be deallocate with a 'delete' in send_event (this likely will
    cause memory leak?). using a 'delete' will crash

MILESTONE
    finish complete arbitrator/peer basic functions and debugged.


2006/07/22 (6)
--------------
- goal: finish first implementation of VASTATE

BUG
    pack_dirty would work if all attributes were set again after initial values, but
    if only some are set (others aren't..) then those after a string attribute would be corrupt

DEBUG
    found out that by using string's replace to sub for new string, it creates additional bytes
    after the replacement string (so essentailly adding more data)

    found out it was because when using a string.replace, the new char array should be
    "NULL-terminated!"

- finish implementating & testing packing/unpacking with per-attribute dirty flag!


2006/07/21 (5)
--------------
- goal: finish first implementation of VASTATE


BUG
    very strange bug where if we try to reset values in 'attributes' class, applying update to ALL
    existing values would make program crash when it terminates, but just one less attribute to 'set'
    wouldn't cause problem.

DEBUG
    a silly mistake where pointer was set incorrectly


2006/07/20 (4)
--------------
- goal: draw overall code structure, finish implemention for half of the procedures

- modify the Addr class into having a publicIP and privateIP, also modify respective use in VAST
  (currently only public IP is supported when using real network)

MAJOR CHANGE
- in VAST, create interface "voronoi.h" and rename the following
    SFVoronoi   as vor_SF_algorithm
    Voronoi     as vor_SF
  also move many datatype definition used by Vorononi to 'typedef.h'

- in VAST, change type 'Point' to 'Position' to avoid conflict with 'Point' used in Voronoi


2006/07/19 (3)
--------------
- goal: draw overall code structure, finish implemention for half of the procedures

- all nodes can share the same network emulation (via export of network in 'vastverse')
- first successful ID for peer
- allow server message to be processed through 'chaining' the message processing function
  (after VAST messages). Note: this requires message type IDs are not redundent.




2006/07/18 (2)
--------------
- goal: draw overall code structure, finish implemention for a couple of basic procedures.

- first buildable skeleton
- create 'arbitrator' 'peer' 'arbitrator_logic' 'peer_logic' classes



2006/07/17 (1)
--------------
- goal: finish 'JOIN' implementation for VSP

- revised procedure, protocl, and psuedocode (draft 4.3 completed)

2006/07/16 (7)
--------------

- discuss ways to allow arbitrator only deal once with event by applying game-logics.
  (no forward necessary)


2006/07/13 (4)
--------------
- goal: finish 'JOIN' implementation for VSP



2006/07/12 (3)
--------------
- goal: finish 'JOIN' implementation for VSP

- revised VSP design (use VON for arbitrator layer)

2006/07/11 (2)
--------------
- goal: finish 'JOIN' implementation for VSP

- revised VSP protocol


2006/07/10 (1)
--------------
- goal: finish 'JOIN' implementation for VSP

- explain VSP interface to plug team and get feedbacks

2006/07/09 (7)
--------------
- goal: finish 'JOIN' implementation for VSP

draft first VSP interface (vast_sm.h)



2006/07/07 (5)
--------------
- goal: define protocol & message encoding

finished 1st draft of protocol

2006/07/03 (1)
--------------
- goal: define protocol & message encoding


2006/06/30 (5)
--------------
- goal: start coding of first version of VASTATE


2006/06/30 (5)
--------------
- goal: start coding of first version of VASTATE


2006/06/29 (4)
--------------
- goal: start coding of first version of VASTATE


2006/06/21 (2)
--------------
- goal: finish first draft of INFOCOM paper, with intro, problme definition and proposed solution.

done & turned in to Prof. Huang.


2006/06/18 (7)
--------------
- goal: write first draft of INFOCOM paper, with intro, problme definition and proposed solution.


2006/06/16 (5)
--------------
- goal: finish VSP psuedocode & message protocol/content design

completed first draft of psuedocode (20 functions) 5:31pm


2006/06/15 (4)
--------------
- goal: finish VSP psuedocode & message protocol/content design


2006/06/14 (3)
--------------
- goal: finish VSP psuedocode & message protocol/content design

- finished psuedocode for Peer Join


2006/06/13 (2)
--------------
- goal: finish VSP procedures & psuedocode

- finished the procedures for Peer Join, Peer Leave, Update Subscription, State Update (Peer Move),
  Load Balance Overload (Process A region reshaping & Process B adding arbitrator),
  Load Balance Underload (remove arbitrator), Arbitrator Failure


2006/06/12 (1)
--------------
- goal: create the first design doc for VSP

MILESTONE:
- finished diagrams and the first design doc in time for discussion at the Plug meeting


2006/06/11 (7)
--------------
- goal: organize ideas for Voronoi-based States Partitioning (VSP), read the Gamasutra articles on
        network programming (X-wing vs. Tie & AOE).

- finished reading the articles
- wrote down design concepts for VSP


2006/06/03 (6)
--------------
- goal: create 'behavior' interface (separate code and function)

random waypoint (RWP)

idea: 'vastate' for "Vast-State" management library.. :)



2006/05/31 (3)
--------------
- goal: release VAST 0.2.1

Release Notes:

This is a maintenance release that changes the memory allocation of Voronoi
class (SFVoronoi) to be dynamic. This allows more neighbors to be kept by each
node during simulations. This release is also used to generate simulation
results for the IEEE Network article: "VON: A scalable peer-to-peer network for
virtual environments" (http://vast.sourceforge.net/docs/pub/2006-hu-VON.pdf). An
improved mechanism to collect statistics during simulations
(vastsim/statistics.h) is also added.

Change Notes:

The major changes are:

1) Dynamic memory allocation for Voronoi construction (simulation now scalable to over 2000 nodes)

2) Node position record has changed from text to binary in order to save space.
Position logs are also renamed from *.txt to *.pos.

3) Improved statistics collection.


2006/05/29 (1)
--------------
- goal: write a gateway node runnable on Linux

first successful gateway node running on Linux (after compiled in ACE 5.5).


2006/05/27 (6)
--------------
- goal: write a gateway node runnable on Linux



2006/04/07 (5)
--------------
- goal: finish IEEE Network draft and submit
        (improve the dAOI curve/transmission size by reducing NODE messages)

- after some experiments, change the AOI adjustment policy from a ratio of current AOI
  (AOI_ADJUST_RATIO) into AOI_ADJUST_SIZE, where different methods can be tried, such as

  #define AOI_ADJUST_SIZE             (_detection_buffer / 3)
  #define AOI_ADJUST_SIZE             (5)
  #define AOI_ADJUST_SIZE             (_self.aoi * 0.05)

  experiments show the specifying a fixed size works better

- some other experiments show that when MAX_ADJUST_COUNT and MAX_DROP_COUNT are
  of the same number, the topology consistency is better (not sure why yet..)
  after some experimentations set it to 4. (higher number like 8 actually gives only
  marginal boost for higher transmission)

- some current parameters:

    MAX_ADJUST_COUNT    4
    MAX_DROP_COUNT      4
    AOI_ADJUST_SIZE     5
    _detection_buffer   15



2006/04/05 (3)
--------------
- goal: re-write a cleaner VASTsim (statistics and vastsim.cpp)

- decide to record all data since beginning (no stablizing steps, except just to record
  the first step to achieve 100%). But for transmission sizes, also record for each interval
  (10 intervals for the whole simulation) separately for later processing (to possibily skip
  the first interval, where the system has not yet stablized)

MILESTONE:
    revised the stat collection mechanism, now time-series of all data are being recorded
    (snapshots)

BUG
    found out TC still isn't 100% even with low # of nodes (e.g. 30). Found out one
    possible scenario is that the missing neighbor need to be notified by another neighbor
    that is itself also just discovered. Seems like there isn't an easy fix for that..

Discovery:
    1. interesting that increasing the # of MAX_DROP_COUNT from 2 to 3, increases the
    transmission, but actually makes TC slightly lower.

    2. it seems like some undiscovery when using dAOI occurs because a notified node
       doesn't view a remote node as relevant (possibily due to different AOI-area, or
       other cause?), yet its neighbor thought they have already notified.. so when
       the new node is in fact relevant (in a short time), the node that should know
       isn't aware. Perhaps a solutoin is to keep a few nodes deemed irrelevant at hand,
       and check for a few rounds before dropping? (just like making disconnections)





2006/04/03 (1)
--------------
- rename 'metaverse' class to 'vastverse'


2006/03/20 (1)
--------------
goal: create a stable simulation version for running on Linux
      (change position records from text format to binary)

- changed default time-step per second from 10 to 5



2006/03/19 (7)
--------------
goal: fix the fixed memory allocation issue in SFVoronoi

DEBUG
- solved the problem in 03/18 by allocating memory when needed, but also record a list
  of memory allocated, then re-organize them into a continous piece of memory it needs
  to be completely cleared (i.e. the Voronoi is redrawn)
  however, initial testing shows that use of memory still continous to increase when
  simulation is running. potential leakage (or normal use of memory) might be
  in the 'statistics' class or 'vast_dc.cpp'



2006/03/18 (6)
--------------
goal: review simulation code for potential unstability,

- remove the use of "siteBuffer" and converted 'sfvBuffer' to be dynamically allocated.
  introduced two define values:

        NODESIZE_INCREMENT  50      // how much space should be allocated if not enough at a time
        MEMSIZE_INCREMENT   25000   // how much memory to allocate when not enough

BUG     problem exists for using "realloc()" to increase the allocation of sfvBuffer.
        As various variables will use the memory allocated for 'sfvBuffer'
        (they keep the memory address), so once realloc has increased the memory,
        those references immediately becomes invalid and unaccessible (cause program
        crash when used). Increasing the MEMSIZE_INCREMENT helps, but is not a
        flexable solution (i.e. it still depends on how much space will be allocated
        during one construction of Voronoi diagram).



2006/03/17 (5)
--------------
goal: review simulation code for potential unstability,
      check on suitabliility of scalbility references
      (possibily removing the scalability analysis paragraph in VON paper)

- made a decision today to let another person work on the simulation in full.
  today asked æ³ç? to help.



2006/03/14 (2)
--------------
- today asked å°æ? to help on NAT.

2006/03/13 (1)
--------------
- goal: reliable & unreliable payload transmission

- made a decision today to delegate implementation of NAT and Linux Makefile setup to
  someone else, and start to work on FLoD's implementation using Python now.
  But before transferring the task, finish reliable & unreliable PAYLOAD transfer first
  (completed with a test program - 'chatva' that allows chatting).



2006/03/11 (6)
--------------
- goal: VAST NAT transversal & gateway node runnable in Linux


DEBUG   after decreasing the update rate from 10/sec to 5/sec, much of the the unstable bug
        in yesterday's tests disappear. So a reasonable guess is that the problem is caused
        by too much I/O at once (running many copies of 'chatva' at the same time, each
        updating at 10 times / sec). Now up to 25 nodes can run reasonably well.

        Also increase the screen/input update to 20 times / sec (50 ms interval).
        So the keyboard appears to be more responsive.



2006/03/10 (5)
--------------
- goal: UDP for VAST + NAT transversal

- UDP done :)  (after reading much about the story and fading of Ken Silverman.. )

  the trick was to first confirm that it was okay to listen for both UDP & TCP traffics
  on the same port. Then I assume the ACE_Reactor should probably work for the message handler
  (so must be something else that went wrong). I went to the text debug mode where every detail
  message was printed, to see if the UDP handler was in fact invoked.. It was, but then it
  went on to use the TCP stream object to receive message (no wonder..). Write another segment
  of code that specifcally obtains UDP traffics and things worked.

BUG     There's still a bug where occasionally a node wouldn't be able to join properly..
        (after some testing it seems like it's when NODE is sent via UDP.. if the initial NODE
         message isn't properly receive, likely it won't join properly)

BUG     if a node just keep moving when there are many nodes.. program would crash for
        unknown reason (join was okay for up to 25 nodes, but constant movements would lose nodes)


2006/03/08 (3)
--------------
- goal: UDP for VAST


2006/03/07 (2)
--------------
- goal: UDP for VAST

- added delivery of NODE & MOVE messages in UDP mode, however, receiving is still a problem.


2006/02/22 (4)
--------------
- goal: real network layer for VAST C++ version using ACE (debug & use UDP)

- rebuilt VAST with VC++ 2003 .net, found out the problem was that ACE itself needs to
  be rebuilt. Spent quite some time making a version both buildable from VC++6 and VC++ 2003.net

CHANGE
    - modify all 'float' in the SFVoronoi class to 'double' (consistent with the Java version)
      also avoid annoying warnings of type conversion from VC++.net


2006/02/22 (3)
--------------
- goal: real network layer for VAST C++ version using ACE (debug & use UDP)

- undergrad special project student reported that VAST isn't buildable with VC.Net


2006/02/17 (5)
--------------
- goal: real network layer for VAST C++ version using ACE (testing)

DEBUG:    solve the concurrent window problem by checking if I"m the current active window
          by calling GetActiveWindow ()

MILESTONE:
        multiple clients connectable using real networks (TCP only, 10 updates/second)
            known issues:   1. some later nodes are not joinable (learn no neighbor)
                            2. not NAT-transversable


2006/02/17 (5)
--------------
- goal: real network layer for VAST C++ version using ACE

DEBUG   - solved the IP address = 0 issue.
          problem was that when a new node is added (insert_node) _id2addr wasn't even updated.
          it was a problem as there are two ways register_conn () might be called (either
          self-initated via connect (), or through a remote connection called from the
          'net_ace_handler'. Originally thought of storing address in register_conn,
          found out it wasn't possible to extract valid remote IP address with just
          the 'ACE_SOCK_Stream' object. So had to store it in connect () (which will
          still be called in the case of remote connection, as it will be executed
          within insert_node (). Still feel that the process is too complicated
          (therefore hard to maintain), but for the time being, seems like it's working now
          (can connect several gui clients together via real networks..)

BUG     small bug where multiple gui clients (chatva) all receive the same inputs.


2006/02/16 (4)
--------------
- goal: real network layer for VAST C++ version using ACE

DEBUG   - solved a hard-to-find bug where IDs received is always 1. Found out it's due to
          the message-receiving processmsg() in net_ace_handler.cpp not passing the right
          pointer position to the next function -- storemsg (it passes the pointer position
          of the original 'msg' variable instead of the pointer that has been advanced
          during extraction of id, msgtype, timestamp info.

BUG     - found another bug where the NODE message received seemed to contain bogus IPs
          (0 for both IP & port)


2006/02/15 (3)
--------------
- goal: real network layer for VAST C++ version using ACE

- compiled & run success for single node, however, as IP used in vast_dc were fake, still not
  functional. change fake IPs.


2006/02/14 (2)
--------------
- goal: real network layer for VAST C++ version using ACE

- has finished most filling-in of the net_ace related classes/files.


2006/02/13 (1)
--------------
- goal: real network layer for VAST C++ version using ACE

BUG
- found out the crash problem of using ACE_Task somehow has to do with calling it from
  a WinMain instead of main. right now can't seem to find solution to it (might resort to
  not using ACE_Task, just reactor then..)

DEBUG
    found a solution for the above bug -- very simple: call ACE::init() in the beginning
    of WinMain()



2006/02/06 (1)
--------------
- goal: real network layer for VAST C++ version using ACE

- re-organizing the directory structure

    OLD                 NEW
    ===                 ===
    include, src        vast
    src_sim             vastsim
    src_test            demo


NOTE:   inclusion of ACE into VAST causes the following warning in Releaes mode
            LINK : warning LNK4089: all references to "WS2_32.dll" discarded by /OPT:REF

        but it is harmless (simply means that WS2_32.dll isnt's used)

DEBUG   - found lots of "error LNK2001: unresolved external symbol" when trying to test
          class 'net_ace' independently. These unresolved external symbols errors
          are either caused by
            1) not include the proper *.lib file or
            2) the class itself wasn't decleared as EXPORT


2006/02/05 (7)
--------------
- goal: real network layer for VAST C++ version using ACE

DEBUG   - change all instance of variable 'set' to 'sett'
        - met with lots of ACE linker errors
            metaverse.obj : error LNK2001: unresolved external symbol "public: virtual int __thiscall ACE_Task_Base::svc(void)" (?svc@ACE_Task_Base@@UAEHXZ)
         (solved by including the ace.lib, aced.lib in Project/Settings)

- added new project 'multiplayer' for a non-simulation mode for testing VAST
- created skeleton for 'multiplayer' (compiled & runnable - displays a movable node with its AOI)

DEBUG   - found the project multiplayer wasn't using Multithread library (fix that)
        - change library used by VAST and VASTsim to be multithread DLL instead of just multithread

BUG     - if any ACE function is called (activate or find local IP) the program crash




2006/02/03 (5)
--------------
- goal: real network layer for VAST C++ version using ACE

BUG     - encounter a strange error in ACE's include files

            error C2955: 'set' : use of class template requires template argument list
            h:\program files\microsoft visual studio\vc98\include\set(124) : see declaration of 'set'

        the problem is caused by a function variable also name 'set' (however, shouldn't
        function variables be seen differently from template names?)



2006/01/19 (4)
--------------
- goal: make VAST java version release 0.1.0

- found out the useful 'ByteBuffer' class to encode messages in bytes (now only 24 bytes for
  a node's information)

DEBUG   - solved a buffer not enough issue (when node size is large) by noticing that
          the same VASTMessage object was being used for sending PEER messages to different nodes.
          added a clear() method to reset the VASTMessage buffer
        - solved another memory issue with large size caused by too many nodes are being
          sent for notification. Now each PEER message is capped to 20 nodes max (so if
          more than 20 nodes need to be notified, then it'll be sent multiple times),
          this way we keep our VASTMessage buffer to be just 512 bytes.




2006/01/10 (2)
--------------
- goal: demo-able JADE-based VAST applet

successfully demo with some GUI improvements (global & local view clickable).


2006/01/06 (5)
--------------
- goal: convert SFVoronoi to Java

BUG     detect a memory leak in the memory allocation/management of the C++ version of SFVoronoi.
(big)   basically each time the Voronoi is re-calculated (recompute in Voronoi.cpp),
        new block of memory is allocated with freeinit in SFVoronoi.cpp without releasing
        previously allocated memory blocks (no wonder simulation tend to break down with too
        many sites or after running for too long).

        to-fix (SFVoronoi.cpp)
            in clearAll():  need to free up memory allocated with myalloc() as well
            in myalloc():   need to allocate dynamically (not just fixed size)

        notes: found out later that the memory is actually being reused (simply overwrite over)
        so not clear if memory leak occurs, though the fixed allocation problem still exists


2006/01/05 (4)
--------------
- goal: convert SFVoronoi to java

during the past few days works have gone into creating the Java version of VAST,
yesterday and today smooth conversation is done to make the SFVoronoi class into Java.


2005/10/16 (7)
--------------
- goal: create VAST Protocol 0.1 (done)
        release VAST 0.1.1 (simulation version)

VAST 0.1.1 released
===================

VAST library changes:
- better consistency is achieved (100% topology consistency if no connection limit is set for up to 1000 nodes)
- timestamps are now added to position updates, so that only the most up-to-date messages are processed
- provides the option for boundary neighbor to check all neighbors or just enclosing neighbors when detecting new neighbors for a moving node.

Simulator changes:
- results are now collected only after simulation has first stablized (that is, topology consistency has reached 100% for the first time).
- non-AOI neighbors are not considered when calculating drift distance, this makes more sense and improves simulation results
- fail rate can now be specified, however, it is not considered functional.



2005/10/15 (6)
--------------
- goal: resubmit VON

Word count breakdown:
                                Final   Reference   Target:
1. Introduction:                1,006       945     1,000           +6
2. The Scalability Problem:     2,405     1,572     2,400           +5
        analysis                     1610      1090       1600
        survey                        795       482        800
3. Voronoi-based P2P Design:      762       540       750           +12
4. Evaluation                     413     1,066       350           +63
5. Conclusion                     395       373       400           -5
---------------------------------------------------------           ----
                                4,981     4,496     4,900           +81

about 10.68% more than 4,500 words quota

Intro+Conclusion        = 1401      28.126%
Scalability Analysis    = 1610      32.322%
Survey                  =  795      15.960%
VON                     = 1175      23.589%
-------------------------------
                          4981

original:

Intro+Conclusion        = 1318      29.347%
Scalability Analysis    = 1090      24.270%
Survey                  =  482      10.732%
VON                     = 1601      35.649%
-------------------------------------------
                          4491


2005/10/10 (1)
--------------
- goal: cut words down from 5,424 to 4,900 for the 1st revision draft

Word count breakdown:
                                        Reference   Target:
1. Introduction:                1,012       945     1,000
2. The Scalability Problem:     2,861     1,572     2,400
        analysis                     1600      1090       1600  18:23 (1606) 18:43 (1600)
        survey                       1261       482        800
3. Voronoi-based P2P Design:      760       540       750
4. Evaluation                     398     1,066       350
5. Conclusion                     393       373       400
---------------------------------------------------------
                                5,424     4,496     4,900


2005/10/09 (7)
--------------
- goal: cut words down from 5,801 to 4,900 for the 1st revision draft

Word count breakdown:
                                        Reference   Target:
1. Introduction:                1,012       945     1,000
2. The Scalability Problem:     3,235     1,572     2,000
        analysis                     1974      1090       1600  18:23 (1606) 18:43 (1600)
        survey                       1261       482        800
3. Voronoi-based P2P Design:      760       540       750
4. Evaluation                     398     1,066       350
5. Conclusion                     393       373       400
---------------------------------------------------------
                                5,798     4,496     4,900

Some ideas about what to do after the IEEE Network draft:

1. definition of VAST protocol              (towards impact)
2. implementation of a real network layer   (towards PLOG project)
3. simulation on node fails                 (towards a paper)


2005/10/08 (6)
--------------
- goal: cut words down from 6,500 to 5,500 for the 1st revision draft

Word count breakdown:
                                        Reference   Target:
1. Introduction:                1,120       945     1,000
2. The Scalability Problem:     3,307     1,572     2,000
        analysis                     1939      1090       1400
        survey                       1261       482        600
3. Voronoi-based P2P Design:    1,107       540       750
4. Evaluation                     973     1,066       350       17:59 (403)
5. Conclusion                     459       373       400       11:53 (392)
---------------------------------------------------------
                                6,966     4,496     4,500



2005/10/07 (5)
--------------
- goal: cut words down from 7,000 to 5,500 for the 1st revision draft

Word count breakdown:
                                        Reference   Target:
1. Introduction:                1,120       945     1,000   checked 16:53
2. The Scalability Problem:     3,307     1,572     2,000
3. Voronoi-based P2P Design:    1,107       540       750   checked 19:05
4. Evaluation                     973     1,066       350
5. Conclusion                     459       373       400
---------------------------------------------------------
                                6,966     4,496     4,500


2005/09/30 (5)
--------------
- goal: finish IEEE Network 1st revision write-up (discussion of MOPAR & ZoneFed)

one important realization is that the development/race of P2P-based NVE
might shift to a newer phase where the detailed design and slight
strength/weakness will play important roles.. as major design concepts
have been explored. the next phase will enter a "fine-tuning" phase
and the one that balances and implements well will win out.

comparision table:
            server-oriented                      P2P
                                super-nodes             fully-distributed
-----------------------------------------------------------------------
issues      scalability         super-node selection    connection choice
            reliability         super-node backup       discovery speed
            responsiveness      crowding                discovery completeness




2005/09/11 (7)
--------------
- goal: implement a workable join/fail mechanism


2005/09/08 (4)
--------------
- goal: implement a workable join/fail mechanism

Ver 0.2.9 (fail/join mechanism)
-------------------------------
- separated the functions of stat calculation from record-keeping
  in record_step () in statistics.h into calculate_stat () and
  record_step ()

  this way all the node-level mechansim (join/fail) can be decided
  entirely in VASTsim.cpp code (without having to rely on statistics.h)

- some small modification to the way data is presented is also made.


2005/09/07 (3)
--------------
- goal: implement a workable join/fail mechanism

DEBUG   the mysterious "debug mode" crash is caused by inproper
        initialization of the 'bool *g_failed_nodes' array.
        As it is dynamically allocated it needs to be initalized to 'false'.
        I used memcpy for speed. however, it was written as

        memset (g_failed_nodes, g_para.NODE_SIZE, sizeof (bool));

        instead of the correct form:

        memset (g_failed_nodes, 0, sizeof (bool)*g_para.NODE_SIZE);

Ver 0.2.9c (fail/join mechanism)  2005/09/08 01:13am
-------------------------------
created a working version that could fail percent 'p' of existing nodes.
the mechanism is guaranteed failure of the fraction of nodes.
for example, for 100 nodes running, a 10% fail_rate would fail 100%0.1=10 nodes
simulation is allowed to run indefinitely to restore consistency to 100%
before the next fail (some stablization step, defaulted to 5 steps is also
allowed the topology become more rebust). the pre-defined TIME_STEPS
is given a different meaning for failure scenarios, where it indicates
the # of scenarios to collect. However, to avoid indefinte running
in case of overlay partition, a maximum of 3000 steps is set in
VASTSim.cpp to cap the running time.

it is found that when doing a leave() cannot remove self from
_neighbors, otherwise GUI would lose the failing node from its view.
(reason unknown..)


2005/09/06 (2)
--------------
- goal: implement a workable join/fail mechanism

- added: ReadPara () in VASTSim, to faciliate shared easier
  sharing of simulation parameters between GUI and console

UNDISCOVERY (simucase-11)
    inconsistency requiring 10 steps to recover is found in relatively
    small simulation (30 nodes) with ver 0.2.9. solvable if neighbor
    discovery check is done against all known neighbors instead of the
    EN set.

DEBUG   found out the increased drift distance when doing node fail
        is caused by not calling setpos (which advances the timestamp)
        when a node fails. so now we will call setpos () after join ()
        when a node fails.

BUG     weird & difficult-to-trace bugs. if failure rate is turned on,
        after a while (> 150 steps) some nodes would keep large number of
        neighbors (25 in a 30 node simulation, for example). When trying
        to trace in Debug mode, it would crash after steps 23 - 25.
        However, if using releae mode then it wouldn't crash.


2005/08/28 (7)
--------------
- goal: improve dAOI consistency


2005/08/27 (6)
--------------
- goal: improve consistency while saving bandwidth


UNDISCOVERY (simucase-10)
    a new node may not be discovered in time because eligible BNs that are
    able to notify are also just learned by the moving node.

    solution: increase the detection radius (BUFFER_MULTIPLIER increased to 3)
    correction: it's not necessary to increase BUFFER_MULTIPLIER,
                all we need to do is to +1 in on the AOI-radius while
                doing neighbor discovery check (!)

Ver 0.2.8 (collect stat after stablized)
-------------
DEBUG   fix a small initialization bug in GUI by adding a bit displacement
        when doing initial join (helps to ensure system will stablize)
        (there's a position set that would make GUI pause for a long time
         waiting for stabliziation)

BUG     found out for the same set of data, the crashset would produce
        inconsistency at a very late step (reason unknown) while
        if the fullset is run (1000 steps) then no inconsistency would
        occur

DEBUG   found out that if the simulation runs without first doing
        displacment-based stablization, then consistency would be good.
        (weird thing described in previous BUG doesn't happen)
        decided to not run psuedo-stablization steps but run
        simulation directly. only to reset stat counters after
        reaching first 100% consistency.


2005/08/26 (5)
--------------
- goal: improve consistency while saving bandwidth

- initial runs of v0.2.7 shows that consistency has improved, however,
  inconsistency still occurs for nodes above 400
    (400 - 99.9994%)
    (500 - 99.9693%)
    (600 - 100.0000%)

  it was quite frustrating until I check out where does the inconsistency
  occurs -- at the beginning of the simulations at time-step 11
  (beyond step 10, the data collection point)

  solution: try to calculate/collect stat only after the system has
  stablized to the first 100% point. also collect # of time-steps to
  reach stable state. (done!)

- another observation is that bandwidth use under v0.2.7 increases
  exponentially. % of NODE message repeats confirms this, where it rises
  fairly high between 70% - 90% of all NODE messages as # of nodes increases.

- print inconsistent nodes within logfile


2005/08/25 (4)
--------------
- goal: improve consistency while saving bandwidth

- some stats:

    check all: all known neighbors are checked for new overlap or new EN
    check EN:  only EN are used for neighbor discovery check
    no clear:  does not clear out _neighbor_states after MOVE is received

Simulation Parameters
nodes: 50 world: (600, 600) steps: 1000
aoi: 100 connlimit: 0 velocity: 5 lossrate: 0 failrate: 0

                Neighbor discovery methods
                --------------------------
                check all   check EN    check all   check EN
                                        (no clear)  (no clear)
----------------------------------------------------------------
max_drift       15          10          10          10
consistency     100.0000%   99.9996%    100.0000%   99.9996%
avg. trans      2671        2398        2436        2278
max. trans      11431       8749        8874        8023
NODE repeat %   0.749       0.775       0.664       0.726
NODE repeat #   2643        1939        1714        1468


Ver 0.2.6 (batch processing of NODE messages)
-------------
DEBUG   found out when processing NODE message, a node may be judged if
        it's relevant on a per node (message) basis. However, this could
        be slightly incorrect if not all new node positions are considered.
        therefore we should make the judgement only after updating the
        most recent/correct position of all moving neighbors.

        fix this problem by recording all NODE messages first and then
        do connection decision in a batch. consistency has improved slightly.

Ver 0.2.7 (time check)
-------------
DEBUG   improve drift distance (and hopefully consistency as well)
        by ensuring each NODE message contains a timestamp, and only
        the more recent timestamped message would be used during
        notification.

        add parameter 'time' into Node data struct in typedef.h
        also changes aoi_t from 'long' to 'short' (so that a Node
        structure remains to be 16 bytes)

2005/08/24 (3)
--------------
- goal: improve consistency while saving bandwidth

- some testing shows that about 75% of NODE messages were redundent
  (notifying for nodes already known). And more redundent messages exist
  when all neighbors are being checked than just ENs (this explains the
  surge of bandwidth use). Certainly an area for improvement.

  some ideas are:
    - when a node beings to send MOVE instead of MOVE_B, this clears the
      neighbor_states list, however this will cause much redundent NODE
      messages be sent when the moving node again sends out MOVE_B
      so overlap & EN checks should always be performed (but sends NODE
      only when necessary)

    - try to see if there are deterministic methods to separate NODE
      message streams (that is, among the notifying nodes, try not to
      send redundent NODE messages)


2005/08/18 (4)
--------------
- goal: write 1. related works 2. DT-overlay

DEBUG   the crash bug from the new fail/join mechanism is caused
        by accessing _neighbor_states during the neighbor discovery
        stage. _neighbor_states becomes invalid because right now
        neighbor discovery is done in batch, so there could be
        DISCONNECT messages in between MOVE_B. Add a check to prevent
        invalid access of _neighbor_states.

- add a small bandwidth saver:
    store the info sent by EN message as the initial data in
    _neighbor_states (assumed that the sender considered those nodes in EN as
    overlapped). this saves about 5% bandwidth use.

- found out that when doing neighbor discovery check, using only the
  EN and all neighbors for the check have different consequence:
  using EN only saves bandwidth but has lower consistency, while using
  all neighbors usually give very good consistency but uses at least 10%
  more bandwidth

2005/08/17 (3)
--------------
- goal: implement node fail/join mechanism

- did a version that implements join/fail, by specifying a FAIL_RATE,
  then within VASTsim library's NextStep () it would randomly make a
  node leave the overlay, then re-join with a new position. Also modify
  vast_dc accordingly to prevent re-obtaining node id. compiled alright
  but would crash when run.

2005/08/16 (2)
--------------
- goal: achieve 100% consistency for basic & dAOI model

DEBUG
- investigate the seemingly large drift distance. found out part of the
  problem is that when a node is first learnt, its position might not be
  the most up-to-date, therefore it would have some non-zero drift.
  however, the situation is usually resolved after the node has moved
  into the AOI. Therefore, calcuation of drift distance should be
  modified to only that within AOI.

Thoughts on VAST 1.0
--------------------

Two new developers will be joining the developments of VAST:
Chang Chen Ye-Zen and Yakko. Both were friends I met back while at
the Computing Center of NSYSU, and have been professional software
developers.

There were some discussions over the weekend on what we should do next,
and Yakko suggested to make a To-Do list for perhaps VAST version 1.0 --
a practically useable version. So here are some initial thoughts organized
into three categories:
(you're welcome to contribute ideas to add or modify the list)


Research
--------
- Latency tolerance mechanism (consistency under packet delay & loss)

  packet delay and loss are inevitable on real networks, therefore
  techniques are needed to deal with latency, jitter, and packet loss.
  100% consistency in "logical time" can be maintained if we can wait
  indefinitely for retransmission of late or lost packets. however,
  "real-time" requirement usually does not allow this.


Design
------
- Data format and protocol specification.

  VAST might be adopted more widely if a set of common protocol and data
  formats are developed.


Implementation
--------------
- 100% topology consistency and low drift under 0% packet loss,
  and "good enough" performance (for example, above 95% consistency) when
  it is acceptable for some packets to be lost.

  The first scenario is useful when real-time response is not strictly required
  (for example, when we're running a simulation without real people involved,
   but accuracy of node positions is required),
  and the second is usually the case for applications that
  involve real human avatars (when we cannot wait for delayed packets indefinitely)

  This characteristic will need to be tested by simulation.

- NAT-transversal.
  VAST should be usable for people who're behind VPNs.

- Cross-platform network transmission.
  delivery of reliable and best-effort data should be supported.
  (that is, regular and reliable UDP delivery)


2005/08/15 (1)
--------------
- goal: achieve 100% consistency for basic model

- found out the addition of sending DISCONNECT message would cost
  much bandwidth (20% more bandwidth to obtain slight increase in
  consistency). Experiment with an alternative approach:
  do not send DISCONNECT message, but notify the moving node if
  a node becomes overlapping new neighbor, regardless of whether it was
  previously notified as an EN.  This will preserve much of
  the original mechanism, and there will be no DISCONNECT message
  to send. this approach increases traffic by about 5% than the OVERLAP_MULTIPLIER=4
  approach for 50 nodes in 600x600 environment.

  following is a list of comparison of various approaches:
  (simulation parameter: 600x600 50 nodes, 1000 steps)

    Approach                Consistency     Avg. Trans.     Max Trans. (bytes/second)
  ----------------------------------------------------------------------
    original                99.9996%        2700            8858
    (overlap multiplier=4)
  ----------------------------------------------------------------------
    DISCONNECT              100.0000%       3269            11444
    (no NODE_EN)
  ----------------------------------------------------------------------
    DISCONNECT              100.0000%       3281            11520
    (NODE_EN)
  ----------------------------------------------------------------------
    no DISCONNECT           100.0000%       2878            10512
    (separately notify)


2005/08/14 (7)
--------------
- goal: try to solve the less than 100% consistency problem with
        OVERLAP_MULTIPLIER set to 2 (instead of 4)

DEBUG   achieved 100% consistency in test run after adding
        DISCONNECT message to include when a node refuses to connect
        to a NODE message (notify the BN that the NODE message was
        not processed, so that it may has a more correct view of
        "known neighbors" for the mover)


- fix a small bug to exclude calcuation of internally generated
  DISCONNECT message (this causes higher amount of RECV traffics than
  actual, avg SEND and RECV should be the same amount when it's 0 packet loss)

-


2005/08/12 (5)
--------------
- goal: try to solve the less than 100% consistency problem with
        OVERLAP_MULTIPLIER set to 2 (instead of 4)

- found out when to call remove_nonoverlapped () is a tricky decision.
  currently it is done immediately when setpos () is called.
  however, there are two other places where it might be placed:
    1. at the end of setpos ()
    2. at the end of processmsg ()

  however, the other two possibilities generate lower consistency than
  when it's called right when setpos is called (reason unknown)


BUG     found out a troubling issue related to the algorithm itself:
(big)   nodes may have different judgements about enclosing neighbors,
        which causes missing neighbor discovery. for example:

        it is possible that node A may disconnect node B because
        node B fails both the overlap and enclosing neighbor test.
        however, a middle node C may still consider node B to be node A's EN
        (due to the fact that node C and node A have different known nodes)
        therefore, to node C, node A should know node B (that it is unaware
        of the disconnection between A & B)

        two possible solutions:
            1. node A also notifies node C when it disconnects node B
            2. node C does not keep enclosing neighbors as part of the
               "known-node" list, thus would continously notify for new EN,
               and would also notify when two nodes become overlapped.

        after some testing, solution 2 does increase consistency,
        at the cost of more traffics (an increase from 200% to 10%,
        depending on node density)

BUG     after applying solution #2 above, there's still some inconsistency.
        Tracing shows that it's because due to the way behavior model
        generates positions, a node may record its own maximum speed
        as somewhat less than actual (for example, 4 instead of 5).
        This would cause misjudgement when this node is doing
        neighbor discovery for others. If it uses other nodes's speed
        as max_velocity, there'd also be problem if a movement packet
        is skipped (due to unknown reason, possibily a reconnect followed
        by disconnection?) and causing max_velocity to be
        greater than actual.

        solution: specify max_velocity when creating a vast node
        instead of calculating from position records
        (not adopted, still use recorded results, in order to maintain
        the separation of functionality)

NOTE    there's another situation that needs to be considered, which is
        when Node1 notifies Node2 for a new neighbor Node3, it would
        subsequently assume that Node 2 will be aware of Node3. However,
        in reality, Node2 would judge independently whether to make the
        connection, therefore Node1's knowledge could be wrong and
        does not notify again when it should. However, analyzing further,
        this shouldn't happen for overlap test (as it is universal)
        but only would happen for enclosing neighbor test, so perhaps
        solution 2 above is a better choice, though it might generate
        more traffics

NOTE:   an important observation is that, for neighbor discovery
        notification to properly function, it's quite important that the
        "buffer area" is correct. Buffer area is ideally the combined
        distance two potentially visible nodes travel to each other.
        However, if the buffer area is specified, we will need assumption
        about maximum velocity of all nodes. If it's too large then
        connections could be wasted, if it's too small then undiscovery
        could occur.

BUG     another subtle bug: because during neighbor discovery check, only
        ENs are involved. therefore if a new neighbor
        that should be learned is not the EN of the checking node, then
        neighbor discovery could miss until other mechanisms set in
        (for example, both A & B ask C to check for new neighbor,
         A is C's EN but not B, therefore B would be notified of A but
         A isn't notified of B. A will learn of B when B contacts A,
         or when B becomes some other nodes' EN which A requests new
         neighbor check)

         solution: one simple solution is to check for new
         neighbor discovery from *ALL* known nodes instead of just the EN set
         (adopted)


2005/08/11 (4)
--------------
- goal: try to solve the less than 100% consistency problem with
        OVERLAP_MULTIPLIER set to 2 (instead of 4)

- organize trace messages

DEBUG:  found out overlap check would miss because _max_velocity was 4
        (instead of the correct 5) as it was actually re-calculated
        every time-step. the problem seems to disappear (for node=30)
        after making sure _max_velocity was indeed 5


2005/07/05 (2)
--------------
- goal: put first initial release of VAST (v0.1)
        (but must be relatively stable under emulator mode and
         can generate all results in the paper without problems)
        add packet_loss parameter


2005/07/04 (1)
--------------
- did some testing & experiments, found out

  using a OVERLAP_RADIUS_MULTIPLIER of 4 can almost guarantee 100% consistency
  even with velocity of 10, though 3 already works well for velocity of 5

  however, if NODE messages are processed in the same time-step as when the
  neighbor discovery is made (instead of the next step), using a multiplier of
  2 is enough (vel=10).

  This supports the hypothesis that the multiplier basically
  serve as a counter-measure to latency.



2005/07/02 (6)
--------------
- attempt to increase consistency while decrasing gap between
  CN & AN.

- tried to fall back to original overlap check method and see if
  consistency can be maintained without using OVERLAP_RADIUS_MULTIPLIER

  had first series of simulation results.. CN increases while consistency
  is maintained, though more inconsistent in dAOI model.

2005/06/16 (4)
--------------
- goal: get new version to work on linux & start generating new result sets
        for IEEE Network paper.

DEBUG   on linux the behavior model seems to generate non-random movements
        (initial positions for many steps do not change at all)
        appearantly every time the program calls the move_all(),
        random number are reset according to current time.

        solved by keeping a _last_seed to reseed random number generator
        every time.  This brings movements to be much more random.

        however, for the same set of positions, linux and windows
        generate slightly different results (reason unknown)



2005/06/15 (3)
--------------
- goal: debug dAOI inconsistency problem (20:30)

Simulation Parameters for below comments
    nodes: 30 world: (600, 600) steps: 990
    aoi: 200 connlimit: 13 velocity: 10 lossrate: 0

- experiment with various multiplier when doing nonoverlap test.
  found out 1x _max_velocity works best (99.88% consistency)
  with more multiplier causing decreasing consistency.
  however, lowest consistency is experiecned (99.78%) when not
  using any multiplier.

- even better consistency (99.9155%) is achieved if no multiplier is
  used to check if the node to be disconnected has larger AOI to cover me
  (reduce from 4x _max_velocity to 0 or 1),
  however AOI-radius is also reduced dramatically.

- after some trials, appearant a BUFFER_MULTIPLIER of 4 will produce
  100% consistency in basic model. However, for dAOI model, using
  a BUFFER_MULTIPLIER of 4 but when doing remove_nonoverlap() use only
  a multplier of 1 seems to achieve better consistency.
  likely reason is that in dAOI it's necessary/preferable to have
  connections be disconnected as quickly as possible if no longer
  relevant (otherwise will take up valuable connection resource)

DEBUG   min_aoi in some cases is 0 or smaller than 0 when running in
(big)   dAOI mode. found out it was because aoi_t was set to "unsigned long"
        which did not allow for negative numbers. However, when shrinking
        AOI it's possible to get negative, which in turn becomes a very
        large positive number. therefore the <= 5 test would not reveal problem.

        fixed by changing "unsigned long" to "long"

MILESTONE
        achieves close to 100% consistency for dAOI after accepting all
        remote connections and do not disconnect unless both nodes are
        mutually unaware, specifically the following modifications are made:
        (recorded as simu-cases "undiscovery 6")

            - removes the HELLO_E to accept all incoming HELLO
            - send out OVERCAP when trying to disconnect a remote node that
              still considers me as its AOI neighbor
            - drop the use of _dist_drop (count_drop was enough)

2005/06/14 (2)
--------------
- goal: debug dAOI inconsistency problem

DEBUG:  a rare undiscovery problem is caused by the remote node's
        notification of a new neighbor. Yet the node that was
        notified did not think the new node falls within its AOI
        (inconsistent judgement as to whether the new node is
         *overlapped*) However, as the remote nodes only notify
         once they go on thinking the new node has been learnt.
        usually this can be avoided as the new node would likely
        also be notified of this node. however, in this case
        the node node was already over-connected, therefore respond
        by shrinking AOI instead of initiating the connection.

        solution: add a buffer of 5 units when doing overlap check
        when receiving NODE message. also decrease OVERLAP_RADIUS_MULTIPLIER
        from 0.20 to 0.15

        consistency 99.9036% -> 99.9950%

        parameters:
        nodes: 25 world: (600, 600) steps: 990
        aoi: 100 connlimit: 10 velocity: 5 lossrate: 0

        OVERLAP_RADIUS_MULTIPLIER       consistency
        0.05                            99.9147%
        0.10                            99.9918%
        0.15                            99.9950%
        0.20                            99.9583%

        NOTE: dAOI consistency fixes in 06/13 and 06/14 are stored as
        simu-case "undiscovery-5 (incremental fixes in dAOI)"

UPDATE  major modifications:

        replace MAX_SPEED, OVERLAP_RADIUS_MULTIPLIER, MAX_DROP_DISTANCE
        in "vast_dc.h" by calculating max_velocity on the fly and
        record remote node's most recent velocity.

        added BUFFER_MULTIPLIER and set to 4 initially.

        also change the return value of dist () from long to double
        in "typedef.h" (Point class)



2005/06/13 (1)
--------------
- goal: debug dAOI inconsistency problem


DEBUG   one dAOI inconsistency problem occurs because in adjust_aoi ()
        the judgement of whether a new adjusted AOI is smaller than the original
        did not take into account of OVERLAP_RADIUS_MULTIPLIER..
        so adjustments weren't made when they should.

        25 nodes @ 600x600
        consistency 99.8749% -> 99.9036%

DEBUG   another problem was caused by removing non-overlapped neighbors
        that has a larger AOI (therefore considers myself as within AOI).
        seems like the OVERCAP message was not processed properly.
        however, also found that the node that disconnects still has
        spare capacity. made a modification to allow continued connection.

        consistency 99.9036% -> 99.9335%

DEBUG   when doing remote_disconnect, a DISCONNECT message is generated
        automatically at the disconnected node with timestamp 0,
        which gives it higher priority than a potential OVERCAP message
        (which notifies the disconnected node to shrink its own AOI)

        consistency 99.9335% -> 99.9616%


2005/06/12 (7)
--------------
- goal: debug dAOI inconsistency problem

DEBUG:  found out the last joined node always knows neighbors from one step
        later (so position is always inaccurate, which would increase
        drift distance by a certain amount)

        fixed by making time_stamp to progress for every call to setpos()
        even if the node does not actually move (unique ID not yet received)
        consistency actually drops from 99.87% to 99.85% for 25 nodes in
        a 600x600 environment. though avg_drift improves from 0.33 to 0.12

2005/06/09 (4)
--------------
- goal: debug dAOI inconsistency problem

DEBUG   found one dAOI bug happens at the processing of HELLO message,
        because it would send OVERCAP if a node is already exceeded in its
        limit. however, if the HELLO message is sent from an already-connected
        node, the same check still applies and could disconnect one that's
        already-connected (violate valid scenarios for disconnection)

        solution: check if the sender of HELLO is already connected, if so
        then simply just update the node information.


2005/06/08 (3)
--------------
- goal: fix the process-within-same-timestep problem in the emulator (21:00)
        debug dAOI inconsistency problem

- added a _time variable to vast_dc class, also modify sendmsg(), recvmsg(),
  and storemsg() in net_emu.cpp to include passing the current logical
  time. Found out the difference after taking logical time into consideration
  (so that no node may process message generated by other nodes within the
   same time-step) is that consistency drops slightly (for 30 nodes)
   but was subsequently fixed by adjusting OVERLAP_RADIUS_MULTIPLIER
   from 1.10 to 1.20. Drift distance also increases (from 0.03 to 0.24)

- 100% also achieved for 100 nodes with the new delay


2005/06/07 (2)
--------------
- goal: debug basic model inconsistency problem (11:10pm)
        debug dAOI inconsistency problem

- add 'w' 's' 'a' 'd' 'o' commands for moving the GUI viewport
  UP DOWN LEFT RIGHT and back to ORIGIN to the GUI interface

- achieves 100% for 1000 nodes @ 1000 timesteps

DEBUG   improved consistency in basic model by resetting drop counters
        when receiving NODE message for already-connected nodes

BUG     there's a nasty inconsistency problem that seems to result
(big)   from Node A learns of Node B in the same step after Node A
        disconnects Node B. So within the same step Node A does:

            1. disconnects Node B (sends Node B a DISCONNECT message)
            2. learns of Node B through NODE again (sends HELLO, so reconnects to Node B)
            3. displays Node B correctly

        however, in the next step
            1. Node B process the disconnect message from Node A
               and sends to Node A a DISCONNECT when doing a delete_node
               (NOTE: usually Node B cannot send Node A a DISCONNECT as
                      the connection should already be broken, yet here
                      it is possible as Node A has re-established the connection again)
            2. Node B process the HELLO message (so learns of Node A again)
            3. Node A process the DISCONNECT from node B (so now Node B doesn't exist)
               and sends another DISCONNECT to Node B

        at this point Node A doesn't see Node B, while Node B still sees Node A
        in the next step
            1. Node B process the DISCONNECT (so finally Node B also loses Node A)

        and likely because the neighbors of Node A and Node B think
        they've already done the notification, so no new notifications
        are sent until several steps later.

DEBUG   fix the above bug by preventing sending DISCONNECTION again
(big)   if delete_node is called as a response to a DISCONNECT message
        (recorded as "undiscovery-4" in /simu-case)


2005/06/06 (1)
--------------
- goal: debug dAOI inconsistency problem

BUG:    slight inconsistency might exist due to bootstrapping
        in the beginning of simulation.

IDEA:   consistency under direct connection model without AOI
        adjustment (the most basic model) is actually important,
        in the sense that for application such as protein folding
        (MD simulation), it is necessary to have 100% consistent
        (fully-synchronized) simulation. This is when the real-time
        requirement is droppable.


2005/06/04 (6)
--------------
- put on first version of VAST's public website at sourceforge
  (http://vast.sourceforge.net)


2005/06/02 (4)
--------------
- goal: simulate dAOI model and tune
        write VAST description for website at sourceforge


2005/06/01 (3)
--------------
- goal: debug inconsistency problem (3:30pm)
        100% consistency @ 500 nodes (20:42)

BUG:  a)found out a rare inconsistent case where node A disconnects node B
(big)   (as it's out of its AOI with enough drop count), however, just at
        that particular step, node B actually has moved into node A's AOI.
        But node A misses this as it hasn't known node B's most recent
        position when making the disconnection decision.
      b)Coupled with this problem is that the neighbors of node A and B
        think they still know each other (could be because a new neighbor
        notification was sent earlier, so these neighbors *thought* the
        two know each other), so no neighbor notification is sent.

        possible solution: [1] use a "drop distance" instead of "drop count"
        when the total distance has accumulated over a threshold then
        do we disconnect. [2] A second solution is to do the disconnection
        check (i.e. remove_nonoverlap()) after learning all the new positions
        of neighbors.

DEBUG:  solve problem b) using solution [2] by calling
        remove_nonoverlapped() before actually setting new position
        in setpos()

MILESTONE
DEBUG:  achieved 100% consistency for up to 100 nodes (AOI-100)
(big)   by adding "drop distance". After testing, use both
        "drop count" and "drop distance" when deciding disconnection.
        (inconsistncy still occurs by using "drop distance" alone)

IDEA:   after consistency is achieved, observe that OVERLAP_RADIUS_MULTIPLIER
        indeed plays a role on consistency in that while both the multiplier
        1.10 and 1.05 achieves 100% consistency for 110 nodes, AOI: 150.
        there are cases of inconsistency that recovers within 1 step for
        multiplier 1.05.

BUG:    consistency drops considerably starting after 100 nodes,
(big)   serious problem at 500 nodes, crash at 1000 nodes
        initial overlay partition occurs during one 110 node trial
        likely causes:
            1) joining too many nodes initially (should do so incrementally)
            2) buffer size inside Voronoi class too small
            3) initial ID-assignment problem (display incorrect ID after node 128)

DEBUG   solve scalability problem by treating case 3)
        by replacing the original id assignment
                _self.id = (id_t)*msg;
        with
                memcpy (&_self.id, (void *)msg, sizeof (id_t));

        then, treat case 1) by run processmsg() inside CreateNode ()
        successfully run 500 nodes for 100 steps

BUG:    still has segmentation fault when running 1000 nodes
        possible cause: exceed neighbor size maintained per node
        (in SFVoronoi.h, set to 100 currently)

2005/05/31 (2)
--------------
- goal: debug inconsistency problem

DEBUG   found out one undiscovery problem is caused by the checking node
        has not processed the MOVE commands of all neighbors, therefore
        using an outdated position to the check.

        also, the checking node did not use OVERLA_RADIUS_MULTIPLIER when
        doing the check.

DEBUG:  do neighbor discovery check after processing all the MOVE messages,
(big)   this does improve consistency. (drift distance has decreased
        dramatically)


2005/05/25 (3)
--------------
- goal: debug inconsistency problem

DEBUG   found out the weird incorrect EN-judgement in some test case
        was caused by when processing a NODE message, we may insert the new
        node to Voronoi first then remove it. However, this practice
        could redundently insert and remove from the Voronoi class when
        the NODE messsage contains an already-known node.
        after fixing this bug only minor inconsistency occur (fixed within
        the next step).

DEBUG   with some tweaking consistency can now bring to above 99.97% for
        up to 30 noes

ideas:
        two possibility for the low consistency:
            - the new event model makes neighbor discovery slower (at least
              one round is required to learn), this could explain many of
              the inconsistency that fixes itself after one step

            - when processing message, if we immediately respond to MOVE
              and respond with NODE, it might not give the most update/current
              view, as subsequent MOVE command could change the Voronoi picture.
              This can be fixed by first do all the movements, then respond
              with NODE in a collective manner. this might explain the slight
              drop of consistency in the basic model when # of nodes is high.

2005/05/16 (1)
--------------
- goal: debug inconsistency problem


2005/05/15 (7)
--------------
- goal: debug inconsistency problem

DEBUG   woke up and realize the inconsistency problem could be due to
        that i've combined the new AOI and new EN neighbor discovery
        together in the current VAST (as opposed to using separate list
        in VON). however, later found out the current inconsistency problem
        is likely caused by incorrect judgement of EN (still to be investigate)

2005/05/13 (5)
--------------
- goal: debug inconsistency problem

DEBUG   solve a little bug where there are some "zombie" nodes that just
        stand still. found out the problem was that delete_node() performs
        checks whether the node to be deleted is an enclosing neighbor.
        Actually it need not check (but this also tells us that, it's possible
        that A disconnects B because A thinks B no longer overlaps and is
        not an enclosing neighbor, yet A could still be an EN to B
        (this could a problem.. isn't it?)

2005/05/12 (4)
--------------
- goal: 1. record/playback mechanism    (done)
        2. low inconsistency debug

- print results to log files (15:18)



2005/05/11 (3)
--------------
- goal: move to linux platform and run test simulations     (17:40)
        find out about the low consistency problem

BUG:    discover the problem from yesterday was that the math and C++ libraries
        do not seem to link right. (even a small test program that calls
        sqrt() cannot be compiled)

        solution: use 'g++' for compliation, and add "-lm" when trying to
                  link against the math library when using "gcc"

BUG:    incorrect results for 500 nodes, 1000 nodes would crash after step 2


2005/05/10 (2)
--------------
- goal: move to linux platform and run test simulations
        find out about the low consistency problem

BUG:    while trying to port VAST to linux, met with this annoying bug:

console.cpp: In function `int main(int, char**)':
console.cpp:52: use of `id_t' is ambiguous
/usr/include/sys/types.h:105:   first declared as `typedef __id_t id_t' here
../../include/typedef.h:15:   also declared as `typedef long unsigned int
   VAST::id_t' here
console.cpp:52: `id_t' denotes an ambiguous type
console.cpp:52: confused by earlier errors, bailing out

        basically it says that 'id_t' has already being defined in "types.h"
        therefore there's a double-definition problem.

        solution: use a work-around by avoiding the use of id_t in all
        client programs. (for example, in statistics.h, I changed
        map<id_t, Node *> into map<unsigned long, Node *>

- got both /src and /src_sim to compile into linux shared library (.so format)
  successfully, however, wasn't able to compile console.cpp properly
  (appearantly due to linker problems where simple methods such as delete[]
   and new could not be found..  a library path problem?)





2005/05/09 (1)
--------------
- goal: 1. implement connection beyond conn_limit restrictions      (16:43)
        2. finish stat collection code                              (21:30)
        3. move to linux platform and run test simulations

- both console and gui mode can run fairly smoothly now, and produce
  good results. now the main problem visible is the dropped
  consistency (reason unknown)



2005/05/07 (6)
--------------
- goal: 1. debug vast_dc using GUI
        2. move to Linux box, run simulation that collects results

BUG:        inconsistency of neighbor position occurs (esp. with large # of neighbors)
            possible cause:
                When A tries to connect to B, B refuses (out of its
                own AOI). however, A is not notified so will keep "zombie"
                node in its local view.
                (however, problem persists when refusal mechanism is removed)

DEBUG:  the above bug was somehow solved by modifying the following:
         1) remove the overlap check in insert_node (so that insertion only
            checks if it was already there)
         2) when responding to an EN with NODE, only relevant ENs
            (i.e. those overlapped with the moving node) are sent back
            (previously ALL missing neighbors are sent, therefore making
             it necessary for insert_node to check if it was necessary
             to connect upon receiving NODE, now we just accept whatever's
             notified by NODE unconditionally)

        some "slight" inconsistency is still observed when a new node is
        discovered, however, it's usually quickly fixed when the moving node's
        next position update is received (reason unknown).

IDEA:   define a new 'topology consistency' matrics:
        should take into account difference between observed & unobserved
        (mix 'consistency' with 'drift distance') consisteny is weighted
        against difference in coordinates

IDEA:   different types of disconnection needs to be categorized:

            Scenario                  Response                  Message
        -------------------------------------------------------------------
           1. leaving overlay         reaction not needed       DISCONNECT
           2. AOI no longer overlap   reaction not needed (?)   DISCONNECT
           3. refuse connection       adjust AOI                OVERCAP
           4. shrink AOI              adjust AOI                OVERCAP

TODO:   joiner exceeds connection limit should be taken care

IDEA:   title for a new paper:
(big)   "Spatial and Temporal Locality-based Visibility" or
        "Scalalbe Visibility based on Spatial and Temporal Locality"

        idea is simple: how do you see "more" in AOI, for a given amount of
        bandwidth, yet require DIRECT connection among all nodes?

        Answer: you set up different update frequency with each sender.
        sort of like using a big pipe for more nearer sender, and smaller pipe
        for further sender. But the combined size of pipes are the same,
        and if you extend more and more (receiving from more and more users),
        the pipes will become smaller and smaller. If done well, this could
        bring very nice "granuality" into update frequency and potentially
        see large number of users, and can even individually priortize
        update frequency.


BUG:    after implementing pieces of the 'statistics' class.
        program just halts/crashes for no appearant reason, even simple
        console mode wouldn't work.

DEBUG:  found out it was a small "fix" in net_emu.cpp that caused the problem
        (and I even removed the whole new 'statistics' class!)

            // failure likely due to memory allocation problem
            if (newnode->size == 0)
                return 0;

        it skipped the proper insertion to message queue, ignoring the fact
        that size of 0 can in fact be legal.


2005/05/06 (5)
--------------
- goal: finished debugging the networked layer


BUG:        very weird bug in send_nodes ()
            had tried to use various methods in the map _id2node
            for example:

            map<id_t, std::vector<Node>::iterator> _id2node;
            map<id_t, Node *> _id2node;

            but when trying to access a particular node in send_nodes ()
            the node access, whether using iterator or Node * would
            give very strange/incorrect data for a particular node_id

DEBUG:      possible cause for the above:
            after insertion to a std::vector, the ordering will change
            (i.e. cannot assume the first inserted can be accessed with index 0)

            solution: change the types of _neighbors and _id2node to

            std::vector<Node *> _neighbors;
            map<id_t, Node>     _id2node;

DEBUG:      found out a major bug left from previous implementation of VON
(big)       it is possible that a remote node has just been disconnected,
            yet it has sent MOVE messages to me. therefore when trying
            to process this MOVE message, the remote list's EN list would
            not be found. It was worked around by 'new' an EN list, however
            the real solution should be simply to ignore the MOVE message

- finished networking layer debug, also created preliminary GUI interface
  (global & switchable local view, AOI-radius display, Voronoi edges)


2005/05/05 (4)
--------------
- goal: finished debugging the networked layer

DEBUG:      the crash when calling vast_dc destructor bug was caused by
            incorrect use of iterator

            in insert_node() the code was

            _neighbors.push_back (node);
            _id2node[node.id] = _neighbors.end ();

            // types
            std::vector<Node>                       _neighbors;
            map<id_t, std::vector<Node>::iterator>  _id2node;

            the proper way should be:

            _id2node[node.id] = _neighbors.insert (_neighbors.end (), node);


2005/05/03 (2)
--------------
- goal: finished implementing the networked layer
- implemented but buggy (crash when destructor of vast_dc is called)



2005/05/02 (1)
--------------
- goal: implement the networked layer

some ideas: A universal (for both emulation and actual network) network
            interface for the protocol layer to use (class 'network')

            In emulation, there'll be a shared node discoverer object
            (class 'netemu') used to find pointer to different 'network'
            objects to send messages to. each network object maintains
            separate "receive queue"

            when network object is first used, it would obtain a temp.
            id from the discoverer object, until it gets its unique ID
            from gateway and register with the discoverer object

- had done a preliminary compilable version of a switchable network layer


2005/04/29 (5)
--------------
- goal: implement the networked layer


2005/04/28 (4)
--------------
- goal: implement the networked layer


2005/04/19 (2)
--------------
- goal: implement a simulated network and have some actual simulation runs


2005/04/18 (1)
--------------
- goal: implement VAST DC model as much as possible

- finished a prelimiary implementation of vast_dc.cpp
  now onto network.cpp

2005/04/16 (6)
--------------
- goal: implement VAST DC model as much as possible


2005/04/15 (5)
--------------
- goal: implement VAST DC model as much as possible

- implemented processmsg() all messages except MOVEs

2005/04/14 (4)
--------------
- goal: implementation of the direct connection model and
        simulated network layers? (miracle required.. :)

- create a factory class for vast called 'metaverse', which offers
  create() and destory()

2005/04/13 (3)
--------------
- goal: implementation of the direct connection model and
        simulated network layers.

- separated vast.dll and VASTsim.dll developments, an outside app that
  uses VASTsim.dll will only need to include VASTsim.h, provided
  that the directory of /src and /src_sim are located in the same place.
  (better way to do it?)


2005/04/12 (2)
--------------
- goal: create workable GUI suitable for displaying
            - global view of nodes                      (check)
            - local view of a particular node
            - regular refresh/updates                   (check)
            - view-pan/move
            - enlarge/smaller (optional?)

- studied DX9 in the morning, but decided to use GDI instead.
  got a working version. also created the VASTsim.dll library apart from
  vast.dll

2005/04/11 (1)
--------------
- goal: define interface and code structure

- defined vast.h        main interface used by outside app (virtual class)
          vast_dc.h     direct connection model implementation
          typedef.h     main data structures
          network.h     network layer
          config.h      platform-specific configurations
          Voronoi.h     Voronoi services
          SFVoronoi.h   Steve Fortune Voronoi

          vast_sim.cpp  a simulator
          behavior.h    behavior model (random movements)

2005/04/10 (7)
--------------
- start log for VAST

VAST: VON-based Adaptive Scalable Toolkit

    - a light-weight C++ library to support VON
    - three layers: 1) simulator 2) protocol 3) network, each is substitutable
    - well-defined interface, easy to use

    - clean, fast, solid
    - ACE is confined to network layer

- goal today: define interface and code structure



